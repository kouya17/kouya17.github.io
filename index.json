[{"content":"最終的な目標はプレイスタイルの解析 近年、動画投稿・配信サービスの普及により、ゲーム実況というものがエンタメの1カテゴリとして確立してきていると思う。感染症流行の影響が大きいと思うが、2021年1月～3月の主要ゲーム配信サイト(Twitch、YouTube Gaming、Facebook Gaming)の合計視聴時間が前年同期比で80%増加したというデータもある。1ゲームのジャンルとしてはアクション・RPG・シミュレーションなどあらゆるものが実況の対象になるが、今回はFPSのゲーム実況にスポットを当てる。\nFPSの分野では、オンライン対戦システムおよび、いわゆるランクシステムが整備されているものが多くなっている。ランクシステムは、プレイヤーの習熟度のようなものを可視化する。こうしたランクに関する情報は、視聴者にとって、プレイヤーのことをパッと理解するために非常に役立っていると思う。\n現状、ランク情報やダメージ数、キルデス比等はゲームシステムから参照できるようになっていることが多いと思う。ただ、個人的にはより細かい、プレイヤーのプレイスタイルのようなものを可視化できる情報があると、プレイヤーについて理解を深めるためのよい材料になると思った。\n最終的にはゲームシステムから直接確認出来るデータだけでなく、ゲームプレイ動画の解析結果から、プレイヤーのプレイスタイルをうまく可視化したい。今回は、Apexを題材にし、プレイヤーのプレイスタイルを示す1つの指標として、武器使用率をゲームプレイ動画から解析できないか試す。まあ、こういった情報はわざわざ動画を解析しなくても、ゲームシステム側で実装してもらえれば、より正確で容易にデータが取れるとは思う。\n全体の作業の流れ 今回の全体の作業の流れは以下のようになっている。\n各作業の内容について説明していく。ただし、ゲームプレイ動画のキャプチャ作業については特に説明することがないので、何も説明しない。\n武器名表示領域の切り取り データセットを整備するために、ゲーム画面のうち、武器名を表示している領域のみを切り取った画像を作成する必要がある。今回は、動画に対して一定時間ごとのフレームを画像化→特定領域を切り取り→特定の場所に画像ファイルとして保存、という流れでデータセット用の画像を作成した。\n一連の画像切り取り作業実施のために、ツールを作成した。GUI部分はPySimpleGUIを利用している。\n主に以下を設定できる。\n 切り取り対象とする動画の保存場所 武器1欄(*1)の画像を保存する場所 武器2欄(*1)の画像を保存する場所 画像を切り抜く領域(*2)(全体サイズに対する割合で指定) (*2)で設定した領域からどれだけずらした画像を何個保存するか 画像切り取り対象とするフレームの時間間隔  (*1)Apexでは、武器を同時に2つ持つことができる。今回は武器1欄と武器2欄を別々のデータセットとして扱う。理由はそちらの方がより精度が上がりそうと思ったためである。\nこのツールを使えば、ボタンを1回ポチーすることで、後は待っていれば所定の場所に武器名表示領域の画像が保存される。\n画像に対して正解ラベルを付ける データセットの正解ラベル付けについて、どのような形式で保存するのが一般的かを私は知らないのだが、今回は各正解ラベル(武器名)ごとに保存フォルダを分けることにした。\n以下のように、全武器+\u0026ldquo;武器なし\u0026rdquo;(_None)について画像をフォルダ分けした。\nこの作業にも専用ツールを作成した。\nデータセットが置かれている場所を指定して、startボタンを押すと、所定の個数づつ未分類の画像が表示される。表示された画像について、正解となる武器名のボタンをポチポチしていき、画像をフォルダ分けする。\nただ、今回データセットの元にしたキャプチャ動画は武器を決まった順番で使うようにしていた。そのため、切り抜かれた画像が最初から決まった武器順で並んでおり、結局このツールはあまり使わなかった。\n学習・モデルの保存 データセットの準備が出来たので、ようやく学習をする。学習部分はコードベースで説明する。\nまず、データセットの読み込みを行う。かなりコードが汚い。武器名のリストは、以下のコードに含まれていない関数を使ってファイルから読みこんでいる。\n Kerasを用いてモデルを生成する関数を作成する。\n Optunaを使ってハイパーパラメータを調整しつつ、成績の良いモデルを保存する。(といってもマシンが貧弱で学習に時間がかかるので、5回しか試行を回していない。)\n 上記のコードを実行したところ、とりあえず今回はテストデータの正解率が99.89%のモデルが出来た。テストデータの推論結果をMatplotlibを使って画像とともに確認してみる。\nいい感じに推論できてそうだ。誤答をしたデータについても確認してみる。\n今回は2つのデータの推論が間違っていたようだ。上は正答がFlatlineなのに対し、Devotionと推論してしまっており、下は正答がNoneなのに対し、R-301と推論してしまっている。\n作成したモデルによるプレイ動画の解析 モデルが出来たので、動画を解析する。解析用のソフトを作成した。\n学習用に使ったデータとは別のキャプチャ動画を使って解析したところ、以下のような結果になった。\n解析結果を見たところ、大体は合っていると思うが、誤検出されている武器名が末尾に並んでいる。解析結果は正確なものではないため、データの扱いは少し考えないといけない。\n最初は自作モデルを作らなくても行けると思っていた 今回、武器使用率算出のために\u0026quot;機械学習のモデルを作成・利用する\u0026quot;という手段を選択した。\nこの手段を取る前は、TesseractとPyOCRを利用して文字列を認識する方法をとっていた。ただ、認識精度が厳しかったので、利用を断念した。機械学習の勉強にもなると思い、代わりに自作モデルの作成という方法をとった。\n今回の識別対象は決まり切った文字列なので、機械学習を使わなくても、より正確な認識方法があるかもしれない。というか、私はApexにあまり詳しくないので、そもそもゲームシステム内で武器使用率を確認できる方法があるかもしれない。\n参考にした書籍・動画 機械学習については主にネット上の情報と、以下の書籍の内容を流し読みした程度の理解度である。\n ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装 オライリージャパン (2016/9/24) Amazon 楽天市場    また、上記書籍を数年前に読んだときは誤差逆伝播法がいまいち理解できなかった。しかし、Twitterのタイムラインで流れてきた以下の動画が非常に参考になり、とりあえず分かった気にはなれた。出来るだけ条件を簡易にして、高校数学の範囲で説明されている。\n まとめ 機械学習を利用してApexのプレイ動画から武器使用率を算出するためのツール群を作成した。解析結果には誤差が含まれるため、利用するためにはまだ工夫をする必要があると思う。\n色々試行錯誤をする中で、モデルの性能に最も寄与したのはデータセットの作り方だった。データセットに偏りがあると、それを学習したモデルも偏りのある結果を出力する。データセットの質の重要性を、身をもって実感した。この部分については本記事で書けなかったので、またどこかで記事を書くかもしれない。\n  Q1 2021 Live Game Streaming Trends - Stream Hatchet\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://kouya17.com/posts/44/","summary":"最終的な目標はプレイスタイルの解析 近年、動画投稿・配信サービスの普及により、ゲーム実況というものがエンタメの1カテゴリとして確立してきていると思う。感染症流行の影響が大きいと思うが、2021年1月～3月の主要ゲーム配信サイト(Twitch、YouTube Gaming、Facebook Gaming)の合計視聴時間が前年同期比で80%増加したというデータもある。1ゲームのジャンルとしてはアクション・RPG・シミュレーションなどあらゆるものが実況の対象になるが、今回はFPSのゲーム実況にスポットを当てる。\nFPSの分野では、オンライン対戦システムおよび、いわゆるランクシステムが整備されているものが多くなっている。ランクシステムは、プレイヤーの習熟度のようなものを可視化する。こうしたランクに関する情報は、視聴者にとって、プレイヤーのことをパッと理解するために非常に役立っていると思う。\n現状、ランク情報やダメージ数、キルデス比等はゲームシステムから参照できるようになっていることが多いと思う。ただ、個人的にはより細かい、プレイヤーのプレイスタイルのようなものを可視化できる情報があると、プレイヤーについて理解を深めるためのよい材料になると思った。\n最終的にはゲームシステムから直接確認出来るデータだけでなく、ゲームプレイ動画の解析結果から、プレイヤーのプレイスタイルをうまく可視化したい。今回は、Apexを題材にし、プレイヤーのプレイスタイルを示す1つの指標として、武器使用率をゲームプレイ動画から解析できないか試す。まあ、こういった情報はわざわざ動画を解析しなくても、ゲームシステム側で実装してもらえれば、より正確で容易にデータが取れるとは思う。\n全体の作業の流れ 今回の全体の作業の流れは以下のようになっている。\n各作業の内容について説明していく。ただし、ゲームプレイ動画のキャプチャ作業については特に説明することがないので、何も説明しない。\n武器名表示領域の切り取り データセットを整備するために、ゲーム画面のうち、武器名を表示している領域のみを切り取った画像を作成する必要がある。今回は、動画に対して一定時間ごとのフレームを画像化→特定領域を切り取り→特定の場所に画像ファイルとして保存、という流れでデータセット用の画像を作成した。\n一連の画像切り取り作業実施のために、ツールを作成した。GUI部分はPySimpleGUIを利用している。\n主に以下を設定できる。\n 切り取り対象とする動画の保存場所 武器1欄(*1)の画像を保存する場所 武器2欄(*1)の画像を保存する場所 画像を切り抜く領域(*2)(全体サイズに対する割合で指定) (*2)で設定した領域からどれだけずらした画像を何個保存するか 画像切り取り対象とするフレームの時間間隔  (*1)Apexでは、武器を同時に2つ持つことができる。今回は武器1欄と武器2欄を別々のデータセットとして扱う。理由はそちらの方がより精度が上がりそうと思ったためである。\nこのツールを使えば、ボタンを1回ポチーすることで、後は待っていれば所定の場所に武器名表示領域の画像が保存される。\n画像に対して正解ラベルを付ける データセットの正解ラベル付けについて、どのような形式で保存するのが一般的かを私は知らないのだが、今回は各正解ラベル(武器名)ごとに保存フォルダを分けることにした。\n以下のように、全武器+\u0026ldquo;武器なし\u0026rdquo;(_None)について画像をフォルダ分けした。\nこの作業にも専用ツールを作成した。\nデータセットが置かれている場所を指定して、startボタンを押すと、所定の個数づつ未分類の画像が表示される。表示された画像について、正解となる武器名のボタンをポチポチしていき、画像をフォルダ分けする。\nただ、今回データセットの元にしたキャプチャ動画は武器を決まった順番で使うようにしていた。そのため、切り抜かれた画像が最初から決まった武器順で並んでおり、結局このツールはあまり使わなかった。\n学習・モデルの保存 データセットの準備が出来たので、ようやく学習をする。学習部分はコードベースで説明する。\nまず、データセットの読み込みを行う。かなりコードが汚い。武器名のリストは、以下のコードに含まれていない関数を使ってファイルから読みこんでいる。\n Kerasを用いてモデルを生成する関数を作成する。\n Optunaを使ってハイパーパラメータを調整しつつ、成績の良いモデルを保存する。(といってもマシンが貧弱で学習に時間がかかるので、5回しか試行を回していない。)\n 上記のコードを実行したところ、とりあえず今回はテストデータの正解率が99.89%のモデルが出来た。テストデータの推論結果をMatplotlibを使って画像とともに確認してみる。\nいい感じに推論できてそうだ。誤答をしたデータについても確認してみる。\n今回は2つのデータの推論が間違っていたようだ。上は正答がFlatlineなのに対し、Devotionと推論してしまっており、下は正答がNoneなのに対し、R-301と推論してしまっている。\n作成したモデルによるプレイ動画の解析 モデルが出来たので、動画を解析する。解析用のソフトを作成した。\n学習用に使ったデータとは別のキャプチャ動画を使って解析したところ、以下のような結果になった。\n解析結果を見たところ、大体は合っていると思うが、誤検出されている武器名が末尾に並んでいる。解析結果は正確なものではないため、データの扱いは少し考えないといけない。\n最初は自作モデルを作らなくても行けると思っていた 今回、武器使用率算出のために\u0026quot;機械学習のモデルを作成・利用する\u0026quot;という手段を選択した。\nこの手段を取る前は、TesseractとPyOCRを利用して文字列を認識する方法をとっていた。ただ、認識精度が厳しかったので、利用を断念した。機械学習の勉強にもなると思い、代わりに自作モデルの作成という方法をとった。\n今回の識別対象は決まり切った文字列なので、機械学習を使わなくても、より正確な認識方法があるかもしれない。というか、私はApexにあまり詳しくないので、そもそもゲームシステム内で武器使用率を確認できる方法があるかもしれない。\n参考にした書籍・動画 機械学習については主にネット上の情報と、以下の書籍の内容を流し読みした程度の理解度である。\n ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装 オライリージャパン (2016/9/24) Amazon 楽天市場    また、上記書籍を数年前に読んだときは誤差逆伝播法がいまいち理解できなかった。しかし、Twitterのタイムラインで流れてきた以下の動画が非常に参考になり、とりあえず分かった気にはなれた。出来るだけ条件を簡易にして、高校数学の範囲で説明されている。\n まとめ 機械学習を利用してApexのプレイ動画から武器使用率を算出するためのツール群を作成した。解析結果には誤差が含まれるため、利用するためにはまだ工夫をする必要があると思う。\n色々試行錯誤をする中で、モデルの性能に最も寄与したのはデータセットの作り方だった。データセットに偏りがあると、それを学習したモデルも偏りのある結果を出力する。データセットの質の重要性を、身をもって実感した。この部分については本記事で書けなかったので、またどこかで記事を書くかもしれない。\n  Q1 2021 Live Game Streaming Trends - Stream Hatchet\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"機械学習を使ってApexのプレイ動画から武器使用率を算出する"},{"content":"先日、NT金沢2021というイベントに、リモートで出展参加した。「リモートで出展参加」というのは、具体的に言うと、展示物としてはオンラインで遊べるものをポスターで出展して、私はGoogleMeet(ビデオ会議)をつないでリモートで一部作品の説明をしていた。(関係機材は現地出展した友人に運んでもらった。)\nGoogleMeetをつなぐには、とりあえずPCが現地(NT金沢の会場)にあれば事足りる。ただせっかくなので、モニターが、話している相手の顔を追跡するようにした。\n動作の様子 人の顔に追従するモニタを試作中…。\n■使用素材\n画像：いらすとや(https://t.co/H01vPNj6QI) pic.twitter.com/H8fLDuSRjz\n\u0026mdash; 青木晃也 (@aoki_kouya) May 30, 2021  検証時の動画だが、概ね完成形。当日はこのモニターに、GoogleMeetの画面を表示して運用していた。\nハードウェア 使用部品一覧    部品名 備考     JetsonNano開発者キット 顔認識・ビデオ会議・サーボ制御用   Webカメラ×2 1個は顔認識用、1個はビデオ会議用   モニター スピーカーも内蔵   サーボモーター S03T/2BBMG/F×2 強そうなものをチョイス   PCA9685モジュール サーボ制御用    サーボやモニタ等を固定するための部品は3Dプリンターで作成した。\nWebカメラはとりあえずAmazonで安いものを適当に買った。1種類目はUSB Wi-Fi子機との相性が良くなかったようで、Wi-Fiの接続が不安定になった。2種類目は特に問題なく動いていそうだったので、採用した。ただ、この記事を書いている時点で、採用したWebカメラのAmazonの商品ページはリンク切れになっていた。\nソフトウェア ソフトウェアに関する部分としては、JetsonNano起動後にコンソールで顔追従用プログラムを実行した後、ブラウザ(Chromium)でGoogleMeetを動かしていた。GoogleMeetの方は特に書くことがないため、顔追従用プログラムのソースコード及びセットアップ周りを書いていく。\n顔追従用プログラムのソース ソースは以下に置いてある。\n 顔追従用プログラムのセットアップ周り 上記のソースを動かすには、関係ライブラリをインストールする必要がある。\nPCA9685用ライブラリのインストール サーボ制御モジュールPCA9685をPythonから制御するためのPythonパッケージをインストールする。まず以下のコマンドでpip3をインストールする。\nsudo apt-get install python3-pip 参考元: NVIDIA Jetson Nano 開発者キットに TensorFlow をインストールする - Qiita\n以下のコマンドでPythonパッケージAdafruit_PCA9685をインストールする。\npip3 install Adafruit_PCA9685 --user 参考元: Jetson nanoとPCA9685でサーボを動かそうとするときのI2Cエラー対処法！ - KOKENSHAの技術ブログ\nTensorFlowのインストール 今回はマスクを付けている顔も認識するため、以下のモデルを利用した。ライセンスはMIT。\n このモデルを利用するためにはTensorFlowをインストールする必要がある。\n利用しているJetPack SDKのバージョンがJetPack4.5でかつ、pipインストール済みの場合は以下コマンドを実行。\nsudo apt-get update sudo apt-get install libhdf5-serial-dev hdf5-tools libhdf5-dev zlib1g-dev zip libjpeg8-dev liblapack-dev libblas-dev gfortran sudo pip3 install -U numpy==1.19.4 future==0.18.2 mock==3.0.5 h5py==2.10.0 keras_preprocessing==1.1.1 keras_applications==1.0.8 gast==0.2.2 futures protobuf pybind11 sudo pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v45 tensorflow 参考元: Installing TensorFlow For Jetson Platform :: NVIDIA Deep Learning Frameworks Documentation\npip3 install -U pipするとよくないという記事があったのでそこらへんはやめた。\n参考元: Jetson Nanoでディープラーニング - Qiita\n運用してみての反省点 安いWebカメラのマイクでは音質がかなり悪く、モニターに付属のスピーカーだと音量が足りなかった。広くて周りに音が出る物がある会場でWeb会議をやるには、マイクスピーカーのようなものを別途用意した方がいいと思った。\nアフィリエイト\n NVIDIA Jetson Nano 2GB 開発者キット NVIDIA Amazon   ","permalink":"https://kouya17.com/posts/43/","summary":"先日、NT金沢2021というイベントに、リモートで出展参加した。「リモートで出展参加」というのは、具体的に言うと、展示物としてはオンラインで遊べるものをポスターで出展して、私はGoogleMeet(ビデオ会議)をつないでリモートで一部作品の説明をしていた。(関係機材は現地出展した友人に運んでもらった。)\nGoogleMeetをつなぐには、とりあえずPCが現地(NT金沢の会場)にあれば事足りる。ただせっかくなので、モニターが、話している相手の顔を追跡するようにした。\n動作の様子 人の顔に追従するモニタを試作中…。\n■使用素材\n画像：いらすとや(https://t.co/H01vPNj6QI) pic.twitter.com/H8fLDuSRjz\n\u0026mdash; 青木晃也 (@aoki_kouya) May 30, 2021  検証時の動画だが、概ね完成形。当日はこのモニターに、GoogleMeetの画面を表示して運用していた。\nハードウェア 使用部品一覧    部品名 備考     JetsonNano開発者キット 顔認識・ビデオ会議・サーボ制御用   Webカメラ×2 1個は顔認識用、1個はビデオ会議用   モニター スピーカーも内蔵   サーボモーター S03T/2BBMG/F×2 強そうなものをチョイス   PCA9685モジュール サーボ制御用    サーボやモニタ等を固定するための部品は3Dプリンターで作成した。\nWebカメラはとりあえずAmazonで安いものを適当に買った。1種類目はUSB Wi-Fi子機との相性が良くなかったようで、Wi-Fiの接続が不安定になった。2種類目は特に問題なく動いていそうだったので、採用した。ただ、この記事を書いている時点で、採用したWebカメラのAmazonの商品ページはリンク切れになっていた。\nソフトウェア ソフトウェアに関する部分としては、JetsonNano起動後にコンソールで顔追従用プログラムを実行した後、ブラウザ(Chromium)でGoogleMeetを動かしていた。GoogleMeetの方は特に書くことがないため、顔追従用プログラムのソースコード及びセットアップ周りを書いていく。\n顔追従用プログラムのソース ソースは以下に置いてある。\n 顔追従用プログラムのセットアップ周り 上記のソースを動かすには、関係ライブラリをインストールする必要がある。\nPCA9685用ライブラリのインストール サーボ制御モジュールPCA9685をPythonから制御するためのPythonパッケージをインストールする。まず以下のコマンドでpip3をインストールする。\nsudo apt-get install python3-pip 参考元: NVIDIA Jetson Nano 開発者キットに TensorFlow をインストールする - Qiita","title":"顔認識+サーボモーターで顔追従モニターの作成"},{"content":"以下の記事を参考にさせていただき、Dartを使ってブロックチェーン(のようなもの)を実装してみた。\n 動作の様子 お試しノードをHeroku上で動かしている。https://dart-blockchain-test-app.herokuapp.com/publicにアクセスすることで、 お試しノードの情報を色々見る事ができる。\nブロックチェーンは特にファイル等で永続的に残しているわけではない。Herokuはしばらくアクセスがないとアプリが停止するため、そのタイミングで情報がリセットされる。\nソースコードとローカルでのノードの立て方 ソースコードは以下に置いてある。設計部分は参考元サイトとほぼほぼ同じになっている。\n ローカルPCでノードを立てる場合は、Dartの実行環境を整えた状態で、ソースコードをダウンロードし、以下のコマンドを実行すれば良い。\ndart pub get dart pub global activate webdev webdev build --output web:public dart bin/back.dart --port 6565 上記コマンドを実行後、ブラウザでhttp://localhost:6565/publicにアクセスすると、ノードの情報を確認できる。\nbin/back.dartを実行する際、--peerオプションで、P2P通信を行うノードを指定できる。Heroku上のお試しノードを立ち上げた状態で、以下のようにbin/back.dartを実行すれば、Heroku上のノードとの取引を行うこともできる。\ndart bin/back.dart --port 6565 --peer ws://dart-blockchain-test-app.herokuapp.com/ws ローカルPC上のノードとHeroku上のノードはWebSocketでP2P通信を実現しており、簡単な構成図を書くと以下のようになる。\nHerokuでDartアプリを動かす 今回Heroku上でDartアプリを実行できるようにした。Heroku上の設定周りは以下のリポジトリを利用させていただいた。\n なお、上記リポジトリのREADME中にはheroku config:add BUILDPACK_URL=https://github.com/igrigorik/heroku-buildpack-dart.gitというコマンドでビルドパックを適用するよう書いてある。しかし、以下のプルリクエストにある通り、今はheroku buildpacks:set https://github.com/igrigorik/heroku-buildpack-dart.gitというコマンドでビルドパックを登録するのが正しいらしい。\n とりあえず形にはなったが、まだまだ不明点がある 今回、ブロックチェーン(みたいなもの)を実装することで、ブロックチェーンへの理解が深まった。ただ、実装していく過程で、Bitcoinに対する疑問点が新たに出てきた。\n マイナーへの報酬は普通のトランザクションと比べて特殊な形式になると思うが、どのような形式になっているか。 大体のネット上の記事では\u0026quot;悪意のあるブロックチェーンを生成するには、善意のブロックチェーンよりも長いチェーンを生成する必要があるため困難\u0026quot;という説明がされている。difficultyを改竄できれば、計算能力が乏しくても、いくらでも長い悪意のあるブロックチェーンを生成できそうだが、そこら辺はどのように回避しているか。 P2P通信をどのように実現しているか。  アフィリエイト\n 絵で見てわかるブロックチェーンの仕組み 翔泳社 (2020/12/21) Amazon Kindle 楽天ブックス 楽天Kobo   ","permalink":"https://kouya17.com/posts/42/","summary":"以下の記事を参考にさせていただき、Dartを使ってブロックチェーン(のようなもの)を実装してみた。\n 動作の様子 お試しノードをHeroku上で動かしている。https://dart-blockchain-test-app.herokuapp.com/publicにアクセスすることで、 お試しノードの情報を色々見る事ができる。\nブロックチェーンは特にファイル等で永続的に残しているわけではない。Herokuはしばらくアクセスがないとアプリが停止するため、そのタイミングで情報がリセットされる。\nソースコードとローカルでのノードの立て方 ソースコードは以下に置いてある。設計部分は参考元サイトとほぼほぼ同じになっている。\n ローカルPCでノードを立てる場合は、Dartの実行環境を整えた状態で、ソースコードをダウンロードし、以下のコマンドを実行すれば良い。\ndart pub get dart pub global activate webdev webdev build --output web:public dart bin/back.dart --port 6565 上記コマンドを実行後、ブラウザでhttp://localhost:6565/publicにアクセスすると、ノードの情報を確認できる。\nbin/back.dartを実行する際、--peerオプションで、P2P通信を行うノードを指定できる。Heroku上のお試しノードを立ち上げた状態で、以下のようにbin/back.dartを実行すれば、Heroku上のノードとの取引を行うこともできる。\ndart bin/back.dart --port 6565 --peer ws://dart-blockchain-test-app.herokuapp.com/ws ローカルPC上のノードとHeroku上のノードはWebSocketでP2P通信を実現しており、簡単な構成図を書くと以下のようになる。\nHerokuでDartアプリを動かす 今回Heroku上でDartアプリを実行できるようにした。Heroku上の設定周りは以下のリポジトリを利用させていただいた。\n なお、上記リポジトリのREADME中にはheroku config:add BUILDPACK_URL=https://github.com/igrigorik/heroku-buildpack-dart.gitというコマンドでビルドパックを適用するよう書いてある。しかし、以下のプルリクエストにある通り、今はheroku buildpacks:set https://github.com/igrigorik/heroku-buildpack-dart.gitというコマンドでビルドパックを登録するのが正しいらしい。\n とりあえず形にはなったが、まだまだ不明点がある 今回、ブロックチェーン(みたいなもの)を実装することで、ブロックチェーンへの理解が深まった。ただ、実装していく過程で、Bitcoinに対する疑問点が新たに出てきた。\n マイナーへの報酬は普通のトランザクションと比べて特殊な形式になると思うが、どのような形式になっているか。 大体のネット上の記事では\u0026quot;悪意のあるブロックチェーンを生成するには、善意のブロックチェーンよりも長いチェーンを生成する必要があるため困難\u0026quot;という説明がされている。difficultyを改竄できれば、計算能力が乏しくても、いくらでも長い悪意のあるブロックチェーンを生成できそうだが、そこら辺はどのように回避しているか。 P2P通信をどのように実現しているか。  アフィリエイト\n 絵で見てわかるブロックチェーンの仕組み 翔泳社 (2020/12/21) Amazon Kindle 楽天ブックス 楽天Kobo   ","title":"Dartでブロックチェーン(のようなもの)を実装してみた"},{"content":"ただのにわかではあるが、ブロックチェーンは信頼性が重要な技術だと思っている。傍目から見たら、これだけ広く不特定多数に使われていても、これまで特に技術自体について大きく信頼性が揺らぐような事象は起きていないように見える。実用に十分耐えうる技術のようだ。なんとなく興味が出てきたのでゴールデンウィークの期間を使って色々勉強してみる。\nBitcoin Coreのインストール まずは暗号通貨Bitcoinを題材に、ブロックチェーンの\u0026quot;ブロック\u0026quot;の中身を見てみる。現在までのブロックを全て取得するために、以下を参考にBitcoin Coreをインストールする。\n 初回起動時はデータ保存場所を聞かれるが、デフォルトの場所を使うようにする。\n起動後はブロックチェーンデータの同期が始まる。\n完全な同期までは数日かかるらしい。自分の場合は3日ほど放置していたら完了していた。\nブロックの中身を読んでみる デフォルトだとMacOSは/Users/\u0026lt;username\u0026gt;/Library/Application Support/Bitcoin/blocks/以下にブロックデータが置かれるらしい。データの総サイズを確認してみる。\n$ du -hs /Users/\u0026lt;username\u0026gt;/Library/Application\\ Support/Bitcoin/blocks/ 361G\t/Users/\u0026lt;username\u0026gt;/Library/Application Support/Bitcoin/blocks/ サイズがだいぶでかい。これはこれからもトランザクションが発生するごとに増えていくのだろう。\n中身はバイナリになっている。フォーマットは以下のようになっているらしい。\n   フィールド サイズ     magic bytes 4 bytes   size 4 bytes   block header 80 bytes   tx count 可変   transaction date 可変    引用元\n 上記引用元サイトの情報を参考に、バイナリを読んでみる。(といっても、バイナリを読まなくても、上記引用元サイトに、必要な情報は全て書かれているので、ほぼ上記サイトの内容をかいつまんで日本語訳するような内容になる。)\n$ hexdump -C -n 293 blk00000.dat 00000000 f9 be b4 d9 1d 01 00 00 01 00 00 00 00 00 00 00 |................| 00000010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| 00000020 00 00 00 00 00 00 00 00 00 00 00 00 3b a3 ed fd |............;...| 00000030 7a 7b 12 b2 7a c7 2c 3e 67 76 8f 61 7f c8 1b c3 |z{..z.,\u0026gt;gv.a....| 00000040 88 8a 51 32 3a 9f b8 aa 4b 1e 5e 4a 29 ab 5f 49 |..Q2:...K.^J)._I| 00000050 ff ff 00 1d 1d ac 2b 7c 01 01 00 00 00 01 00 00 |......+|........| 00000060 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| 00000070 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ff ff |................| 00000080 ff ff 4d 04 ff ff 00 1d 01 04 45 54 68 65 20 54 |..M.......EThe T| 00000090 69 6d 65 73 20 30 33 2f 4a 61 6e 2f 32 30 30 39 |imes 03/Jan/2009| 000000a0 20 43 68 61 6e 63 65 6c 6c 6f 72 20 6f 6e 20 62 | Chancellor on b| 000000b0 72 69 6e 6b 20 6f 66 20 73 65 63 6f 6e 64 20 62 |rink of second b| 000000c0 61 69 6c 6f 75 74 20 66 6f 72 20 62 61 6e 6b 73 |ailout for banks| 000000d0 ff ff ff ff 01 00 f2 05 2a 01 00 00 00 43 41 04 |........*....CA.| 000000e0 67 8a fd b0 fe 55 48 27 19 67 f1 a6 71 30 b7 10 |g....UH'.g..q0..| 000000f0 5c d6 a8 28 e0 39 09 a6 79 62 e0 ea 1f 61 de b6 |\\..(.9..yb...a..| 00000100 49 f6 bc 3f 4c ef 38 c4 f3 55 04 e5 1e c1 12 de |I..?L.8..U......| 00000110 5c 38 4d f7 ba 0b 8d 57 8a 4c 70 2b 6b f1 1d 5f |\\8M....W.Lp+k.._| 00000120 ac 00 00 00 00 |.....| 最初の4バイト(f9 be b4 d9)がmagic bytes。このmagic bytesはどのネットワーク上のメッセージかによって異なる値をとるようだ。メインネットの場合はf9 be b4 d9となる。magic bytesはメッセージの先頭を識別するために存在しているらしい。\n次の4バイト(1d 01 00 00)がサイズ。hexdumpコマンドはリトルエンディアンで出力するので、エンディアン変換をして00 00 01 1d。16進数から10進数に変換すると285になる。つまり、この後285バイト分データが続く。\n次の80バイトがブロックヘッダー。このヘッダーは以下のような構造になっているらしい。また、\u0026ldquo;値\u0026quot;列に今回見てみたブロックの情報を記載している。\n   フィールド サイズ 値     Version 4 bytes 01 00 00 00   Previous Block Hash 32 bytes 00 00 00 00 \u0026hellip; 00 00 00 00   Merkle Root 32 bytes 3b a3 ed fd \u0026hellip; 4b 1e 5e 4a   Time 4 bytes 29 ab 5f 49   Bits 4 bytes ff ff 00 1d   Nonce 4 bytes 1d ac 2b 7c    このヘッダーの詳細について調べるとそれだけで1つの記事になりそうなので、今回は詳細については書かないことにする。気になるのはPrevious Block Hashが全て0であることだ。これが最初のブロックということなのだろうか。\n次の1バイト(01)がtx count。このブロックには取引が1つしか含まれていないようなので、tx countは01のみ。\n以降のデータが取引データ。この取引データは以下のような構造になっているらしい。\n   フィールド サイズ     Version 4 bytes   Input Count 可変   Input(s) 可変   Output Count 可変   Output(s) 可変   Locktime 4 bytes    さらにInputとOutputはそれぞれ構造をもつが、Inputは今回無視する。Outputの構造は以下の通り。\n   フィールド サイズ     Value 8 bytes   ScriptPubKey Size 可変   ScriptPubKey 可変(ScriptPubKey Sizeで指定)    今回見てみたブロックだと、OutputのValueは00 f2 05 2a 01 00 00 00になっている。エンディアン変換をして00 00 01 2a 05 f2 00。10進数に直すと5,000,000,000になる。つまりこのブロックは5,000,000,000 Satoshi = 50 BTCを出力している。\nブロックの中身を読んでみての感触 以上のように、Bitcoinブロックチェーンのブロックの中身を少し見てみた。初めはデータの構造が大雑把に分かればいいかなという程度だったのだが、今回参考にしたサイトが詳しすぎた。想定していた以上の情報があり、かなり参考になった。ブロックチェーンについてだけでなく、サイトの構成も、ページ間のリンクやマウスオーバーによる表示など、かなり作り込まれていて参考になった。\nアフィリエイト\n 絵で見てわかるブロックチェーンの仕組み 翔泳社 (2020/12/21) Amazon Kindle 楽天ブックス 楽天Kobo   ","permalink":"https://kouya17.com/posts/41/","summary":"ただのにわかではあるが、ブロックチェーンは信頼性が重要な技術だと思っている。傍目から見たら、これだけ広く不特定多数に使われていても、これまで特に技術自体について大きく信頼性が揺らぐような事象は起きていないように見える。実用に十分耐えうる技術のようだ。なんとなく興味が出てきたのでゴールデンウィークの期間を使って色々勉強してみる。\nBitcoin Coreのインストール まずは暗号通貨Bitcoinを題材に、ブロックチェーンの\u0026quot;ブロック\u0026quot;の中身を見てみる。現在までのブロックを全て取得するために、以下を参考にBitcoin Coreをインストールする。\n 初回起動時はデータ保存場所を聞かれるが、デフォルトの場所を使うようにする。\n起動後はブロックチェーンデータの同期が始まる。\n完全な同期までは数日かかるらしい。自分の場合は3日ほど放置していたら完了していた。\nブロックの中身を読んでみる デフォルトだとMacOSは/Users/\u0026lt;username\u0026gt;/Library/Application Support/Bitcoin/blocks/以下にブロックデータが置かれるらしい。データの総サイズを確認してみる。\n$ du -hs /Users/\u0026lt;username\u0026gt;/Library/Application\\ Support/Bitcoin/blocks/ 361G\t/Users/\u0026lt;username\u0026gt;/Library/Application Support/Bitcoin/blocks/ サイズがだいぶでかい。これはこれからもトランザクションが発生するごとに増えていくのだろう。\n中身はバイナリになっている。フォーマットは以下のようになっているらしい。\n   フィールド サイズ     magic bytes 4 bytes   size 4 bytes   block header 80 bytes   tx count 可変   transaction date 可変    引用元\n 上記引用元サイトの情報を参考に、バイナリを読んでみる。(といっても、バイナリを読まなくても、上記引用元サイトに、必要な情報は全て書かれているので、ほぼ上記サイトの内容をかいつまんで日本語訳するような内容になる。)\n$ hexdump -C -n 293 blk00000.dat 00000000 f9 be b4 d9 1d 01 00 00 01 00 00 00 00 00 00 00 |.","title":"Bitcoinのブロックチェーンの中身を見てみる"},{"content":"できたもの 動画の出来が色々とアレだが、動作時の様子を以下の動画の後半で確認できる。\n 動画だと映りが悪いが、写真だとLEDをつけた時の様子は以下のような感じになる。\n作成目的 M5系列からYouTube Data APIを使った作例を作ってみたくて、作成した。\n使用部品  M5StickC　1個 PCA9685 16チャンネル PWMモジュール　3個 フルカラーLED　12個  筐体設計 STLデータは以下に公開してある。\n M5StickCとPCA9685モジュールが3個入るような箱を設計した。最初はだいぶコンパクトに設計していたのだが、配線が箱に収まらないことが分かり、途中で拡張した。もともとはM5StickCも箱の中に収める想定だったのだが、入らなかったので、横につけることにした。\nフルカラーLEDは配置する場所に自由度を持たせられるように、個別に格納する箱を作成して、それぞれ4本の配線でM5StickC側の箱と接続するようにした。\n配線 今回の配線の概要図を以下に示す。\nPCA9685モジュールとフルカラーLEDの接続部分はテキトーになっているが、LEDについてはRGB順で1個目のLEDは1個目のPCA9685モジュールの0,1,2chに接続、2個目のLEDは1個目のPCA9685モジュールの3,4,5chに接続…という形で接続している。\nここが一番しんどかった。PCA9685モジュールを3個使っているのでやろうと思えば最大16個のフルカラーLEDを接続できるが、12個配線したところで力尽きた。\nソフト実装 ソフト全体は以下のリポジトリに置いてある。\n src/main.cppについて以下の部分は各ユーザー毎に書き換える必要がある。\n APIキー アクセスポイントのSSID アクセスポイントのパスワード  あと、src/main.cpp中のchannelsをいじれば対象とするYouTubeチャンネルとLEDの色を変更できる。\nM5StickCからYouTube Data APIを利用する部分は、以下のAPIラッパーを利用させていただいた。\n ただし、このままではチャンネル情報しか取得できないため、以下の関数等を追加している。\n std::vector\u0026lt;String\u0026gt; YoutubeApi::getUpcomingBroadcasts(char *channelId)  指定されたChannelIDのチャンネルの配信予定(VideoID)を取得する。   BroadcastDetails YoutubeApi::getBroadcastDetails(char *videoId)  指定されたVideoIDの動画の配信詳細情報を取得する。 配信開始予定時刻は詳細情報からしか取れない。    課題 きちんと検証はしていないが、現状のソースだと、YouTube Data APIのクォータを異常に消費する。本当は1分ごとに情報取得とかをしたかったが、無料枠ではクォータが足りなくなる。\n各LEDに紐づくチャンネルIDと、LEDの色がハードコーディングされているので、外部から設定できるようにしたい。\nアフィリエイト\n M5Stack M5StickC ESP32ミニIoT開発ボードm5stack iotキット フィンガーコンピューターカラー0.96インチLCD Vivan-Star Amazon   ","permalink":"https://kouya17.com/posts/40/","summary":"できたもの 動画の出来が色々とアレだが、動作時の様子を以下の動画の後半で確認できる。\n 動画だと映りが悪いが、写真だとLEDをつけた時の様子は以下のような感じになる。\n作成目的 M5系列からYouTube Data APIを使った作例を作ってみたくて、作成した。\n使用部品  M5StickC　1個 PCA9685 16チャンネル PWMモジュール　3個 フルカラーLED　12個  筐体設計 STLデータは以下に公開してある。\n M5StickCとPCA9685モジュールが3個入るような箱を設計した。最初はだいぶコンパクトに設計していたのだが、配線が箱に収まらないことが分かり、途中で拡張した。もともとはM5StickCも箱の中に収める想定だったのだが、入らなかったので、横につけることにした。\nフルカラーLEDは配置する場所に自由度を持たせられるように、個別に格納する箱を作成して、それぞれ4本の配線でM5StickC側の箱と接続するようにした。\n配線 今回の配線の概要図を以下に示す。\nPCA9685モジュールとフルカラーLEDの接続部分はテキトーになっているが、LEDについてはRGB順で1個目のLEDは1個目のPCA9685モジュールの0,1,2chに接続、2個目のLEDは1個目のPCA9685モジュールの3,4,5chに接続…という形で接続している。\nここが一番しんどかった。PCA9685モジュールを3個使っているのでやろうと思えば最大16個のフルカラーLEDを接続できるが、12個配線したところで力尽きた。\nソフト実装 ソフト全体は以下のリポジトリに置いてある。\n src/main.cppについて以下の部分は各ユーザー毎に書き換える必要がある。\n APIキー アクセスポイントのSSID アクセスポイントのパスワード  あと、src/main.cpp中のchannelsをいじれば対象とするYouTubeチャンネルとLEDの色を変更できる。\nM5StickCからYouTube Data APIを利用する部分は、以下のAPIラッパーを利用させていただいた。\n ただし、このままではチャンネル情報しか取得できないため、以下の関数等を追加している。\n std::vector\u0026lt;String\u0026gt; YoutubeApi::getUpcomingBroadcasts(char *channelId)  指定されたChannelIDのチャンネルの配信予定(VideoID)を取得する。   BroadcastDetails YoutubeApi::getBroadcastDetails(char *videoId)  指定されたVideoIDの動画の配信詳細情報を取得する。 配信開始予定時刻は詳細情報からしか取れない。    課題 きちんと検証はしていないが、現状のソースだと、YouTube Data APIのクォータを異常に消費する。本当は1分ごとに情報取得とかをしたかったが、無料枠ではクォータが足りなくなる。\n各LEDに紐づくチャンネルIDと、LEDの色がハードコーディングされているので、外部から設定できるようにしたい。\nアフィリエイト\n M5Stack M5StickC ESP32ミニIoT開発ボードm5stack iotキット フィンガーコンピューターカラー0.","title":"所定のYouTubeチャンネルの配信予定をLEDで通知する(M5StickC)"},{"content":"CSSによるアニメーションについて CSSではanimationプロパティを利用することによって、JavaScriptを使うことなく、HTML要素にアニメーションを付与することができる。\nCSSアニメーションの概要は以下の記事を参考にさせていただいた。\n 今回作りたいもの 今回、以下のような流れでアニメーションする要素を作りたい。\n 要素の左端から背景色がだんだん元々の背景色Aから色B変わっていき、背景全体が色Bになる。この時、色Aと色Bの境界はグラデーションにする。 背景全体が色Bのまましばらく維持する。 要素の左端から背景色がだんだん色Bから色Aに変わっていき、背景全体が色Aにもどる。この時、色Bと色Aの境界はグラデーションにする。  少し調べてみると、グラデーションは\u0026lt;gradient\u0026gt;データ型を利用すれば問題なさそうだった。問題は、グラデーションの境界位置をアニメーションする部分である。以下のように、ただ単に0%と100%のkeyframeを指定しただけでは、思ったようなアニメーションにならなかった。\nSee the Pen wrong case by kouya17 (@kouya17) on CodePen.  私の知識ではスマートなやり方が分からなかったので、keyframeを0%から100%まですべて指定する形で(無理やり)実装することにした。 keyframes生成用のpythonスクリプトを作成する pythonで、以下のようなkeyframes生成用のプログラムを作成した。\n このスクリプトによって生成したテキストをkeyframesとしてCSSに貼り付けると、以下のようなアニメーションになる。\nSee the Pen linear by kouya17 (@kouya17) on CodePen.  とりあえずやりたいことは出来た。しかし、完成したアニメーションを見ると、動きにメリハリがない。アニメーションの進行具合はCSSの`animation`要素の`animation-timing-function`プロパティを設定すればいじることができる。 ただ、今回のパターンだと、うまく設定できないようだ。下の例はanimation-timing-functionプロパティをlinearとease-in-outにそれぞれ設定したものを並べた例である。それぞれのアニメーションの進行具合に違いが生じるはずだが、特に差がない。\nSee the Pen linear and ease-in-out by kouya17 (@kouya17) on CodePen.  先ほど作成したpythonスクリプトを修正して、イージングの要素も取り入れることにする。 イージングの要素を実装する イージングの要素を実装するには、3次ベジェ曲線について把握しておく必要がある。ベジェ曲線については以下の記事を参考にさせていただいた。\n pythonでベジェ曲線生成クラスを作成する pythonでベジェ曲線を生成するためのクラスを以下のように作成した。\n 上のスクリプトを実行すると p0=[0,0], p1=[0,1], p2=[1,0], p3=[1,1] とした時の3次ベジェ曲線が生成される。グラフにプロットすると以下のような曲線になる。\nkeyframes生成用のpythonスクリプトを修正する 先ほど作成したkeyframes生成用のスクリプトにベジェ曲線生成用クラスを組み込む。組み込んだ後のスクリプトは以下のようになる。\n このスクリプトを使って、 p0=[0,0], p1=[0,0.5], p2=[1,0.5], p3=[1,1] のときのベジェ曲線に従うようにアニメーションの進行速度を調整したkeyframesを生成した。生成されたkeyframesをCSSに貼り付けると、以下のようなアニメーションになる。\nSee the Pen bezier by kouya17 (@kouya17) on CodePen.  正直元々長いアニメーションではないので、変化が分かりずらいが、少し動きにメリハリが付いたような気がする。ここまでで、自分のやりたかったことが一応すべて実装できた。 アフィリエイト\n 1冊ですべて身につくHTML \u0026 CSSとWebデザイン入門講座 SBクリエイティブ (2019/3/16) Amazon Kindle 楽天ブックス 楽天Kobo   ","permalink":"https://kouya17.com/posts/39/","summary":"CSSによるアニメーションについて CSSではanimationプロパティを利用することによって、JavaScriptを使うことなく、HTML要素にアニメーションを付与することができる。\nCSSアニメーションの概要は以下の記事を参考にさせていただいた。\n 今回作りたいもの 今回、以下のような流れでアニメーションする要素を作りたい。\n 要素の左端から背景色がだんだん元々の背景色Aから色B変わっていき、背景全体が色Bになる。この時、色Aと色Bの境界はグラデーションにする。 背景全体が色Bのまましばらく維持する。 要素の左端から背景色がだんだん色Bから色Aに変わっていき、背景全体が色Aにもどる。この時、色Bと色Aの境界はグラデーションにする。  少し調べてみると、グラデーションは\u0026lt;gradient\u0026gt;データ型を利用すれば問題なさそうだった。問題は、グラデーションの境界位置をアニメーションする部分である。以下のように、ただ単に0%と100%のkeyframeを指定しただけでは、思ったようなアニメーションにならなかった。\nSee the Pen wrong case by kouya17 (@kouya17) on CodePen.  私の知識ではスマートなやり方が分からなかったので、keyframeを0%から100%まですべて指定する形で(無理やり)実装することにした。 keyframes生成用のpythonスクリプトを作成する pythonで、以下のようなkeyframes生成用のプログラムを作成した。\n このスクリプトによって生成したテキストをkeyframesとしてCSSに貼り付けると、以下のようなアニメーションになる。\nSee the Pen linear by kouya17 (@kouya17) on CodePen.  とりあえずやりたいことは出来た。しかし、完成したアニメーションを見ると、動きにメリハリがない。アニメーションの進行具合はCSSの`animation`要素の`animation-timing-function`プロパティを設定すればいじることができる。 ただ、今回のパターンだと、うまく設定できないようだ。下の例はanimation-timing-functionプロパティをlinearとease-in-outにそれぞれ設定したものを並べた例である。それぞれのアニメーションの進行具合に違いが生じるはずだが、特に差がない。\nSee the Pen linear and ease-in-out by kouya17 (@kouya17) on CodePen.  先ほど作成したpythonスクリプトを修正して、イージングの要素も取り入れることにする。 イージングの要素を実装する イージングの要素を実装するには、3次ベジェ曲線について把握しておく必要がある。ベジェ曲線については以下の記事を参考にさせていただいた。\n pythonでベジェ曲線生成クラスを作成する pythonでベジェ曲線を生成するためのクラスを以下のように作成した。\n 上のスクリプトを実行すると p0=[0,0], p1=[0,1], p2=[1,0], p3=[1,1] とした時の3次ベジェ曲線が生成される。グラフにプロットすると以下のような曲線になる。\nkeyframes生成用のpythonスクリプトを修正する 先ほど作成したkeyframes生成用のスクリプトにベジェ曲線生成用クラスを組み込む。組み込んだ後のスクリプトは以下のようになる。\n このスクリプトを使って、 p0=[0,0], p1=[0,0.","title":"html要素の背景色が変化するアニメーションをCSSで(無理やり)実装する"},{"content":"環境についてざっくり クライアント側  ブラウザ: Google Chrome バージョン 87.0.4280.141 (Official Build) (64ビット) OS: Windows 10 Home 19041.746 HTTPクライアント: axios v0.21.1  サーバ側  OS: Raspbian GNU/Linux 10 (buster) webフレームワーク: actix-web v3.3.2  現象 Access to XMLHttpRequest ... has been blocked by CORS policyというエラーが出力される Raspberry Pi 4 Model B上でバックエンド(actix-web)とフロントエンド(Vue CLI)を稼働させている。WindowsPCからブラウザを使ってフロントエンドにアクセスし、axiosを使ってバックエンド(actix-web)にGETリクエストを実行した。すると、ブラウザのコンソールに以下のようなエラーが出力された。(ローカルIPが記述されていたところは一部編集している。)\nAccess to XMLHttpRequest at \u0026#39;http://\u0026lt;localIP\u0026gt;:50001/\u0026#39; from origin \u0026#39;http://\u0026lt;localIP\u0026gt;:8080\u0026#39; has been blocked by CORS policy: No \u0026#39;Access-Control-Allow-Origin\u0026#39; header is present on the requested resource. 問題発生時のソースコード この時のサーバ側(actix-web)のコードは以下の通り。(ローカルIPとメールアドレスが記述されていたところは一部編集している。)\nmain.rs\n Cargo.toml\n この時クライアント側はJavaScriptから以下のようにaxiosを実行していた。(ローカルIPが記述されていたところは一部編集している。)\n 調査及び解決方法 actix_corsを利用し、GETリクエストについてはエラーが解消した エラーログを見ると、CORSポリシーによって、リソースへのアクセスがブロックされているようだ。CORSポリシーに対応するため、サーバ側のプログラムを修正していく。web上で\u0026quot;actix CORS\u0026quot;あたりで調べてみる。すると、actix_corsというcrateがあった。actix_corsを使うために、main.rsとCargo.tomlについて、以下のように修正した。(ローカルIPが記述されていたところは一部編集している。)\nmain.rs\n+ use actix_cors::Cors; use actix_web::{get, post, web, App, HttpResponse, HttpServer, Responder}; ... HttpServer::new(|| { + let cors = Cors::default() + .allowed_origin(\u0026#34;http://\u0026lt;localIP\u0026gt;:8080\u0026#34;) + .allowed_methods(vec![\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;]); + App::new().wrap(cors).service(index).service(post_operations) - App::new().service(index).service(post_operations) }) Cargo.toml\n[dependencies] actix-web = \u0026#34;3\u0026#34; serde = { version = \u0026#34;1.0\u0026#34;, features = [\u0026#34;derive\u0026#34;] } serde_json = \u0026#34;1.0\u0026#34; + actix-cors = \u0026#34;0.5.4\u0026#34; この状態で再度確認してみた。GETリクエストは正常に処理されるようになった。しかし、POSTリクエストはまだCORSエラーが出る状態だった。なお、ブラウザ経由ではなく、Postmanを利用してリクエストを投げた場合、特にエラーなくレスポンスが返ってきた。\nデバッグログを確認したところ、ブラウザからPOSTする前にOPTIONSリクエストが実行されていた ブラウザを経由したPOSTリクエストのみエラーが出続ける問題について、web上で調べても自分の検索力では解決のための情報を見つけられなかった。そこで、サーバ側のデバッグログを出力し、その出力から原因を調べることにした。\nデバッグログ出力のためにactix_web::middleware::Loggerを使用した。main.rsとCargo.tomlについて、以下のように修正した。(ローカルIPが記述されていたところは一部編集している。)\nmain.rs\n+ extern crate env_logger; use actix_cors::Cors; use actix_web::{get, post, web, App, HttpResponse, HttpServer, Responder}; use serde::{Deserialize, Serialize}; + use actix_web::middleware::Logger; ... async fn main() -\u0026gt; std::io::Result\u0026lt;()\u0026gt; { + std::env::set_var(\u0026#34;RUST_LOG\u0026#34;, \u0026#34;actix_web=debug\u0026#34;); + env_logger::init(); HttpServer::new(|| { let cors = Cors::default() .allowed_origin(\u0026#34;http://\u0026lt;localIP\u0026gt;:8080\u0026#34;) .allowed_methods(vec![\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;]); + App::new().wrap(cors).wrap(Logger::default()).service(index).service(post_operations) - App::new().wrap(cors).service(index).service(post_operations) }) Cargo.toml\n[dependencies] actix-web = \u0026#34;3\u0026#34; serde = { version = \u0026#34;1.0\u0026#34;, features = [\u0026#34;derive\u0026#34;] } serde_json = \u0026#34;1.0\u0026#34; actix-cors = \u0026#34;0.5.4\u0026#34; + env_logger = \u0026#34;0.8.2 上記のようにサーバ側のソースを変更し、ログを確認した。するとPOSTリクエスト実行時はOPTIONSリクエストが最初に実行されていることが分かった。(ログ内容について一部を省略・編集している。)また、OPTIONSリクエスト時にHeaderNotAllowedというエラーが発生している。\nログの内容\n[2021-01-16T05:56:21Z DEBUG actix_web::middleware::logger] Error in response: HeadersNotAllowed [2021-01-16T05:56:21Z INFO actix_web::middleware::logger] ... \u0026#34;OPTIONS /operations HTTP/1.1\u0026#34; 400 44 \u0026#34;http://\u0026lt;localIP\u0026gt;:8080/\u0026#34; ... content-typeヘッダを許可するように修正することで、エラーが解消された ChromeのデベロッパーツールのNetworkタブから、OPTIONSリクエスト実行時のヘッダーを見てみる。Access-Control-Request-Headersヘッダーに以下のような設定がされていた。\nAccess-Control-Request-Headers: content-type 恐らくこれがエラーの原因と思われる。content-typeヘッダーを許可するように、main.rsについて、以下のように修正した。(ローカルIPが記述されていたところは一部編集している。)\nmain.rs\n... use actix_cors::Cors; + use actix_web::{get, post, http, web, App, HttpResponse, HttpServer, Responder}; - use actix_web::{get, post, web, App, HttpResponse, HttpServer, Responder}; use serde::{Deserialize, Serialize}; ... HttpServer::new(|| { let cors = Cors::default() .allowed_origin(\u0026#34;http://\u0026lt;localIP\u0026gt;:8080\u0026#34;) + .allowed_methods(vec![\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;]) - .allowed_methods(vec![\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;]); + .allowed_header(http::header::CONTENT_TYPE); App::new().wrap(cors).wrap(Logger::default()).service(index).service(post_operations) }) ... ここまでの変更で、ようやくChromeからのPOSTリクエストが通るようになった。最初GETだけ通ってPOSTが通らなかったのは、ヘッダー許可の設定が足りていなかったということだった。\n結論 actix-webに対してブラウザからHTTPリクエストを実行した際、Access to XMLHttpRequest ... has been blocked by CORS policyというエラーが発生した場合は、actix_cors等を使ってCORSポリシーに対応させる必要がある。\n[備考]OPTIONSリクエストについて 以下が参考になった。\n アフィリエイト\n 実践Rustプログラミング入門 秀和システム (2020/8/22) Amazon Kindle 楽天ブックス 楽天Kobo   ","permalink":"https://kouya17.com/posts/38/","summary":"環境についてざっくり クライアント側  ブラウザ: Google Chrome バージョン 87.0.4280.141 (Official Build) (64ビット) OS: Windows 10 Home 19041.746 HTTPクライアント: axios v0.21.1  サーバ側  OS: Raspbian GNU/Linux 10 (buster) webフレームワーク: actix-web v3.3.2  現象 Access to XMLHttpRequest ... has been blocked by CORS policyというエラーが出力される Raspberry Pi 4 Model B上でバックエンド(actix-web)とフロントエンド(Vue CLI)を稼働させている。WindowsPCからブラウザを使ってフロントエンドにアクセスし、axiosを使ってバックエンド(actix-web)にGETリクエストを実行した。すると、ブラウザのコンソールに以下のようなエラーが出力された。(ローカルIPが記述されていたところは一部編集している。)\nAccess to XMLHttpRequest at \u0026#39;http://\u0026lt;localIP\u0026gt;:50001/\u0026#39; from origin \u0026#39;http://\u0026lt;localIP\u0026gt;:8080\u0026#39; has been blocked by CORS policy: No \u0026#39;Access-Control-Allow-Origin\u0026#39; header is present on the requested resource.","title":"Chromeからactix-webにHTTPリクエストを実行した際に発生したCORSエラーの解消"},{"content":"今年もあっという間だったがもう終わるらしい。この1年で周りの環境は急激に変わった。ただ、自分の趣味のモノづくり周りについては、やってることは特に去年と変わらず、自分の作りたいものを作るという感じだった。\nついでに過去の振り返りもリンクを貼り付けておく。\n今年(2019年)作ったものを振り返る | kouya17.com\n今年作ったもの 去年11月~2月　ボール収集ロボの作成 色が付いたボールを所定の場所に集めるロボコンのようなものに参加する機会があったので、作成した。\n機体は3Dプリンタで作成し、ラズパイ4とラズパイ用カメラを搭載している。カメラからの画像を処理し、所定の色のボールを認識し、ボールに近づく。ボールにある程度近づいたらボールを確保する。\n機体の写真を以下に載せる。\n機体上部に謎の模様があるのは、より広い範囲が見えるカメラが取り付けられているJetsonNanoから機体を制御するため。JetsonNanoは機体の向きを確認し、ソケット通信によって機体に指令を送る。\nできたもの \n\n関連記事  カメラ画像から対象物の向きを検出する(OpenCV) | kouya17.com  2月~4月　所属サークルのHP作成 Laravelの勉強がてら、HPを作成した。初めてVue.jsを使ってSPAを作成したこともあり、フロントエンドとバックエンドの分離についての理解を深めることができた。\n画面デザインやリソースの構造についてあまりしっかり検討せずに進めてしまったのが少し心残りではある。特にリソースの構造については、現状、記事を作品カテゴリとコラムカテゴリにカテゴリ分けしているのだが、特にカテゴリ分けの意味がないという問題がある。ここについては何とか出来たんじゃないかなと思っている。\nできたもの  関連記事  Laravel+Vue.jsでLighthouseのスコアを0点から97点にした(バンドルサイズ削減) | kouya17.com Laravel+Vue.jsでSPA(シングルページアプリケーション)を作成した | kouya17.com Vue.jsを使ったSPAにおいてTwitter等でのリンク表示がいい感じになるようにする | kouya17.com  3月~　射的ゲーム作成 今年の大部分はこの作業をしていた。昨年から引き続きこの作業はしているが、まだしっかりとした形にはなっていない。来年にはどこかで公開できるようにしたい。\nできたもの まだ公開できるものはなし。\n関連記事  マトリクスLEDドライバを使ってフルカラーLEDを制御してみる | kouya17.com 赤外線を周期的に(矩形波で)出力するモジュールを作る | kouya17.com  5月　感染シミュレーション用プログラムの作成 学生時代に勉強していた数値計算の復習がてら、感染シミュレーション用のプログラムを作成した。\nSEIRモデルという単語は結構色んなところで見かけるようになったので、ここで少し勉強して、理解を深めておいて良かったと思っている。\nできたもの \n関連記事  感染症数理モデルについて触りの部分だけ学ぶ | kouya17.com 都道府県間通勤・通学を考慮したパンデミックシミュレーションもどき | kouya17.com 全国規模の感染シミュレーションと結果の可視化をしてみる | kouya17.com  10月　換気状況モニタの作成 感染症関連で何か作れないか考え、二酸化炭素濃度モニタを作成した。こういった環境モニタ系はもっと利用シーンが増えると思っていた(店舗が換気状況のアピールに使う等)のだが、今のところあまりそういった(利用シーンが増えた・よく見かけるようになった)実感はない。\nできたもの M5Stack Basic と TVOC/eCO2 ガスセンサユニット を使って簡単な換気状況モニタ?を作ってみました。\n表示系は LovyanGFX を使わせていただいています。\n先週末から運用してて、1分に1回ambientにデータを記録しているのだけれど、時々急激に値が上昇することがあるのは何の影響なのか分からない…。 pic.twitter.com/n1Nvp3RpIv\n\u0026mdash; 青木晃也 (@aoki_kouya) October 16, 2020  今年を振り返ってみて 今年もメカ・エレキ・ソフト色々な分野の勉強が出来たと思う。特にどの分野を中心に取り組んだとかはない。\n大体モノを作るときは 3DCADで設計→そこら辺にあるフィラメントを使って3Dプリンタで出力→終わり という流れなのだが、出来上がったモノのデザインに統一性がないのが気になってきた。まあ統一性がないというのも一つの個性ではあると思う。しかし、出来上がったモノを見て、これは青木晃也が作ったものだなと認識できるようになるのが理想的だなと思っている。来年はよりデザイン面を意識してモノづくりをしていきたいと思う。\n","permalink":"https://kouya17.com/posts/37/","summary":"今年もあっという間だったがもう終わるらしい。この1年で周りの環境は急激に変わった。ただ、自分の趣味のモノづくり周りについては、やってることは特に去年と変わらず、自分の作りたいものを作るという感じだった。\nついでに過去の振り返りもリンクを貼り付けておく。\n今年(2019年)作ったものを振り返る | kouya17.com\n今年作ったもの 去年11月~2月　ボール収集ロボの作成 色が付いたボールを所定の場所に集めるロボコンのようなものに参加する機会があったので、作成した。\n機体は3Dプリンタで作成し、ラズパイ4とラズパイ用カメラを搭載している。カメラからの画像を処理し、所定の色のボールを認識し、ボールに近づく。ボールにある程度近づいたらボールを確保する。\n機体の写真を以下に載せる。\n機体上部に謎の模様があるのは、より広い範囲が見えるカメラが取り付けられているJetsonNanoから機体を制御するため。JetsonNanoは機体の向きを確認し、ソケット通信によって機体に指令を送る。\nできたもの \n\n関連記事  カメラ画像から対象物の向きを検出する(OpenCV) | kouya17.com  2月~4月　所属サークルのHP作成 Laravelの勉強がてら、HPを作成した。初めてVue.jsを使ってSPAを作成したこともあり、フロントエンドとバックエンドの分離についての理解を深めることができた。\n画面デザインやリソースの構造についてあまりしっかり検討せずに進めてしまったのが少し心残りではある。特にリソースの構造については、現状、記事を作品カテゴリとコラムカテゴリにカテゴリ分けしているのだが、特にカテゴリ分けの意味がないという問題がある。ここについては何とか出来たんじゃないかなと思っている。\nできたもの  関連記事  Laravel+Vue.jsでLighthouseのスコアを0点から97点にした(バンドルサイズ削減) | kouya17.com Laravel+Vue.jsでSPA(シングルページアプリケーション)を作成した | kouya17.com Vue.jsを使ったSPAにおいてTwitter等でのリンク表示がいい感じになるようにする | kouya17.com  3月~　射的ゲーム作成 今年の大部分はこの作業をしていた。昨年から引き続きこの作業はしているが、まだしっかりとした形にはなっていない。来年にはどこかで公開できるようにしたい。\nできたもの まだ公開できるものはなし。\n関連記事  マトリクスLEDドライバを使ってフルカラーLEDを制御してみる | kouya17.com 赤外線を周期的に(矩形波で)出力するモジュールを作る | kouya17.com  5月　感染シミュレーション用プログラムの作成 学生時代に勉強していた数値計算の復習がてら、感染シミュレーション用のプログラムを作成した。\nSEIRモデルという単語は結構色んなところで見かけるようになったので、ここで少し勉強して、理解を深めておいて良かったと思っている。\nできたもの \n関連記事  感染症数理モデルについて触りの部分だけ学ぶ | kouya17.com 都道府県間通勤・通学を考慮したパンデミックシミュレーションもどき | kouya17.com 全国規模の感染シミュレーションと結果の可視化をしてみる | kouya17.com  10月　換気状況モニタの作成 感染症関連で何か作れないか考え、二酸化炭素濃度モニタを作成した。こういった環境モニタ系はもっと利用シーンが増えると思っていた(店舗が換気状況のアピールに使う等)のだが、今のところあまりそういった(利用シーンが増えた・よく見かけるようになった)実感はない。","title":"今年(2020年)作ったものを振り返る"},{"content":"環境  OS: Windows10 Home 1903 VSCode: version 1.51.1 PlatformIO IDE: version 2.2.1  PlatformIOとは PlafrotmIOとは、組み込みソフト開発向けの、様々なプラットフォーム・アーキテクチャ・フレームワークに対応した開発用ツール。\n 公式のドキュメントによると、このツールは以下の課題を解消することを目的としている。\n 特定のMCU/ボード用の開発ソフトウェアをセットアップするためのプロセスが複雑であること。そして、そのソフトウェアがサポートされているOSを搭載しているPCを入手する必要があること。 それぞれのハードウェアプラットフォームはそれぞれ異なるツールチェーン、IDE等を必要とし、開発環境の学習に時間がかかること。 一般的なセンサやアクチュエータの使用方法を示す適切なライブラリやコードサンプルを探す必要があること。 チームメンバと、それぞれが使用しているOSに依らずにプロジェクトを共有する必要があること。  上記の課題はPlatformIOを用いれば、次の手段によって解決される。\n platform.iniに対象のボードを設定する。 ボードのリストに基づいて、PlatformIOは必要なツールチェーンをダウンロード・インストールする。 ユーザはコードを開発し、PlatformIOはコードがコンパイルされ、対象のボードに書き込めるように手配する。  要するに、組み込みソフト開発向けの、開発環境整備用ツールらしい。 様々なIDE向けのプラグインを提供しているが、公式が推奨しているIDEはVSCodeとCLion。\n今回はM5Stack用のプログラムを作成しようと思って、開発環境周りを少し調べたら、 PlatformIOが良い という記事を見かけたので試してみた。ただし、この記事ではPlafromIOの使い方については特に触れない。\nPlatformIOのHome画面がloading状態のままスタックする VSCodeに、以下のPlatformIOプラグインを入れた。\n PlatformIOには以下のようなHome画面がある。\nインストール直後は、この画面が以下のように loading\u0026hellip; という表示のまま数分経っても変わらなかった。以前他のPCにインストールした時はこのようなことはなかったので、何かよろしくない環境にハマったのだろうと思った。\nプラグインをインストールし直しても状況は変わらず 以下の公式コミュニティへの問い合わせを参考に、pip uninstall platformio後に~/.platformio/を削除し、プラグインを再インストールしたが、状況は変わらなかった。\n 久しぶりに起動したPCで、色々アップデートが走っていたので、諸々のバージョンの整合性等が合ってないのかなと思っていたが、そうではないようだった。\nIEを無効化したら解消した 以下の問い合わせを参考に、IEを無効化したら解消した。\n IEの無効化は以下のwindowsの機能の有効化または無効化の画面から設定できる。\nPlatformIOのHome画面は、IEだと表示できないらしい。なぜ表示ブラウザとしてIEが選択されたのか等はわからないが、とりあえず問題が解消されたので良しとする。\nなお、一度表示されるようになってからは、IEの無効化を解除しても、表示されるようになった。\nアフィリエイト\n みんなのM5Stack入門 リックテレコム (2019/11/8) Amazon Kindle 楽天ブックス 楽天Kobo   ","permalink":"https://kouya17.com/posts/36/","summary":"環境  OS: Windows10 Home 1903 VSCode: version 1.51.1 PlatformIO IDE: version 2.2.1  PlatformIOとは PlafrotmIOとは、組み込みソフト開発向けの、様々なプラットフォーム・アーキテクチャ・フレームワークに対応した開発用ツール。\n 公式のドキュメントによると、このツールは以下の課題を解消することを目的としている。\n 特定のMCU/ボード用の開発ソフトウェアをセットアップするためのプロセスが複雑であること。そして、そのソフトウェアがサポートされているOSを搭載しているPCを入手する必要があること。 それぞれのハードウェアプラットフォームはそれぞれ異なるツールチェーン、IDE等を必要とし、開発環境の学習に時間がかかること。 一般的なセンサやアクチュエータの使用方法を示す適切なライブラリやコードサンプルを探す必要があること。 チームメンバと、それぞれが使用しているOSに依らずにプロジェクトを共有する必要があること。  上記の課題はPlatformIOを用いれば、次の手段によって解決される。\n platform.iniに対象のボードを設定する。 ボードのリストに基づいて、PlatformIOは必要なツールチェーンをダウンロード・インストールする。 ユーザはコードを開発し、PlatformIOはコードがコンパイルされ、対象のボードに書き込めるように手配する。  要するに、組み込みソフト開発向けの、開発環境整備用ツールらしい。 様々なIDE向けのプラグインを提供しているが、公式が推奨しているIDEはVSCodeとCLion。\n今回はM5Stack用のプログラムを作成しようと思って、開発環境周りを少し調べたら、 PlatformIOが良い という記事を見かけたので試してみた。ただし、この記事ではPlafromIOの使い方については特に触れない。\nPlatformIOのHome画面がloading状態のままスタックする VSCodeに、以下のPlatformIOプラグインを入れた。\n PlatformIOには以下のようなHome画面がある。\nインストール直後は、この画面が以下のように loading\u0026hellip; という表示のまま数分経っても変わらなかった。以前他のPCにインストールした時はこのようなことはなかったので、何かよろしくない環境にハマったのだろうと思った。\nプラグインをインストールし直しても状況は変わらず 以下の公式コミュニティへの問い合わせを参考に、pip uninstall platformio後に~/.platformio/を削除し、プラグインを再インストールしたが、状況は変わらなかった。\n 久しぶりに起動したPCで、色々アップデートが走っていたので、諸々のバージョンの整合性等が合ってないのかなと思っていたが、そうではないようだった。\nIEを無効化したら解消した 以下の問い合わせを参考に、IEを無効化したら解消した。\n IEの無効化は以下のwindowsの機能の有効化または無効化の画面から設定できる。\nPlatformIOのHome画面は、IEだと表示できないらしい。なぜ表示ブラウザとしてIEが選択されたのか等はわからないが、とりあえず問題が解消されたので良しとする。\nなお、一度表示されるようになってからは、IEの無効化を解除しても、表示されるようになった。\nアフィリエイト\n みんなのM5Stack入門 リックテレコム (2019/11/8) Amazon Kindle 楽天ブックス 楽天Kobo   ","title":"Windows10でPlatformIOが起動しない問題の解消"},{"content":"ASMPワーカープログラム作成後、プロジェクトのクリーンに失敗する 環境  OS : macOS Mojave v10.14.6 Spesense SDK : v2.0.1 Spresense VSCode IDE : v1.2.0  現象 公式のマルチコアアプリケーション作成ガイドに沿って、プロジェクトをビルドし、ボードに書き込んだ。\nしかし思った通りにアプリケーションが動かなかったため、プロジェクトのクリーンを一度行ったら以下のようなエラーが出た。\n\u0026gt; Executing task in folder myproject: .vscode/build.sh clean \u0026lt; usage: grep [-abcDEFGHhIiJLlmnOoqRSsUVvwxZ] [-A num] [-B num] [-C[num]] [-e pattern] [-f file] [--binary-files=value] [--color=when] [--context[=num]] [--directories=action] [--label] [--line-buffered] [--null] [pattern] [file ...] Create .version make[3]: Nothing to be done for `clean\u0026#39;. Makefile:12: /Users/username/spresense/spresense/sdk/apps/.vscode/worker.mk: No such file or directory make[6]: *** No rule to make target `/Users/username/spresense/spresense/sdk/apps/.vscode/worker.mk\u0026#39;. Stop. make[5]: *** [displayCamera_worker/_clean] Error 2 make[4]: *** [/Users/username/Desktop/Lchika/spresense/myproject_clean] Error 2 make[3]: *** [clean] Error 2 make[2]: *** [/Users/username/spresense/spresense/sdk/apps/spresense/_clean] Error 2 また、SDKコンフィグファイルをSpresense VSCode IDE上から開こうとしても、\u0026ldquo;コンフィグファイルの解析に失敗した\u0026quot;というメッセージが出て、開けなくなってしまった。\nこの時のワークスペース関係のフォルダ設定は以下の通り。\n Spresense SDKのパス  /Users/username/spresense/spresense/   プロジェクトフォルダーのパス  /Users/username/Desktop/Lchika/spresense/myproject/    とりあえずの対応策 ASMPワーカープログラムプログラムのmakefileを以下のように書き換える。\n- include $(APPDIR)/.vscode/worker.mk + include $(SPRESENSE_HOME)/.vscode/worker.mk 原因 詳細は分かっていない。\nエラー文から、/Users/username/spresense/spresense/sdk/apps/.vscode/worker.mkという存在しないファイルを参照しようとしていることがわかる。 make関係でパスの設定がおかしくなっていると思われるが、ASMPワーカーとアプリケーションで、makefileの違いを見たら、上記対応策のような違いがあった。 アプリケーション側に合わせたら、エラーが出なくなった。\n備考 最初にSpresenseの公式にあるビルドエラーが起こるを参考にプロジェクトフォルダ設定の更新を行ったが解決しなかった。\n マルチコアアプリケーション実行時にmptask_init() failure. -2というエラーが出る 環境  OS : macOS Mojave v10.14.6 Spesense SDK : v2.0.1 Spresense VSCode IDE : v1.2.0  現象 公式のマルチコアアプリケーション作成ガイドに沿って、プロジェクトをビルドし、ボードに書き込んだ。\nシリアルターミナル上で作成したマルチコアアプリケーションasmpAppを実行すると、以下のような表示になった。\nnsh\u0026gt; asmpApp mptask_init() failure. -2 原因 /mnt/spif以下の容量が足りず、ワーカープログラムを書き込めていなかった。\n上記現象において、mptask_init()の戻り値が-2になっている。mptask_init()は、ワーカープログラムを参照できないときに戻り値が-2になる。\nサンプルプログラムでは、ワーカープログラムは/mnt/spif以下に書き込まれるようになっている。以下のように/mnt/spif配下を確認したところ、以前保存していた画像ファイルがあり、これのせいでワーカープログラムをうまく書き込めていないようだった。\nnsh\u0026gt; cd /mnt nsh\u0026gt; cd spif nsh\u0026gt; ls /mnt/spif: VIDEO001.YUV VIDEO002.YUV VIDEO003.YUV ... VIDEO024.YUV 対応策 以下のようにrmコマンドで/mnt/spif以下の不要なファイルを削除する。(nuttxでは*のようなワイルドカードは指定できないようだった。)\nnsh\u0026gt; rm VIDEO001.YUV 備考 上記対応策を実施した後、再書き込みし、アプリケーションを実行したところ、正常に動作した。\nnsh\u0026gt; asmpApp attached at 00010000 Worker response: ID = 1, data = 1234 Worker said: Hello, ASMP World! Worker exit status = 0  スタートアップスクリプトの機能を有効にしようとしたときにビルドエラーが起きる 環境  OS : macOS Catalina v10.15.6 Spesense SDK : v2.0.1 Spresense VSCode IDE : v1.2.0  現象 アプリケーションの自動起動を有効にするためにfeature/startup_scriptのdefconfigを参考に、以下をsdk.condigに追加してビルドした。\nNSH_CUSTOMROMFS=y NSH_CUSTOMROMFS_HEADER=\u0026#34;../../system/startup_script/nsh_romfsimg.h\u0026#34; SYSTEM_STARTUP_SCRIPT=y すると、ビルド時に以下のエラーがでた。\nCC: nsh_romfsetc.c nsh_romfsetc.c:82:6: error: #error You must select CONFIG_BOARDCTL_ROMDISK in your configuration file # error You must select CONFIG_BOARDCTL_ROMDISK in your configuration file ^~~~~ nsh_romfsetc.c: In function \u0026#39;nsh_romfsetc\u0026#39;: nsh_romfsetc.c:99:29: error: storage size of \u0026#39;desc\u0026#39; isn\u0026#39;t known struct boardioc_romdisk_s desc; ^~~~ nsh_romfsetc.c:99:29: warning: unused variable \u0026#39;desc\u0026#39; [-Wunused-variable] make[3]: *** [nsh_romfsetc.o] Error 1 make[2]: *** [/Users/username/spresense/spresense/sdk/apps/nshlib_all] Error 2 make[1]: *** [../sdk/apps/libapps.a] Error 2 make: *** [all] Error 2 ターミナル プロセス \u0026#34;/bin/bash \u0026#39;-c\u0026#39;, \u0026#39;.vscode/build.sh build\u0026#39;\u0026#34; が終了コード 2 で終了しました。 CONFIG_BOARDCTL_ROMDISKをコンフィグファイルで選択しなければならないらしい。\n対応策 sdk.configに以下を追記する。\nCONFIG_BOARDCTL_ROMDISK=y アフィリエイト\n SONY SPRESENSE メインボード CXD5602PWBMAIN1 スプレッセンス(Spresense) Amazon 楽天   ","permalink":"https://kouya17.com/posts/35/","summary":"ASMPワーカープログラム作成後、プロジェクトのクリーンに失敗する 環境  OS : macOS Mojave v10.14.6 Spesense SDK : v2.0.1 Spresense VSCode IDE : v1.2.0  現象 公式のマルチコアアプリケーション作成ガイドに沿って、プロジェクトをビルドし、ボードに書き込んだ。\nしかし思った通りにアプリケーションが動かなかったため、プロジェクトのクリーンを一度行ったら以下のようなエラーが出た。\n\u0026gt; Executing task in folder myproject: .vscode/build.sh clean \u0026lt; usage: grep [-abcDEFGHhIiJLlmnOoqRSsUVvwxZ] [-A num] [-B num] [-C[num]] [-e pattern] [-f file] [--binary-files=value] [--color=when] [--context[=num]] [--directories=action] [--label] [--line-buffered] [--null] [pattern] [file ...] Create .version make[3]: Nothing to be done for `clean\u0026#39;. Makefile:12: /Users/username/spresense/spresense/sdk/apps/.vscode/worker.mk: No such file or directory make[6]: *** No rule to make target `/Users/username/spresense/spresense/sdk/apps/.","title":"Spresense VSCode IDEトラブルシューティング(適宜更新予定)"},{"content":"UIFlowとは  M5Stack系列がサポートしている開発環境の1つ。主な特徴は以下の通り。\n Web上でコーディングから書き込みまでできる。 ビジュアルプログラミング言語Blockly及びMicroPythonをサポートしている。  HuskyLensとは  AI処理向けのSoC Kendryte K210 を搭載したAIカメラモジュール。顔認識、オブジェクト追跡、オブジェクト認識、ライン追跡、色認識、タグ（AprilTag）認識などの機能が実装されている。接続インタフェースはUART及びI2C。\nMicroPython環境でHuskyLensを使う Pythonで実装されたHuskyLensとの通信用ライブラリは、RaspberryPiでの利用向けに公開されている。\n MicroPython上でHuskyLensを使う場合、基本的にはここから必要な処理をコピペして使えば良さそうなのだが、一部 MicroPython(on ESP32) でサポートされていないモジュールを利用している箇所があるため、その部分を書き換える必要がある。ただし、今回はUARTでの通信を前提とする。\n環境  UIFlowファームウェア : ver1.4.5.1 HuskyLens用Pythonライブラリ : 2020/06/28時点の最新  書き換えが必要な箇所 UART通信用オブジェクト生成部分 書き換え前\nself.huskylensSer = serial.Serial(port=comPort, baudrate=speed) 書き換え後例\nself.serial = machine.UART(1, tx=32, rx=33) self.serial.init(9600, bits=8, parity=None, stop=1) MicroPythonではmachineモジュールがもつクラスUARTを使ってUART通信バスにアクセスできる。ピン番号やボーレートは自分の環境に合わせて設定する。\n参考 : クラス UART -- 二重シリアル通信バス — MicroPython 1.12 ドキュメント\n16進数文字列からバイナリデータへの変換部分 書き換え前\ndef cmdToBytes(self, cmd): return bytes.fromhex(cmd) 書き換え後例\nimport ubinascii def cmdToBytes(self, cmd): return ubinascii.unhexlify(cmd) MicroPythonにはbytes.fromhex()は実装されていないらしい。実行しようとしたら そんな関数はない と怒られた。代わりにMicroPythonでも標準でサポートされているubinasciiモジュールを使う。\n参考 : ubinascii -- バイナリ/ASCII 変換 — MicroPython 1.12 ドキュメント\nバイナリデータから16進数文字列への変換部分 書き換え前\ncommandSplit = self.splitCommandToParts(byteString.hex()) 書き換え後例\ncommandSplit = self.splitCommandToParts(ubinascii.hexlify(byteString)) bytes.hex()も同様に実装されていないらしいので、代わりにubinasciiモジュールを使う。ただし、ubinascii.hexlify()は戻り値が文字列ではなく、バイト列であるため注意。\n受信データと文字列の比較部分 書き換え前\nif(commandSplit[3] == \u0026#34;2e\u0026#34;): return \u0026#34;Knock Recieved\u0026#34; 書き換え後\nif(commandSplit[3] == b\u0026#39;2e\u0026#39;): return \u0026#34;Knock Recieved\u0026#34; 先ほどの書き換えの影響で、commandSplitには文字列ではなくてバイト列が格納されている。そのため比較部分もバイト列と比較するようにする。\nアフィリエイト\nゼロから作るDeep Learningposted with ヨメレバ斎藤 康毅 オライリー・ジャパン 2016年09月24日 楽天ブックスで購入Amazonで購入Kindleで購入 ","permalink":"https://kouya17.com/posts/34/","summary":"UIFlowとは  M5Stack系列がサポートしている開発環境の1つ。主な特徴は以下の通り。\n Web上でコーディングから書き込みまでできる。 ビジュアルプログラミング言語Blockly及びMicroPythonをサポートしている。  HuskyLensとは  AI処理向けのSoC Kendryte K210 を搭載したAIカメラモジュール。顔認識、オブジェクト追跡、オブジェクト認識、ライン追跡、色認識、タグ（AprilTag）認識などの機能が実装されている。接続インタフェースはUART及びI2C。\nMicroPython環境でHuskyLensを使う Pythonで実装されたHuskyLensとの通信用ライブラリは、RaspberryPiでの利用向けに公開されている。\n MicroPython上でHuskyLensを使う場合、基本的にはここから必要な処理をコピペして使えば良さそうなのだが、一部 MicroPython(on ESP32) でサポートされていないモジュールを利用している箇所があるため、その部分を書き換える必要がある。ただし、今回はUARTでの通信を前提とする。\n環境  UIFlowファームウェア : ver1.4.5.1 HuskyLens用Pythonライブラリ : 2020/06/28時点の最新  書き換えが必要な箇所 UART通信用オブジェクト生成部分 書き換え前\nself.huskylensSer = serial.Serial(port=comPort, baudrate=speed) 書き換え後例\nself.serial = machine.UART(1, tx=32, rx=33) self.serial.init(9600, bits=8, parity=None, stop=1) MicroPythonではmachineモジュールがもつクラスUARTを使ってUART通信バスにアクセスできる。ピン番号やボーレートは自分の環境に合わせて設定する。\n参考 : クラス UART -- 二重シリアル通信バス — MicroPython 1.12 ドキュメント\n16進数文字列からバイナリデータへの変換部分 書き換え前\ndef cmdToBytes(self, cmd): return bytes.fromhex(cmd) 書き換え後例\nimport ubinascii def cmdToBytes(self, cmd): return ubinascii.","title":"UIFlowの環境でHuskyLensを使えるようにする"},{"content":"現在作成中である射的ゲームの銃ユニット用に、赤外線を周期的に(矩形波で)出力するモジュールを作成する。\n電源 まず必要な電圧・電流を調べる 今回使う赤外線LED → ５ｍｍ赤外線ＬＥＤ　９４０ｎｍ　ＯＳＩ５ＦＵ５１１１Ｃ－４０　（５個入）: LED(発光ダイオード) 秋月電子通商-電子部品・ネット通販\n赤外線LEDの絶対最大定格\n 最大順方向電流 If : 100mA  赤外線LEDの電気的特性\n 順方向電圧 : If=100mAの時 代表値 1.35V、 最大値 1.6V  If=100mAとなっているが(なかなか高出力…)、50%の50mAが流れるように設計する。\n電源の選定 50mA流しても大丈夫 + 1.35V以上を確保できる電源を選ぶ。\n値段等も考えると、1000円でバッテリー+電池保護+電源管理がついてるPowerCが自分にとって使いやすそう。\nM5StickCは使わないので、BATピンだけ活用する。\nPowerCの給電周りの仕組みについては公式ドキュメントとM5StickCのバッテリー拡張HATをためす – Lang-shipを参考にさせていただいた。\n安定化 定電圧をとるために3端子レギュレータをかます。\n低飽和型レギュレーター　３Ｖ５００ｍＡ　ＮＪＭ２８８４Ｕ１－０３: 半導体 秋月電子通商-電子部品・ネット通販とかでよさそう。\n→ ip3005のデータシートによると2.5V付近まで落ちるみたいなことが書いてあるので、これだとバッテリ残量少ないときダメそう。\n秋月で買えるやつでベストそうなのは低ドロップレギュレーター　２．５Ｖ１．５Ａ　ＬＴ１９６３ＡＥＳ８－２．５: 半導体 秋月電子通商-電子部品・ネット通販。これを使う。\nだいぶいいやつなので高い…(1個200円)。\n赤外線出力 矩形波生成 矩形波（方形波）発生回路の理論的な理解 - 電子工作で覚える！電子回路を参考にさせていただく。\n上記サイト中の式\n$$ T = 1.386 \\times C_1R_4 $$\nにおいて、\\(C_1 = 0.1 \\mu F\\)に固定した場合、\n$$ R_4 = \\frac{T}{1.386} \\times 10^{7} $$\n今回は周期\\(T\\)を2ms, 4ms, 6ms,\u0026hellip;と変更できるようにしたいので、\\(R_4=14.43k, 28.86k, 43.29k, \u0026hellip;\\)のような値をとれる必要がある。\n以下のような回路にすれば恐らくスライドスイッチで周期を管理できる(ショートする危険性があるので本当はよろしくないが)。\nスイッチング 赤外線LEDに流れる電流はトランジスタで増幅させる。\nトランジスタはリッチにトランジスタ　ＴＴＣ００４Ｂ　１６０Ｖ１．５Ａ: 半導体 秋月電子通商-電子部品・ネット通販を使う。\n諸々抵抗値の計算はトランジスタ(NPN)の使い方 [Arduino]を参考にさせていただく。\nデータシートより\\(h_{FE}=140\\)(\\(I_C=0.1A\\))として諸々計算する。\nベース電流 = 50mA(負荷に流すコレクタ電流) ÷ 140(\\(h_{FE}\\)) = 0.357mA\nベース抵抗 = 2.5V(ベース電圧) - 1.0V(ベース・エミッタ間飽和電圧) ÷ 0.357mA ≒ 4.0kΩ\nエミッタ抵抗は他のサイトの例も見つつ、とりあえず10kΩにする。\n基板設計 KiCadを使って回路図、パターン図を作成する。\n回路図 パターン図 3Dイメージ プリント基板発注、実装 今回はElecrowでプリント基板を発注した。\nスケジュールとしては以下のような感じだった。\n5/30：発注 → 6/5：発送 → 6/8：到着\n代金は5枚の注文で2113円だった。うち4分の3が送料(OCS)なので、少量注文だとかなり割高になる。\nプリント基板が届いたので、部品を実装する。\nPowerCとの接続部分のピン配置を間違っていた。悲しみ。\nとりあえずジャンパ線でなんとかした。\n動作確認 出力電圧をデジタルオシロADALM2000を使って確認する。\n周期の誤差は10%~15%ほど出ていそうだが、まあそれっぽい電圧は出ている。\nハマったところ 最初、オペアンプにNJM4580DDを使ったのだが、低電圧時が0Vでなくて1.3Vくらいになっており、高電圧時も2.5V出ていなかった。\nデータシートを見ると、このオペアンプは最大出力電圧=電源電圧ではないらしい。\n出力電圧=電源電圧としたい場合は入出力フルスイングオペアンプというものを使う必要があるらしく、最終的にNJM2732Dを使った。\nアフィリエイト\nKiCadではじめる「プリント基板」製作posted with ヨメレバ外川貴規 工学社 2018年02月 楽天ブックスで購入Amazonで購入 ","permalink":"https://kouya17.com/posts/33/","summary":"現在作成中である射的ゲームの銃ユニット用に、赤外線を周期的に(矩形波で)出力するモジュールを作成する。\n電源 まず必要な電圧・電流を調べる 今回使う赤外線LED → ５ｍｍ赤外線ＬＥＤ　９４０ｎｍ　ＯＳＩ５ＦＵ５１１１Ｃ－４０　（５個入）: LED(発光ダイオード) 秋月電子通商-電子部品・ネット通販\n赤外線LEDの絶対最大定格\n 最大順方向電流 If : 100mA  赤外線LEDの電気的特性\n 順方向電圧 : If=100mAの時 代表値 1.35V、 最大値 1.6V  If=100mAとなっているが(なかなか高出力…)、50%の50mAが流れるように設計する。\n電源の選定 50mA流しても大丈夫 + 1.35V以上を確保できる電源を選ぶ。\n値段等も考えると、1000円でバッテリー+電池保護+電源管理がついてるPowerCが自分にとって使いやすそう。\nM5StickCは使わないので、BATピンだけ活用する。\nPowerCの給電周りの仕組みについては公式ドキュメントとM5StickCのバッテリー拡張HATをためす – Lang-shipを参考にさせていただいた。\n安定化 定電圧をとるために3端子レギュレータをかます。\n低飽和型レギュレーター　３Ｖ５００ｍＡ　ＮＪＭ２８８４Ｕ１－０３: 半導体 秋月電子通商-電子部品・ネット通販とかでよさそう。\n→ ip3005のデータシートによると2.5V付近まで落ちるみたいなことが書いてあるので、これだとバッテリ残量少ないときダメそう。\n秋月で買えるやつでベストそうなのは低ドロップレギュレーター　２．５Ｖ１．５Ａ　ＬＴ１９６３ＡＥＳ８－２．５: 半導体 秋月電子通商-電子部品・ネット通販。これを使う。\nだいぶいいやつなので高い…(1個200円)。\n赤外線出力 矩形波生成 矩形波（方形波）発生回路の理論的な理解 - 電子工作で覚える！電子回路を参考にさせていただく。\n上記サイト中の式\n$$ T = 1.386 \\times C_1R_4 $$\nにおいて、\\(C_1 = 0.1 \\mu F\\)に固定した場合、","title":"赤外線を周期的に(矩形波で)出力するモジュールを作る"},{"content":"Ambientとは 本当にすばらしいサービス。\nAmbient – IoTデーター可視化サービス\nデータをテキストでダウンロードできるということで、今回使用させていただくことを決めた。\nデーターのダウンロード機能をリリースしました – Ambient\nPowerCとは M5StickCと接続できるバッテリー充電モジュール。\n16340バッテリー(3.7V/700mAh)を2個搭載できる。\nUSB Type-A端子から、外部へ電源供給(5V/1.5A)ができる。\n Ambientを使ってバッテリーの電圧を記録する 今回はAmbientの使い方の勉強を兼ねて、PowerCを使ってバッテリーの情報を記録する。\nPowerCは電源管理IC IP5209 が付いており、I2C通信によってバッテリーの情報を確認できる。\nIP5209から取得できる情報 PowerCのサンプルスケッチをみると、電圧値と電流(電気量)を取得できるらしい。\nスイッチサイエンスの紹介ページには\n M5StickCと接続すると、I2C（アドレス0x75）を通じて、電圧、電流、その他の情報を確認できます。\n とあり、その他も取得できるとあるので、他にどんな情報を取得できるかIP5209の以下のデータシートを読んでみた。\nIP5209-Injoinic.pdf\nこのデータシートには\n The built-in 14bit ADC in IP5209 measures battery voltage and current accurately. ADC data are available on I2C interface. IP5209 has integrated a fuel gauge algorithm, acquiring battery’s state of charge precisely.\n とあり、バッテリーの電圧と電流が測定できるとしか書かれていない。\n私が見つけたこのデータシートにはレジスタの情報が(私が読んだ範囲では)見つからなかった。\nもうちょっと探せば他の形式のデータシートがあるかもしれない。\n少し調べてみると、M5Stackで使われている電源IC IP5306 はカスタム品で、出回っているデータシートにはレジスタの情報がないらしい。\n以下のようなTweetを見つけたので引用させていただく。\nIP5306は本来I2C未対応ですが、M5Stackで使用しているのはカスタム品だそうです。\n\u0026mdash; しお (@o_sio) February 26, 2019  IP5209もこのパターンなのかもしれない。 今回はとりあえずサンプルコードに従って、電圧と電流を記録する。\n測定環境 充電時は以下のように接続し、\n放電時は以下のように接続した。\n記録結果 1分ごとに電圧と電流の値を記録したところ、結果は以下のようになった。\nグラフの横軸は記録時刻(hh:mm)。\nAmbientでもデータを公開している。\nPowerCの電圧推移 -Ambient\n満充電状態から、今回は8時間程バッテリーがもった。\n充電時の電流値が少し気になる挙動をしているが、PowerCの回路構成をあまり把握していないし、ハードの知識がないので、理由はよく分からない。\n今回使用したプログラムは以下に置いてある。\n (補足)M5StickCの内蔵バッテリー周りについて M5StickC側の電源回りも少し調べた。\nM5StickCは電源管理ICとしてAXP192というものがついており、このICが色々よしなになってくれているらしい。\nArduino Libraryに実装されているM5.Axpクラスを通してAXP192からの情報が色々取れるらしいので、内蔵バッテリの情報がほしい場面が今後あったら有用かもしれない。\n以下を参考にさせていただいた。\n M5Stack Docs-The reference docs for M5Stack products. M5StickCのAXP192で外部電源を使う – Lang-ship M5StickCのバッテリー管理AXP192を図にまとめる – Lang-ship  アフィリエイト\nIoT開発スタートブック ── ESP32でクラウドにつなげる電子工作をはじめよう！posted with ヨメレバ下島健彦 技術評論社 2019年08月13日 楽天ブックスで購入楽天koboで購入Amazonで購入Kindleで購入 ","permalink":"https://kouya17.com/posts/32/","summary":"Ambientとは 本当にすばらしいサービス。\nAmbient – IoTデーター可視化サービス\nデータをテキストでダウンロードできるということで、今回使用させていただくことを決めた。\nデーターのダウンロード機能をリリースしました – Ambient\nPowerCとは M5StickCと接続できるバッテリー充電モジュール。\n16340バッテリー(3.7V/700mAh)を2個搭載できる。\nUSB Type-A端子から、外部へ電源供給(5V/1.5A)ができる。\n Ambientを使ってバッテリーの電圧を記録する 今回はAmbientの使い方の勉強を兼ねて、PowerCを使ってバッテリーの情報を記録する。\nPowerCは電源管理IC IP5209 が付いており、I2C通信によってバッテリーの情報を確認できる。\nIP5209から取得できる情報 PowerCのサンプルスケッチをみると、電圧値と電流(電気量)を取得できるらしい。\nスイッチサイエンスの紹介ページには\n M5StickCと接続すると、I2C（アドレス0x75）を通じて、電圧、電流、その他の情報を確認できます。\n とあり、その他も取得できるとあるので、他にどんな情報を取得できるかIP5209の以下のデータシートを読んでみた。\nIP5209-Injoinic.pdf\nこのデータシートには\n The built-in 14bit ADC in IP5209 measures battery voltage and current accurately. ADC data are available on I2C interface. IP5209 has integrated a fuel gauge algorithm, acquiring battery’s state of charge precisely.\n とあり、バッテリーの電圧と電流が測定できるとしか書かれていない。\n私が見つけたこのデータシートにはレジスタの情報が(私が読んだ範囲では)見つからなかった。\nもうちょっと探せば他の形式のデータシートがあるかもしれない。\n少し調べてみると、M5Stackで使われている電源IC IP5306 はカスタム品で、出回っているデータシートにはレジスタの情報がないらしい。\n以下のようなTweetを見つけたので引用させていただく。","title":"Ambientを使って、PowerCに接続されているバッテリーの情報を記録する"},{"content":"スマホの場合は、画面下端の方が圧倒的にタップしやすく、スマホアプリは大体画面下部に重要なナビゲーションが並んでいる。\n本サイトもそれに倣って改修する。\n環境  Django v2.1.4 Bootstrap v4.2.1  スマホのみに表示する Bootstrapの機能を使う。\n以下のようにHTMLに記載すれば、スマホ(画面サイズ768px未満)の場合のみ表示される。\n\u0026lt;div class=\u0026#34;d-block d-sm-none\u0026#34;\u0026gt;スマホのみ表示\u0026lt;/div\u0026gt; iPhone8の画面サイズで表示すると以下のように最下部に\u0026quot;スマホのみ表示\u0026quot;というテキストが表示されるが、\niPadの画面サイズで表示すると\u0026quot;スマホのみ表示\u0026quot;というテキストが表示されなくなる。\n参考\n 表示ユーティリティ～Bootstrap4移行ガイド  下端に固定する CSSで実装する 表示位置を絶対位置で指定する。\n以下のようにCSSで指定することで画面下端に要素を固定できる。\n/* スマホ用画面下端メニュー */ .footer-menu-bar { position: fixed; /* 要素の位置を固定する */ bottom: 0px; /* 絶対位置を指定する(下0px) */ } ただ、この方法だと他コンテンツと重なって表示されてしまうので他の用途に使う場合は注意。(今回の用途としては重なっても問題なしとしている)\n参考\n 【css】フッターをページ下部に固定する方法【お手軽コピペ】 | Pで作業軽減しましょ  横並びでリスト表示する Bootstrapの機能を使ってリスト表示を実装する。\nHTMLを以下のように変更する。\n\u0026lt;div class=\u0026#34;d-block d-sm-none\u0026#34;\u0026gt;スマホのみ表示\u0026lt;/div\u0026gt; ↓\n\u0026lt;div class=\u0026#34;d-sm-none d-block footer-menu-bar w-100 border-top\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;row m-2\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col\u0026#34;\u0026gt; TOP \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col\u0026#34;\u0026gt; 検索 \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col\u0026#34;\u0026gt; タグ \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 表示は以下のようになる。\nなお、ここでは説明を省略したが、CSSでtext-align: centerを適用し、テキストを中央揃えにしている。\n使い勝手をよくする アイコンを表示する Font Awesomeのアイコンを使う。\n以下のようにHTMLの\u0026lt;head\u0026gt;要素でFont Awesomeで読み込むことで(今回は自サーバ上にFont Awesomeをダウンロードしている)、\n\u0026lt;head\u0026gt; \u0026lt;link href=\u0026#34;/your-path-to-fontawesome/css/all.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34;\u0026gt; \u0026lt;!--load all styles --\u0026gt; \u0026lt;/head\u0026gt; アイコンは以下のように使用、表示できる。\n\u0026lt;body\u0026gt; ... \u0026lt;i class=\u0026#34;fas fa-user\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;!-- uses solid style --\u0026gt; \u0026lt;i class=\u0026#34;far fa-user\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;!-- uses regular style --\u0026gt; \u0026lt;i class=\u0026#34;fal fa-user\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;!-- uses light style --\u0026gt; \u0026lt;!--brand icon--\u0026gt; \u0026lt;i class=\u0026#34;fab fa-github-square\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;!-- uses brands style --\u0026gt; ... \u0026lt;/body\u0026gt; 今回であればリスト表示部分の実装(HTML)を以下のように変更する。\n\u0026lt;div class=\u0026#34;d-sm-none d-block footer-menu-bar w-100 border-top\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;row m-2\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fas fa-home\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; TOP \u0026lt;!-- アイコンを追加 --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fas fa-search\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; 検索 \u0026lt;!-- アイコンを追加 --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fas fa-tag\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; タグ \u0026lt;!-- アイコンを追加 --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 表示は以下のようになる。\n参考\n Hosting Font Awesome Yourself | Font Awesome  背景に透過色を設定する CSSで背景色を指定してメニューバーとその他のコンテンツの区別がつきやすいようにする。\n背景色は透過させて、下に隠れているコンテンツが見えるようにする。\nCSSを以下のように変更し、実装する。\n/* スマホ用画面下端メニュー */ .footer-menu-bar { text-align: center; position: fixed; /* 要素の位置を固定する */ bottom: 0; /* 絶対位置を指定する(下0px) */ background-color: rgba(255, 255, 255, 0.85); /* 背景は透過させる */ } 表示は以下のようになる。\n参考\n 背景色(background-color)を透過させて文字色は不透明にする方法とhtml,slim,css,scssでの記述方法 - Qiita  最後にリンクをつけて完成 以下のようにHTMLにリンクの設定を追加して完成。(一部見た目修正のためstyle要素で強引に設定している。)\n\u0026lt;div class=\u0026#34;d-sm-none d-block footer-menu-bar w-100 border-top\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;row m-2\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{% url \u0026#39;top\u0026#39; %}\u0026#34; style=\u0026#34;color:#212529; text-decoration:none;\u0026#34;\u0026gt;\u0026lt;i class=\u0026#34;fas fa-home\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; TOP\u0026lt;/a\u0026gt; \u0026lt;!-- TOPへのリンク --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;#search-within-site\u0026#34; style=\u0026#34;color:#212529; text-decoration:none;\u0026#34;\u0026gt;\u0026lt;i class=\u0026#34;fas fa-search\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; 検索\u0026lt;/a\u0026gt; \u0026lt;!-- 検索へのページ内リンク --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;#tag-list\u0026#34; style=\u0026#34;color:#212529; text-decoration:none;\u0026#34;\u0026gt;\u0026lt;i class=\u0026#34;fas fa-tag\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; タグ\u0026lt;/a\u0026gt; \u0026lt;!-- タグへのページ内リンク --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ここのリンク先は実装するサイトに合わせて設定する形になる。\nページ内リンクについてはスムーススクロールを実装したかったが、画像遅延ロードがある影響でスクロール先が正確に取得できないため、実装しなかった。\n参考\n HTMLでページ内リンク（ジャンプ）をスクロールする方法｜SEOラボ スタイルシート［CSS］/リンク/リンクテキストの下線を消す - TAG index  1冊ですべて身につくHTML \u0026 CSSとWebデザイン入門講座posted with ヨメレバMana SBクリエイティブ 2019年03月18日 楽天ブックスで購入楽天koboで購入Amazonで購入Kindleで購入 ","permalink":"https://kouya17.com/posts/31/","summary":"スマホの場合は、画面下端の方が圧倒的にタップしやすく、スマホアプリは大体画面下部に重要なナビゲーションが並んでいる。\n本サイトもそれに倣って改修する。\n環境  Django v2.1.4 Bootstrap v4.2.1  スマホのみに表示する Bootstrapの機能を使う。\n以下のようにHTMLに記載すれば、スマホ(画面サイズ768px未満)の場合のみ表示される。\n\u0026lt;div class=\u0026#34;d-block d-sm-none\u0026#34;\u0026gt;スマホのみ表示\u0026lt;/div\u0026gt; iPhone8の画面サイズで表示すると以下のように最下部に\u0026quot;スマホのみ表示\u0026quot;というテキストが表示されるが、\niPadの画面サイズで表示すると\u0026quot;スマホのみ表示\u0026quot;というテキストが表示されなくなる。\n参考\n 表示ユーティリティ～Bootstrap4移行ガイド  下端に固定する CSSで実装する 表示位置を絶対位置で指定する。\n以下のようにCSSで指定することで画面下端に要素を固定できる。\n/* スマホ用画面下端メニュー */ .footer-menu-bar { position: fixed; /* 要素の位置を固定する */ bottom: 0px; /* 絶対位置を指定する(下0px) */ } ただ、この方法だと他コンテンツと重なって表示されてしまうので他の用途に使う場合は注意。(今回の用途としては重なっても問題なしとしている)\n参考\n 【css】フッターをページ下部に固定する方法【お手軽コピペ】 | Pで作業軽減しましょ  横並びでリスト表示する Bootstrapの機能を使ってリスト表示を実装する。\nHTMLを以下のように変更する。\n\u0026lt;div class=\u0026#34;d-block d-sm-none\u0026#34;\u0026gt;スマホのみ表示\u0026lt;/div\u0026gt; ↓\n\u0026lt;div class=\u0026#34;d-sm-none d-block footer-menu-bar w-100 border-top\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;row m-2\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col\u0026#34;\u0026gt; TOP \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col\u0026#34;\u0026gt; 検索 \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col\u0026#34;\u0026gt; タグ \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 表示は以下のようになる。","title":"サイトをスマホで表示したとき下端にメニューバーが表示されるようにする"},{"content":"Pythonでロジスティック回帰を使って、VTuberの動画タイトルから投稿者を推論してみる。\nただ、投稿者を推論するといっても、二値分類なので、投稿者が A か B かを分類するのみ。\n使用するライブラリ 今回は以下のライブラリを使用する。\n scikit-learn : テキストのベクトル化、ロジスティック回帰 MeCab : 日本語形態素解析  データセット 適当に選んだRan Channel / 日ノ隈らん 【あにまーれ】さんとKuzuha Channelさんの動画タイトルをデータセットとして使う。\n動画タイトルはYouTube Data APIを使って取得した。\n1日のAPIリクエスト上限に引っかかって、すべては取得できなかったが、日ノ隈らんさんのタイトルは475個、葛葉さんのタイトルは456個取得できた。\nタイトル取得に使用したプログラムは以下に置いてある。\n また、タイトル中の【】内には大体投稿者名が入っている。\nこれを使うと簡単に分類できてしまうと思うので、今回は削除する。\n(まあ、【】内でなくても、投稿者名が入っていることがあるので結局、というところもあるが…)\nプログラムと推論結果 今回推論に使用したプログラムは以下に置いてある。\n テストデータは直近(このプログラムを作成中)に投稿された以下のタイトルを使う。\nIce breaker / ChroNoiR けんきさんとあびつんさんとランクいってくる！！！ １行目が葛葉さんの動画タイトルで、2行目が日ノ隈らんさんの動画タイトルになっている。\n推論結果 推論結果は以下のようになった。\n[0.45 0.69] なお、1個目が1行目の結果で、2個目が2行目の結果になる。\nまた、この結果が 1.0 に近いほど日ノ隈らんさんのタイトル、 0 に近いほど葛葉さんのタイトル\u0026quot;っぽい\u0026quot;と判断できる。\n推論結果の分析 どういった理由で今回の推論結果が出たのか分析する。\nロジスティック回帰では各特徴量の係数(重み)と切片(バイアス)を見ることができる。 今回の特徴量はタイトルに含まれる単語なので、各単語について係数(重み)を見てみる。\nまず1行目のタイトルの各単語の結果は以下の通り。\n以下でwordとtf_idfというラベルで出力しているものが、それぞれ単語名及びtf-idfという手法によって得られた、各単語の重要度になる。\nただ、このデータに関してはすべての単語について、学習元データに含まれていなかったため、this word does not existと表示されている。\nつまり、上記推論結果で0.45と出ているが、これは切片(バイアス)成分によるものである。\ntitle_index: 0 word: Ice tf_idf: this word dose not exist word: breaker tf_idf: this word dose not exist word: / tf_idf: this word dose not exist word: ChroNoiR tf_idf: this word dose not exist 次に２行目のタイトルの各単語の結果は以下の通り。\nこちらに関しては学習元に含まれる単語があるため、tf-idfによる重要度が出力されている。\nまた、最後にcoefというラベルで出力しているものが各単語の係数(重み)になる。\nこの例では「くる」という単語の係数が最も絶対値の大きい正の値を持っており、\u0026ldquo;日ノ隈らんさんのタイトルっぽい\u0026quot;単語といえる。\n確かに学習元データをみると、日ノ隈らんさんのタイトルは「くる」という単語が入っていることが多い。\ntitle_index: 1 word: けん tf_idf: 0.543427527727261 word: き tf_idf: this word dose not exist word: さん tf_idf: 0.3682797658199599 word: と tf_idf: this word dose not exist word: あ tf_idf: this word dose not exist word: びつんさんと tf_idf: this word dose not exist word: ランク tf_idf: 0.407136077797198 word: いっ tf_idf: 0.4598608078047089 word: て tf_idf: this word dose not exist word: くる tf_idf: 0.43797806771223935 word: ！ tf_idf: this word dose not exist word: ！ tf_idf: this word dose not exist word: ！ tf_idf: this word dose not exist model.coef: [[ 0.13 -0.27 -0.4 ... -0.61 -0.36 -0.36]] word: けん coef: 0.17567560417510206 word: さん coef: 0.3083888173761661 word: ランク coef: 0.2137489896925166 word: いっ coef: 0.6840435462897954 word: くる coef: 0.9504970445317464 見て試してわかる機械学習アルゴリズムの仕組み 機械学習図鑑posted with ヨメレバ秋庭 伸也/杉山 阿聖 翔泳社 2019年04月17日 楽天ブックスで購入楽天koboで購入Amazonで購入Kindleで購入 ","permalink":"https://kouya17.com/posts/30/","summary":"Pythonでロジスティック回帰を使って、VTuberの動画タイトルから投稿者を推論してみる。\nただ、投稿者を推論するといっても、二値分類なので、投稿者が A か B かを分類するのみ。\n使用するライブラリ 今回は以下のライブラリを使用する。\n scikit-learn : テキストのベクトル化、ロジスティック回帰 MeCab : 日本語形態素解析  データセット 適当に選んだRan Channel / 日ノ隈らん 【あにまーれ】さんとKuzuha Channelさんの動画タイトルをデータセットとして使う。\n動画タイトルはYouTube Data APIを使って取得した。\n1日のAPIリクエスト上限に引っかかって、すべては取得できなかったが、日ノ隈らんさんのタイトルは475個、葛葉さんのタイトルは456個取得できた。\nタイトル取得に使用したプログラムは以下に置いてある。\n また、タイトル中の【】内には大体投稿者名が入っている。\nこれを使うと簡単に分類できてしまうと思うので、今回は削除する。\n(まあ、【】内でなくても、投稿者名が入っていることがあるので結局、というところもあるが…)\nプログラムと推論結果 今回推論に使用したプログラムは以下に置いてある。\n テストデータは直近(このプログラムを作成中)に投稿された以下のタイトルを使う。\nIce breaker / ChroNoiR けんきさんとあびつんさんとランクいってくる！！！ １行目が葛葉さんの動画タイトルで、2行目が日ノ隈らんさんの動画タイトルになっている。\n推論結果 推論結果は以下のようになった。\n[0.45 0.69] なお、1個目が1行目の結果で、2個目が2行目の結果になる。\nまた、この結果が 1.0 に近いほど日ノ隈らんさんのタイトル、 0 に近いほど葛葉さんのタイトル\u0026quot;っぽい\u0026quot;と判断できる。\n推論結果の分析 どういった理由で今回の推論結果が出たのか分析する。\nロジスティック回帰では各特徴量の係数(重み)と切片(バイアス)を見ることができる。 今回の特徴量はタイトルに含まれる単語なので、各単語について係数(重み)を見てみる。\nまず1行目のタイトルの各単語の結果は以下の通り。\n以下でwordとtf_idfというラベルで出力しているものが、それぞれ単語名及びtf-idfという手法によって得られた、各単語の重要度になる。\nただ、このデータに関してはすべての単語について、学習元データに含まれていなかったため、this word does not existと表示されている。\nつまり、上記推論結果で0.45と出ているが、これは切片(バイアス)成分によるものである。\ntitle_index: 0 word: Ice tf_idf: this word dose not exist word: breaker tf_idf: this word dose not exist word: / tf_idf: this word dose not exist word: ChroNoiR tf_idf: this word dose not exist 次に２行目のタイトルの各単語の結果は以下の通り。","title":"ロジスティック回帰を使ってYouTubeタイトルから投稿者を推論する"},{"content":"先日、都道府県間通勤・通学を考慮した感染症流行のシミュレーションを行った。\nこの時は関東のみのシミュレーションだったが、今回は全国規模の感染シミュレーションを実施してみる。\nなお、この記事ではモデルに関する説明は特に記載しないため、モデルの詳細は以下過去の記事を参照していただきたい。\n感染症数理モデルについて触りの部分だけ学ぶ | kouya17.com\n都道府県間通勤・通学を考慮したパンデミックシミュレーションもどき | kouya17.com\n※本シミュレーションや上記リンク先のシミュレーションは、実測値等には全く基づいていないため、完全に空想上のシミュレーションになります。\n本シミュレーションの結果が実情を反映していたり、今後の予測を示していたりはしません。\n利用するデータ シミュレーションを全国規模に拡張するには各都道府県の人口のデータと、各都道府県間の通勤・通学者人数のデータが必要になる。\nこれらのデータは総務省統計局「平成27年国勢調査結果」1を利用した。\nパラメータ、初期状態について SEIRモデルに置ける各パラメータは今回以下のように設定する。\n基本再生産数 \\(R_0\\) : 2.0\n平均潜伏期間 \\(l\\) : 5日\n平均発症期間 \\(i\\) : 14日\n初期状態はAM0時に、東京都に1人の感染者がいる状態とする。\n今回は都道府県間通勤・通学について\n 通勤・通学者なし 通勤・通学者あり、通勤・通学者数は特に加工しない 通勤・通学者あり、通勤・通学者数は元データの20%にする 通勤・通学者あり、通勤・通学者数は元データの0.01%にする  の4パターン、計算を実施して、通勤・通学者数が与える影響を調べてみる。\nまた、前回のシミュレーションでは休日の概念を設けていなかった。\n働きっぱなしは可哀想なので、土日は休みにし、通勤・通学はしないものとする。\nただし、祝日や土日以外の長期休暇は考慮しない。\n結果及び可視化 今回はデータが少し多くなってくるので、可視化の方法も工夫する必要がある。\njapanmapという、日本地図を都道府県別に色分けできるライブラリがあるようなので、これを利用させていただく。\n発症者比率の可視化 各都道府県について、人口に占める発症者の比率(\\(\\frac{I}{S+E+I+R}\\))を可視化する。\n発症者の比率を赤色の濃淡で表したアニメーションを以下に示す。\n通勤・通学者なしの場合 都道府県の人の移動が発生しないため、東京でしか流行しない。\n通勤・通学者ありの場合 日本全国に感染が広がる。\nただし、感染のピークは東京から遠い都道府県ほど遅くなる。\n通勤・通学者あり、ただし人数20%の場合 人数を絞る前の結果とほぼ変わらないように見える。\n通勤・通学者あり、ただし人数0.01%の場合 人数を思いっきり絞ってみると、人数を絞る前と比べて感染が広がるスピードが遅いように見える。\n1000日目時点での累計感染者数の可視化 それぞれの条件で1000日目時点での累計感染者数($E+I+R$)をグラフ化する。\n横軸を各都道府県、縦軸を累計感染者数にした棒グラフを以下に示す。\n通勤・通学における各条件の結果をそれぞれ色を変えて並べている。\n累計感染者数に関しては、通勤・通学者数を絞っても変化はない。\nソースコード 今回使用したコードは以下に置いてある。\nただし、全然整理できていない…。\n   統計局ホームページ/平成27年国勢調査/調査の結果\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://kouya17.com/posts/29/","summary":"先日、都道府県間通勤・通学を考慮した感染症流行のシミュレーションを行った。\nこの時は関東のみのシミュレーションだったが、今回は全国規模の感染シミュレーションを実施してみる。\nなお、この記事ではモデルに関する説明は特に記載しないため、モデルの詳細は以下過去の記事を参照していただきたい。\n感染症数理モデルについて触りの部分だけ学ぶ | kouya17.com\n都道府県間通勤・通学を考慮したパンデミックシミュレーションもどき | kouya17.com\n※本シミュレーションや上記リンク先のシミュレーションは、実測値等には全く基づいていないため、完全に空想上のシミュレーションになります。\n本シミュレーションの結果が実情を反映していたり、今後の予測を示していたりはしません。\n利用するデータ シミュレーションを全国規模に拡張するには各都道府県の人口のデータと、各都道府県間の通勤・通学者人数のデータが必要になる。\nこれらのデータは総務省統計局「平成27年国勢調査結果」1を利用した。\nパラメータ、初期状態について SEIRモデルに置ける各パラメータは今回以下のように設定する。\n基本再生産数 \\(R_0\\) : 2.0\n平均潜伏期間 \\(l\\) : 5日\n平均発症期間 \\(i\\) : 14日\n初期状態はAM0時に、東京都に1人の感染者がいる状態とする。\n今回は都道府県間通勤・通学について\n 通勤・通学者なし 通勤・通学者あり、通勤・通学者数は特に加工しない 通勤・通学者あり、通勤・通学者数は元データの20%にする 通勤・通学者あり、通勤・通学者数は元データの0.01%にする  の4パターン、計算を実施して、通勤・通学者数が与える影響を調べてみる。\nまた、前回のシミュレーションでは休日の概念を設けていなかった。\n働きっぱなしは可哀想なので、土日は休みにし、通勤・通学はしないものとする。\nただし、祝日や土日以外の長期休暇は考慮しない。\n結果及び可視化 今回はデータが少し多くなってくるので、可視化の方法も工夫する必要がある。\njapanmapという、日本地図を都道府県別に色分けできるライブラリがあるようなので、これを利用させていただく。\n発症者比率の可視化 各都道府県について、人口に占める発症者の比率(\\(\\frac{I}{S+E+I+R}\\))を可視化する。\n発症者の比率を赤色の濃淡で表したアニメーションを以下に示す。\n通勤・通学者なしの場合 都道府県の人の移動が発生しないため、東京でしか流行しない。\n通勤・通学者ありの場合 日本全国に感染が広がる。\nただし、感染のピークは東京から遠い都道府県ほど遅くなる。\n通勤・通学者あり、ただし人数20%の場合 人数を絞る前の結果とほぼ変わらないように見える。\n通勤・通学者あり、ただし人数0.01%の場合 人数を思いっきり絞ってみると、人数を絞る前と比べて感染が広がるスピードが遅いように見える。\n1000日目時点での累計感染者数の可視化 それぞれの条件で1000日目時点での累計感染者数($E+I+R$)をグラフ化する。\n横軸を各都道府県、縦軸を累計感染者数にした棒グラフを以下に示す。\n通勤・通学における各条件の結果をそれぞれ色を変えて並べている。\n累計感染者数に関しては、通勤・通学者数を絞っても変化はない。\nソースコード 今回使用したコードは以下に置いてある。\nただし、全然整理できていない…。\n   統計局ホームページ/平成27年国勢調査/調査の結果\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","title":"全国規模の感染シミュレーションと結果の可視化をしてみる"},{"content":"環境 Anacondaの環境情報\n\u0026gt;conda info conda version : 4.8.2 conda-build version : 3.18.11 python version : 3.7.6.final.0 platform : win-64 現象 pythonでLogisticRegression.fit()を実行したところ、以下のワーニングが出た。\nConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. 実行時のjupyter notebookは以下のような感じ。\n 一応正解率は出せているらしい。\n解決策の1つ モデルが収束していないそうなので、max_iterを明示的に設定して、反復回数を増やす。\n max_iterの初期値は100だが、上の例では1000に設定している。\n今回はこれで警告が出なくなったので、よしとする。\n参考  見て試してわかる機械学習アルゴリズムの仕組み 機械学習図鑑posted with ヨメレバ秋庭 伸也/杉山 阿聖 翔泳社 2019年04月17日 楽天ブックスで購入楽天koboで購入Amazonで購入Kindleで購入 ","permalink":"https://kouya17.com/posts/27/","summary":"環境 Anacondaの環境情報\n\u0026gt;conda info conda version : 4.8.2 conda-build version : 3.18.11 python version : 3.7.6.final.0 platform : win-64 現象 pythonでLogisticRegression.fit()を実行したところ、以下のワーニングが出た。\nConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. 実行時のjupyter notebookは以下のような感じ。\n 一応正解率は出せているらしい。\n解決策の1つ モデルが収束していないそうなので、max_iterを明示的に設定して、反復回数を増やす。\n max_iterの初期値は100だが、上の例では1000に設定している。\n今回はこれで警告が出なくなったので、よしとする。\n参考  見て試してわかる機械学習アルゴリズムの仕組み 機械学習図鑑posted with ヨメレバ秋庭 伸也/杉山 阿聖 翔泳社 2019年04月17日 楽天ブックスで購入楽天koboで購入Amazonで購入Kindleで購入 ","title":"LogisticRegression().fit()のConvergenceWarningを解消する"},{"content":"ローディング表示モジュールの調査 vue.jsでローディング表示を行う場合、ローディング表示に関係するモジュールについて調査する。 出来れば、\n 実装が簡単 よく見かけるような見た目(独自性がない) メンテされている  という条件にあてはまるものを採用したい。\n調べた結果を以下の表にまとめた。\n   名前 種類 GitHub Star 最終更新(2020/05/06時点) 備考     vue-loading Vueコンポーネント  2018/12/22 紹介している記事が多い   vue-spinner Vueコンポーネント  2017/10/07 Vue2.0をサポートしていない   Single Element CSS Spinners css  2019/12/09 よく見かける見た目   vue-loading-overlay Vueコンポーネント  2020/04/18 全画面に表示するらしい   vue-wait Vueコンポーネント  2019/10/08 だいぶ機能がリッチそう   ajaxload.info gif - - よく見かける見た目    調べて出てきた候補は以上。\n今回は見た目が個人的に好み+cssなので、レスポンシブに対応しやすそうという理由でSingle Element CSS Spinnersを使うことにする。\n実装 cssの読み込みについて今回はBladeテンプレートファイルで行う。\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ asset(\u0026#39;css/app.css\u0026#39;) }}\u0026#34;\u0026gt; あとは単一ファイルコンポーネントの方で、ロード表示を行いたところに以下のように処理を追加する。\n\u0026lt;template\u0026gt; ... \u0026lt;div class=\u0026#34;loader\u0026#34; v-if=\u0026#34;!posts.length\u0026#34;\u0026gt;Loading...\u0026lt;/div\u0026gt; ... \u0026lt;/template\u0026gt; なお、上の例ではdataプロパティにpostsというオブジェクトを持っており、ロード前は空配列になっている。\nそのため、postsが空配列の場合は、ローディング表示を行うような処理になっている。\n空配列の判定は以下の記事を参考にさせたいただいた。\n また、ロード前が空配列ではなく、nullになっている場合は、以下のように実装した。\n\u0026lt;div class=\u0026#34;loader\u0026#34; v-if=\u0026#34;!post\u0026#34;\u0026gt;Loading...\u0026lt;/div\u0026gt; nullの判定は以下の記事を参考にさせていただいた。\n また、今回実際に実装したサイトは以下のサイトになる。\nローディングが必要な箇所に簡易的なローディング表示を行っている。\n  これからはじめるVue.js実践入門 山田 祥寛 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/26/","summary":"ローディング表示モジュールの調査 vue.jsでローディング表示を行う場合、ローディング表示に関係するモジュールについて調査する。 出来れば、\n 実装が簡単 よく見かけるような見た目(独自性がない) メンテされている  という条件にあてはまるものを採用したい。\n調べた結果を以下の表にまとめた。\n   名前 種類 GitHub Star 最終更新(2020/05/06時点) 備考     vue-loading Vueコンポーネント  2018/12/22 紹介している記事が多い   vue-spinner Vueコンポーネント  2017/10/07 Vue2.0をサポートしていない   Single Element CSS Spinners css  2019/12/09 よく見かける見た目   vue-loading-overlay Vueコンポーネント  2020/04/18 全画面に表示するらしい   vue-wait Vueコンポーネント  2019/10/08 だいぶ機能がリッチそう   ajaxload.info gif - - よく見かける見た目    調べて出てきた候補は以上。","title":"Vue.jsでローディング表示を実装する"},{"content":"先日、SEIRモデルを用いた感染症流行のシミュレーションを行った。\nこのプログラムを少し拡張して、都道府県間通勤・通学を考慮したパンデミックシミュレーションもどきを行う。\n※本シミュレーションや上記リンク先のシミュレーションは、実測値等には全く基づいていないため、完全に空想上のシミュレーションになります。\n本シミュレーションの結果が実情を反映していたり、今後の予測を示していたりはしません。\n都道府県間通勤・通学をシミュレーションに組み込む 都道府県間通勤・通学の影響をシミュレーションに組み込むため、総務省統計局「平成27年国勢調査結果」1を参考にする。\n今回は東京都・群馬県・栃木県・茨城県・埼玉県・千葉県・神奈川県間の自宅外就業者数及び通学者数データを扱う。\nモデルへの組み込み方としては、移動用のグループを形成し、移動用のグループは移動先のデータと感染率を共有するという方法を採る。\nなお、今回はシミュレーション内時刻が8:00の時に通勤・通学者が一斉に(移動時間0で)移動し、シミュレーション内時刻が17:00の時に通勤・通学者が一斉に(移動時間0で)帰宅すると仮定する。\n東京都と千葉県間の人口移動を例にして説明する。\nまず、初期状態として以下のような状態になっているとする。\n値はすべて適当だが、東京都に10人感染者(\\(I\\))がいるとする。\nこの後、時間を進行させる。\nシミュレーション内の時刻が8:00になった時、人口移動用のグループを分割する。\n移動用グループの総人数は総務省統計局「平成27年国勢調査結果」1における自宅外就業者数及び通学者数に従う。\n\\(S:E:I:R\\)比は移動元の都道府県データの比と等しくする。\nここで、移動用グループは移動先の都道府県のデータと、感染率\\(\\frac{R_0I}{iN}\\)を共有する。\n感染率を共有させた状態で、時間を進行させる。\nなお、感染率を共有する関係で、非感染者から潜伏感染者への遷移に関する項については時間刻みに対して1次精度になってしまう。\nシミュレーション内の時刻が17:00になった時、人口移動用のグループを移動元のデータに合流させる。\n上の図中の値もすべて適当だが、以上のような形で、都道府県間の感染伝播をシミュレートする。\n計算する 人口データについて 人口データは各自治体のページを参考に、以下の値を使う。\n   都道府県 人口[人] 備考     東京都 13951635 2020年1月1日時点推計2   茨城県 2866325 2020年1月1日時点推計3   栃木県 1942313 2019年10月1日時点推計4   群馬県 1938053 2019年10月1日時点推計5   埼玉県 7341794 2020年4月1日時点推計6   千葉県 6280344 2020年4月1日時点推計7   神奈川県 9204965 2020年4月1日時点推計8    各都道府県毎の通勤・通学による流入・流出人口は総務省統計局「平成27年国勢調査結果」1より、以下の値(単位は[人])を使う。縦が流出元で横が流出先。\nただし、自分自身への人口移動は今回の結果に影響を与えないため、考慮しない。\n    東京都 茨城県 栃木県 群馬県 埼玉県 千葉県 神奈川県     東京都 - 7619 2770 2251 140961 82706 238314   茨城県 67284 - 22098 1166 17807 41734 3748   栃木県 17301 18175 - 23503 12067 1197 1772   群馬県 13614 1075 16561 - 27904 895 1518   埼玉県 936105 14437 10049 29250 - 43074 28111   千葉県 716882 35427 1145 791 41668 - 25966   神奈川県 1068513 2688 1433 1151 14035 14932 -    「平成27年国勢調査結果」（総務省統計局）（https://www.stat.go.jp/data/kokusei/2015/kekka.html）を加工して作成\nまた、今回は通勤・通学による都道府県間通勤の人の移動がないパターンとあるパターンで2通り計算を行う。\nパラメータ、初期状態について 感染に関する各パラメータは今回以下のように設定する。\n基本再生産数 \\(R_0\\) : 2.0\n平均潜伏期間 \\(l\\) : 5日\n平均発症期間 \\(i\\) : 14日\nまた、AM0時に、東京都に1人の感染者がいる状態を初期状態とする。\nプログラム 前回使ったプログラムを改修した。\n今回用いたプログラムは以下に置いてある。\n 結果 通勤・通学による都道府県間の人の移動がないパターン 都道府県間を移動する人数を0に設定した場合、以下のような結果になった。\n東京都以外は感染者が発生しない。\nまあ、感染する要素がないので、そうなる。\n通勤・通学による都道府県間の人の移動があるパターン 都道府県間の人の移動を考慮した場合、以下のような結果になった。\n東京都以外も東京都と同様に感染が広がる。\n感染の進行具合に違いが見られるかなと思っていたが、一目で分かるような違いはなかった。\n雑記 今回はめんどくさそうなところは極力考えないことにして、感染シミュレーションモデルもどきを拡張した。\nなかなかしんどかったが、pythonのよい勉強になった。\npythonではプライベートメソッドはオーバーライドできない9ということも今回の取り組みを通して学べた。\njapanmapという、日本地図を県別に色分けできるpython用のかなり便利なライブラリがあるようなので、これも何か活用したい。\n 感染症プラチナマニュアル 2020 岡 秀昭 Amazon 楽天市場 楽天ブックス      統計局ホームページ/平成27年国勢調査/調査の結果\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 「東京都の人口（推計）」の概要-令和2年1月1日現在｜東京都\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 茨城県の人口と世帯（推計）-令和2年（2020年）1月1日現在-／茨城県\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 栃木県／人口・面積\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 群馬県 - 群馬県の将来推計人口について\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 埼玉県推計人口 - 埼玉県\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 市区町村別人口と世帯(最新)／千葉県\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 神奈川県の人口と世帯 - 神奈川県ホームページ\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Python ではプライベートメソッドはオーバーライドできない – PIXELA Developers Blog\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://kouya17.com/posts/25/","summary":"先日、SEIRモデルを用いた感染症流行のシミュレーションを行った。\nこのプログラムを少し拡張して、都道府県間通勤・通学を考慮したパンデミックシミュレーションもどきを行う。\n※本シミュレーションや上記リンク先のシミュレーションは、実測値等には全く基づいていないため、完全に空想上のシミュレーションになります。\n本シミュレーションの結果が実情を反映していたり、今後の予測を示していたりはしません。\n都道府県間通勤・通学をシミュレーションに組み込む 都道府県間通勤・通学の影響をシミュレーションに組み込むため、総務省統計局「平成27年国勢調査結果」1を参考にする。\n今回は東京都・群馬県・栃木県・茨城県・埼玉県・千葉県・神奈川県間の自宅外就業者数及び通学者数データを扱う。\nモデルへの組み込み方としては、移動用のグループを形成し、移動用のグループは移動先のデータと感染率を共有するという方法を採る。\nなお、今回はシミュレーション内時刻が8:00の時に通勤・通学者が一斉に(移動時間0で)移動し、シミュレーション内時刻が17:00の時に通勤・通学者が一斉に(移動時間0で)帰宅すると仮定する。\n東京都と千葉県間の人口移動を例にして説明する。\nまず、初期状態として以下のような状態になっているとする。\n値はすべて適当だが、東京都に10人感染者(\\(I\\))がいるとする。\nこの後、時間を進行させる。\nシミュレーション内の時刻が8:00になった時、人口移動用のグループを分割する。\n移動用グループの総人数は総務省統計局「平成27年国勢調査結果」1における自宅外就業者数及び通学者数に従う。\n\\(S:E:I:R\\)比は移動元の都道府県データの比と等しくする。\nここで、移動用グループは移動先の都道府県のデータと、感染率\\(\\frac{R_0I}{iN}\\)を共有する。\n感染率を共有させた状態で、時間を進行させる。\nなお、感染率を共有する関係で、非感染者から潜伏感染者への遷移に関する項については時間刻みに対して1次精度になってしまう。\nシミュレーション内の時刻が17:00になった時、人口移動用のグループを移動元のデータに合流させる。\n上の図中の値もすべて適当だが、以上のような形で、都道府県間の感染伝播をシミュレートする。\n計算する 人口データについて 人口データは各自治体のページを参考に、以下の値を使う。\n   都道府県 人口[人] 備考     東京都 13951635 2020年1月1日時点推計2   茨城県 2866325 2020年1月1日時点推計3   栃木県 1942313 2019年10月1日時点推計4   群馬県 1938053 2019年10月1日時点推計5   埼玉県 7341794 2020年4月1日時点推計6   千葉県 6280344 2020年4月1日時点推計7   神奈川県 9204965 2020年4月1日時点推計8    各都道府県毎の通勤・通学による流入・流出人口は総務省統計局「平成27年国勢調査結果」1より、以下の値(単位は[人])を使う。縦が流出元で横が流出先。","title":"都道府県間通勤・通学を考慮したパンデミックシミュレーションもどき"},{"content":"目的 特にしっかりした目的はない。\nなんとなく数値シミュレーションについて学び直したくなったため、現在身近にある現象を題材にして学ぶ。\nネットの情報を参考に、適当にシミュレーションを走らせてみる。\n※本シミュレーションは、実測値等には全く基づいていないため、完全に空想上のシミュレーションになります。\n本シミュレーションの結果が実情を反映していたり、今後の予測を示していたりはしません。\n今回扱うシミュレーションモデル SEIRモデルを扱う。\nSEIRモデルとは  SEIRモデル(エスイーアイアールモデル)とは感染症流行の数理モデルである。\nモデルは\n 感染症に対して免疫を持たない者(Susceptible) 感染症が潜伏期間中の者(Exposed) 発症者(Infectious) 感染症から回復し免疫を獲得した者(Recovered)  から構成され、その頭文字をとってSEIRモデルと呼ばれる。\n(Wikipediaより)\n モデル式は以下のような式で表される。\n$$ \\begin{aligned} \\frac{dS}{dt} \u0026amp;= m(N - S) - bSI \u0026amp;(1) \\cr \\frac{dE}{dt} \u0026amp;= bSI - (m + a)E \u0026amp;(2) \\cr \\frac{dI}{dt} \u0026amp;= aE - (m + g)I \u0026amp;(3) \\cr \\frac{dR}{dt} \u0026amp;= gI - mR \u0026amp;(4) \\end{aligned} $$\nここで\\(t\\)は時間、\\(m\\)は出生率及び死亡率、\\(a\\)は感染症の発生率、\\(b\\)は感染症への感染率、\\(g\\)は感染症からの回復率を表す。\nまた\\(N\\)は全人口を示し、\n$$N \\equiv S + E + I + R　(5)$$\nで定義される。\nこの式をみて、以前私が扱ったことのある、化学反応モデルにおける各化学種の質量分率の計算式に似てるなと思った。\n化学反応も逐次的に進行するし、感染も未感染から回復まで逐次的に進行するから、モデル的には同じような扱いになるのだろうか。\n全体的に構成はシンプルだが、一つ気になったのは(1)式右辺の項bSIについて。\nこの項は非感染者と感染者の接触によってSusceptibleの人がExposedになる振る舞いを表しているのだと思う。\nここで接触率とかは対象の環境によるのだと思うのだが、式上では接触率のようなものは出てこない。\nパラメータbにそこらへんも含まれているのかもしれないが、ここら辺はもう少し調べないと分からないなと思った。\n計算してみる 上の式を数値的に解いてみる。\nシミュレーションを行う前に、上の式を少し修正し、適当にパラメータを決める。\n式を修正する 上の式はあくまで定義式なので、計算しやすいように、また、今回取り扱う問題に合うように修正する。\n修正する内容は以下の資料を参考にさせていただく。\nhttp://www.bs.s.u-tokyo.ac.jp/content/files/covid/COVID-19_SEIRmodel_full_ver4.1.pdf\n今回は扱うモデル式を以下のようにする。\n$$ \\begin{aligned} \\frac{dS}{dt} \u0026amp;= -(\\frac{R_0}{i})\\frac{S}{N}I \u0026amp;(6) \\cr \\frac{dE}{dt} \u0026amp;= -(\\frac{R_0}{i})\\frac{S}{N}I - (\\frac{1}{l})E \u0026amp;(7) \\cr \\frac{dI}{dt} \u0026amp;= (\\frac{1}{l})E - (\\frac{1}{i})I \u0026amp;(8) \\cr \\frac{dR}{dt} \u0026amp;= (\\frac{1}{i})I \u0026amp;(9) \\end{aligned} $$\nここで、\\(R_0\\)は基本再生産数、\\(l\\)は平均潜伏期間、\\(i\\)は平均発症期間である。\nwikipediaの式と比べて変わったところは、\n 出生・死亡については無視する 各パラメータについて、基本再生産数、平均潜伏期間、平均発症期間から与えられるように式を変形  なお、ここでの式変形については参考元のページに非常に分かりやすい説明があるため、そちらを参照いただきたい。\nパラメータを決める モデル式が決まったので、パラメータを決める。\n今回、式に含まれるパラメータは\\(R_0\\)、\\(l\\)、\\(i\\)の値、及び\\(S\\)、\\(E\\)、\\(I\\)、\\(R\\)の初期値である。\n\\(R_0\\)、\\(l\\)、\\(i\\)はネットの情報をもとにそれっぽい値に設定する。\n\\(R_0\\)を決める 今回のモデル式においては、このパラメータには人と人との接触率みたいなものも含まれるので、最も重要になると思う。\n基本再生産数について改めて調べてみると色々な調査結果がある。\n2月12日に公開されている論文1によると、1.4から6.49まで調査結果に開きがあるらしい。\nそれぞれ調査期間や場所、方法が違うため当然なのだが、今回はWHOが1月18日に調査したらしい結果、1.4-2.5(平均1.95)という結果を参考にする。\nとりあえず平均の1.95と、この値の変化による全体の傾向の変化をつかむため、1.5の場合と3.0の場合も計算しておく。\n\\(l\\)を決める 平均潜伏期間について調査する。\nこれについても様々な調査結果が報告されている。\nJohns Hopkins Bloomberg School of Public Healthの調査2によると、\n There were 181 confirmed cases with identifiable exposure and symptom onset windows to estimate the incubation period of COVID-19. The median incubation period was estimated to be 5.1 days (95% CI, 4.5 to 5.8 days), and 97.5% of those who develop symptoms will do so within 11.5 days (CI, 8.2 to 15.6 days) of infection.\n とあり、感染から発症までの潜伏期間の中央値は5.1日らしい。\n日本感染症学会のページ3には、\n 潜伏期間は1~14日で平均5.8日と報告されている。\n と書かれている。\n日本国内の自治体の報告例としては、福井県が、県内の罹患者のうち、潜伏期間が推定できたケースの平均値は5.2日であると報告4している。\nこれらの報告例から、今回の平均潜伏期間$l$はとりあえず5日と設定する。\n\\(i\\)を決める 平均発症期間について調査する。\nWHOの中国における調査結果5によると、\n Using available preliminary data, the median time from onset to clinical recovery for mild cases is approximately 2 weeks and is 3-6 weeks for patients with severe or critical disease. Preliminary data suggests that the time period from onset to the development of severe disease, including hypoxia, is 1 week. Among patients who have died, the time from symptom onset to outcome ranges from 2-8 weeks.\n とあり、軽症の場合は約2週間、重症以上の場合は3～6週間かかるらしい。\n国内の報告例としては国立感染症研究所が\n 入院開始日と退院日ともに判明している102例における入院期間の平均値（標準偏差）は14.3日（±5.2日）であった。\n と報告6している。\nこれらの報告例から、今回の平均発症期間$i$はとりあえず14日と設定する。\n\\(S\\),\\(E\\),\\(I\\),\\(R\\)の初期値を決める これは今回の本質とはあまり関係ないと思われる。\nとりあえず\\(S\\)の初期値は東京都の人口13,951,636人(令和2年1月1日推計7)に設定し、\\(I\\)の初期値は1、他の初期値は0にする。\nよって、東京都に一人の感染者がいる状態を初期状態とする。(もちろん都道府県間の人の移動は今回考慮しない。)\nプログラムを作成する pythonで書く。\nこの方程式は時間微分しかないためエクセルでも数値的に解けるが、勉強のためpythonを使ってみる。\nとくに急峻な値の変化はなさそうなので、数値安定性とかを気にする必要はなさそうだが、せっかくなので数値解析してる風にするため、時間発展手法には4段4次陽的ルンゲクッタ法を使う。\n4段4次陽的ルンゲクッタ法の計算式は以下のサイトを参考にさせていただいた。\n 作成したプログラムを以下に載せる。\n 計算結果を確認する とりあえず今回作成プログラムに問題がないか、参考元と同じパラメータを設定して結果を確認する。\n参考元の\\(R_0\\)=2.0のパターンと同じ条件で計算した結果、以下のようなグラフが得られた。\n大体同じ結果が得られていそうなのでよしとする。\n次に、先ほど検討したパラメータを設定して再度計算を行う。\n\\(R_0\\)=1.5の場合\n\\(R_0\\)=1.95の場合\n\\(R_0\\)=3.0の場合\n\\(R_0\\)が大きい(感染力が強い、人と人の接触率が高い)ほど\n ピークが急激 罹患者数が多い 収束が早い  という結果になるようだ。\n雑記 久しぶりに数値計算のコードを書いたが、やはり数値計算は対象とする現象への理解が重要だなと思った。 今回に関しても、コードを作成している時間よりも、パラメータ設定のための調査の方に時間がかかった。\n今回の感染症について調べてみて、色々なところが調査結果を出していることが改めて分かった。\nまた、ネットに落ちている情報としては予防や感染までの情報が多く、感染後の情報は比較的少ないと感じた。\n 感染症プラチナマニュアル 2020 岡 秀昭 Amazon 楽天市場 楽天ブックス      reproductive number of COVID-19 is higher compared to SARS coronavirus | Journal of Travel Medicine | Oxford Academic\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The Incubation Period of COVID-19 From Publicly Reported Confirmed Cases | Annals of Internal Medicine | American College of Physicians\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 新型コロナウイルス感染症（COVID-19 infection）｜症状からアプローチするインバウンド感染症への対応～東京2020大会にむけて～ - 感染症クイック・リファレンス｜日本感染症学会\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 新型コロナ潜伏期間は平均「5．2日」　福井県が暫定値算出　入院日数は「15．7日間」（福井新聞ＯＮＬＩＮＥ） - Yahoo!ニュース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://www.who.int/docs/default-source/coronaviruse/who-china-joint-mission-on-covid-19-final-report.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 感染症発生動向調査及び積極的疫学調査により報告された新型コロナウイルス感染症確定症例287例の記述疫学（2020年3月9日現在）\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n 「東京都の人口（推計）」の概要-令和2年1月1日現在｜東京都\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://kouya17.com/posts/24/","summary":"目的 特にしっかりした目的はない。\nなんとなく数値シミュレーションについて学び直したくなったため、現在身近にある現象を題材にして学ぶ。\nネットの情報を参考に、適当にシミュレーションを走らせてみる。\n※本シミュレーションは、実測値等には全く基づいていないため、完全に空想上のシミュレーションになります。\n本シミュレーションの結果が実情を反映していたり、今後の予測を示していたりはしません。\n今回扱うシミュレーションモデル SEIRモデルを扱う。\nSEIRモデルとは  SEIRモデル(エスイーアイアールモデル)とは感染症流行の数理モデルである。\nモデルは\n 感染症に対して免疫を持たない者(Susceptible) 感染症が潜伏期間中の者(Exposed) 発症者(Infectious) 感染症から回復し免疫を獲得した者(Recovered)  から構成され、その頭文字をとってSEIRモデルと呼ばれる。\n(Wikipediaより)\n モデル式は以下のような式で表される。\n$$ \\begin{aligned} \\frac{dS}{dt} \u0026amp;= m(N - S) - bSI \u0026amp;(1) \\cr \\frac{dE}{dt} \u0026amp;= bSI - (m + a)E \u0026amp;(2) \\cr \\frac{dI}{dt} \u0026amp;= aE - (m + g)I \u0026amp;(3) \\cr \\frac{dR}{dt} \u0026amp;= gI - mR \u0026amp;(4) \\end{aligned} $$\nここで\\(t\\)は時間、\\(m\\)は出生率及び死亡率、\\(a\\)は感染症の発生率、\\(b\\)は感染症への感染率、\\(g\\)は感染症からの回復率を表す。\nまた\\(N\\)は全人口を示し、\n$$N \\equiv S + E + I + R　(5)$$","title":"感染症数理モデルについて触りの部分だけ学ぶ"},{"content":" Chart.js公式のドキュメントを参考にして、Chart.jsで時刻データを扱う場合の描画オプションについて確認する。\n入力データ 入力データはx軸データをy軸データを各点それぞれ指定する。\n今回はx軸データに時刻データを使う。\n例としては、以下のように指定すればOK。\ndata: [{ x: \u0026#39;1995-12-17T00:00:00\u0026#39;, y: 1 }, { x: \u0026#39;1995-12-18T00:00:00\u0026#39;, y: 10 }, { x: \u0026#39;1995-12-21T00:00:00\u0026#39;, y: 20 }, { x: \u0026#39;1995-12-25T12:00:00\u0026#39;, y: 30 }, { x: \u0026#39;1996-01-01T00:00:00\u0026#39;, y: 40 }] データフォーマット 時刻スケールのデータを使う場合、フォーマットはMoment.jsが扱えるフォーマットであればどういった形式でも良い。詳細はMoment.jsのドキュメント参照。\n描画オプション 時刻目盛りを扱う場合、以下のオプションを設定できる。\nadapters.date 外部の時刻ライブラリ(つまりMoment.js以外)を使うためのオプション。\ndistribution データのプロット方法。\n指定できる値はlinearまたはseriesのいずれか。\n初期値はlinear。\nlinearはデータ描画間隔が時間間隔に応じて変化する。\nseriesはデータ描画間隔がすべて同じになる。\n以下に同一データを用いてdistribution設定のみ変えた場合の描画例を示す。\n linearを指定した場合  \nvar ctx = document.getElementById('distribution_linear').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'linear', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1995-12-17T00:00:00', y: 1 }, { x: '1995-12-18T00:00:00', y: 10 }, { x: '1995-12-21T00:00:00', y: 20 }, { x: '1995-12-25T12:00:00', y: 30 }, { x: '1996-01-01T00:00:00', y: 40 }] }] }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', //! distribution に linear を指定 distribution: 'linear', time: { unit: 'day' } }] } } });    seriesを指定した場合  \nvar ctx = document.getElementById('distribution_series').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'series', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1995-12-17T00:00:00', y: 1 }, { x: '1995-12-18T00:00:00', y: 10 }, { x: '1995-12-21T00:00:00', y: 20 }, { x: '1995-12-25T12:00:00', y: 30 }, { x: '1996-01-01T00:00:00', y: 40 }] }] }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', //! distribution に series を指定 distribution: 'series', time: { unit: 'day' } }] } } });   bounds 時刻目盛りの境界の描画方法。\n指定できる値はdataまたはticksのいずれか。\n初期値はdata。\ndataはすべてのデータが見えるように描画する。描画エリア外に出てしまったラベルは取り除かれる。\nticksはすべての刻みラベルが見えるように描画する。描画エリア外に出てしまったデータは取り除かれる。\nこの設定によってどう見た目が変わるか色々なパターンで確認したが、結局どういったパターンで違いが出るのか分からなかった。\nticks.source 目盛り刻み生成方法。\n指定できる値はauto、data、labelsのいずれか。\n初期値はauto。\nautoは範囲やほかの設定に基づいて最適な目盛りを生成する。\ndataはdata項目(及びそれらに紐づくlabels項目)から目盛りを生成する。\nlabelsはlabels項目のみから目盛りを生成する。\n以下に同一データを用いてticks.source設定のみ変えた場合の描画例を示す。\n autoを指定した場合  \nvar ctx = document.getElementById('ticks_source_auto').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'ticks.source - auto', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1995-12-17T00:00:00', y: 1 }, { x: '1995-12-18T00:00:00', y: 10 }, { x: '1995-12-21T00:00:00', y: 20 }, { x: '1995-12-25T12:00:00', y: 30 }, { x: '1996-01-10T00:00:00', y: 40 }] }], labels: ['1996-01-01T00:00:00'] }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', ticks: { //! ticks.source に auto を指定 source: 'auto' }, time: { unit: 'day' } }] } } });    dataを指定した場合  \nvar ctx = document.getElementById('ticks_source_data').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'ticks.source - data', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1995-12-17T00:00:00', y: 1 }, { x: '1995-12-18T00:00:00', y: 10 }, { x: '1995-12-21T00:00:00', y: 20 }, { x: '1995-12-25T12:00:00', y: 30 }, { x: '1996-01-10T00:00:00', y: 40 }] }], labels: ['1996-01-01T00:00:00'] }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', ticks: { //! ticks.source に data を指定 source: 'data' } }] } } });    labelsを指定した場合  \nvar ctx = document.getElementById('ticks_source_labels').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'ticks.source - labels', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1995-12-17T00:00:00', y: 1 }, { x: '1995-12-18T00:00:00', y: 10 }, { x: '1995-12-21T00:00:00', y: 20 }, { x: '1995-12-25T12:00:00', y: 30 }, { x: '1996-01-10T00:00:00', y: 40 }] }], labels: ['1996-01-01T00:00:00'] }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', ticks: { //! ticks.source に labels を指定 source: 'labels' }, time: { unit: 'day' } }] } } });   time.displayFormats 時間刻みをそれぞれどのように文字列に変換するかを設定する。\n書式はMoment.jsのページ参照。\n以下に同一データを用いてtime.displayFormats設定(及びtime.unit設定)を変えた場合の描画例を示す。\n time.displayFormatsのday設定を変更した場合  \nvar ctx = document.getElementById('time_displayFormats_day').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'time.displayFormats - day', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1994-01-01T00:00:00', y: 1 }, { x: '1995-01-01T00:00:00', y: 10 }, { x: '1996-01-01T00:00:00', y: 20 }] }], }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', time: { //! 横軸の目盛り刻みを日刻みにする unit: 'day', displayFormats: { //! displayFormats の day を設定 day: 'D' } } }] } } });    time.displayFormatsのmonth設定を変更した場合  \nvar ctx = document.getElementById('time_displayFormats_month').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'time.displayFormats - month', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1994-01-01T00:00:00', y: 1 }, { x: '1995-01-01T00:00:00', y: 10 }, { x: '1996-01-01T00:00:00', y: 20 }] }], }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', time: { //! 横軸の目盛り刻みを月刻みにする unit: 'month', displayFormats: { //! displayFormats の month を設定 month: 'MMM' } } }] } } });    time.displayFormatsのyear設定を変更した場合  \nvar ctx = document.getElementById('time_displayFormats_year').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'time.displayFormats - year', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1994-01-01T00:00:00', y: 1 }, { x: '1995-01-01T00:00:00', y: 10 }, { x: '1996-01-01T00:00:00', y: 20 }] }], }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', time: { //! 横軸の目盛り刻みを年刻みにする unit: 'year', displayFormats: { //! displayFormats の year を設定 year: 'YYYY' } } }] } } });   time.isoWeekday trueに設定し、かつ表示刻み(time.unit)がweekに設定されている場合、週の最初の日が月曜日になる。それ以外の場合は日曜日始まりになる。\n初期値はfalse。\n以下に同一データを用いてtime.isoWeekday設定のみを変えた場合の描画例を示す。\n falseを指定した場合  \nvar ctx = document.getElementById('time_isoWeekday_false').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'time.isoWeekday - false', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '2020-04-26T00:00:00', y: 1 }, { x: '2020-04-27T00:00:00', y: 2 }, { x: '2020-05-03T00:00:00', y: 3 }, { x: '2020-05-04T00:00:00', y: 4 }] }], }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', time: { //! 横軸の目盛り刻みを週刻みにする unit: 'week', //! isoWeekday を false に設定する isoWeekday: false, displayFormats: { week: 'YYYY/M/D ddd' } } }] } } });    trueを指定した場合  \nvar ctx = document.getElementById('time_isoWeekday_true').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'time.isoWeekday - true', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '2020-04-26T00:00:00', y: 1 }, { x: '2020-04-27T00:00:00', y: 2 }, { x: '2020-05-03T00:00:00', y: 3 }, { x: '2020-05-04T00:00:00', y: 4 }] }], }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', time: { //! 横軸の目盛り刻みを週刻みにする unit: 'week', //! isoWeekday を true に設定する isoWeekday: true, displayFormats: { week: 'YYYY/M/D ddd' } } }] } } });   time.parser 日時データのパーサを設定する。\n文字列で指定した場合、カスタム書式として認識され、Moment.jsが日時データをパースする際に使われる。\n関数で指定した場合、適切な日時情報を持つMoment.jsオブジェクトを返さなければならない。\ntime.round この設定に従って日時が丸められる。\n指定できる値はtime.unitと同様。\n初期値はfalse。\n以下に同一データを用いてtime.round設定のみを変えた場合の描画例を示す。\n falseを指定した場合  \nvar ctx = document.getElementById('time_round_false').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'time.round - false', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '2020-04-26T00:00:00', y: 1 }, { x: '2020-04-27T00:00:00', y: 2 }, { x: '2020-05-03T00:00:00', y: 3 }, { x: '2020-05-04T00:00:00', y: 4 }] }], }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', time: { //! 横軸の目盛り刻みを週刻みにする unit: 'week', //! time.round を false に設定する round: false, displayFormats: { week: 'YYYY/M/D ddd' } } }] } } });    'week'を指定した場合  \nvar ctx = document.getElementById('time_round_week').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'time.round - week', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '2020-04-26T00:00:00', y: 1 }, { x: '2020-04-27T00:00:00', y: 2 }, { x: '2020-05-03T00:00:00', y: 3 }, { x: '2020-05-04T00:00:00', y: 4 }] }], }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', time: { //! 横軸の目盛り刻みを週刻みにする unit: 'week', //! time.round を week に設定する round: 'week', displayFormats: { week: 'YYYY/M/D ddd' } } }] } } });   time.tooltipFormat ツールチップ(マウスオーバー時の表示)に使われるMoment.jsの書式文字列を指定する。\ntime.unit この項目が設定されていた場合、目盛り刻みが設定に従って固定される。\n指定できる値はmillisecond、second、minute、hour、day、week、month、quarter、yearのいずれか。\n初期値はfalse。\n以下に同一データを用いてtime.unit設定のみを変えた場合の描画例を示す。\n 'day'を指定した場合  \nvar ctx = document.getElementById('time_unit_day').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'time.unit - day', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1994-01-01T00:00:00', y: 1 }, { x: '1994-02-01T00:00:00', y: 10 }, { x: '1994-03-01T00:00:00', y: 20 }] }], }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', time: { //! 横軸の目盛り刻みを日刻みにする unit: 'day', } }] } } });    'month'を指定した場合  \nvar ctx = document.getElementById('time_unit_month').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'time.unit - month', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1994-01-01T00:00:00', y: 1 }, { x: '1994-02-01T00:00:00', y: 10 }, { x: '1994-03-01T00:00:00', y: 20 }] }], }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', time: { //! 横軸の目盛り刻みを月刻みにする unit: 'month', } }] } } });   time.stepSize 目盛り線の間の刻み幅。\n数値で指定し、初期値は1。\n以下に同一データを用いてtime.stepSize設定のみを変えた場合の描画例を示す。\n 1を指定した場合  \nvar ctx = document.getElementById('time_stepSize_1').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'time.stepSize - 1', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1994-01-01T00:00:00', y: 1 }, { x: '1994-02-01T00:00:00', y: 10 }, { x: '1994-03-01T00:00:00', y: 20 }] }], }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', time: { //! 横軸の目盛り刻みを日刻みにする unit: 'day', //! 目盛り線の刻みを1にする stepSize: 1 } }] } } });    10を指定した場合  \nvar ctx = document.getElementById('time_stepSize_10').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'time.stepSize - 10', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1994-01-01T00:00:00', y: 1 }, { x: '1994-02-01T00:00:00', y: 10 }, { x: '1994-03-01T00:00:00', y: 20 }] }], }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', time: { //! 横軸の目盛り刻みを日刻みにする unit: 'day', //! 目盛り線の刻みを10にする stepSize: 10 } }] } } });   time.minUnit 時間刻み幅に使われる最小単位。\n文字列で指定し、初期値はmillisecond。\n 確かな力が身につくJavaScript「超」入門 第2版 狩野 祐東 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/23/","summary":"Chart.js公式のドキュメントを参考にして、Chart.jsで時刻データを扱う場合の描画オプションについて確認する。\n入力データ 入力データはx軸データをy軸データを各点それぞれ指定する。\n今回はx軸データに時刻データを使う。\n例としては、以下のように指定すればOK。\ndata: [{ x: \u0026#39;1995-12-17T00:00:00\u0026#39;, y: 1 }, { x: \u0026#39;1995-12-18T00:00:00\u0026#39;, y: 10 }, { x: \u0026#39;1995-12-21T00:00:00\u0026#39;, y: 20 }, { x: \u0026#39;1995-12-25T12:00:00\u0026#39;, y: 30 }, { x: \u0026#39;1996-01-01T00:00:00\u0026#39;, y: 40 }] データフォーマット 時刻スケールのデータを使う場合、フォーマットはMoment.jsが扱えるフォーマットであればどういった形式でも良い。詳細はMoment.jsのドキュメント参照。\n描画オプション 時刻目盛りを扱う場合、以下のオプションを設定できる。\nadapters.date 外部の時刻ライブラリ(つまりMoment.js以外)を使うためのオプション。\ndistribution データのプロット方法。\n指定できる値はlinearまたはseriesのいずれか。\n初期値はlinear。\nlinearはデータ描画間隔が時間間隔に応じて変化する。\nseriesはデータ描画間隔がすべて同じになる。\n以下に同一データを用いてdistribution設定のみ変えた場合の描画例を示す。\n linearを指定した場合  \nvar ctx = document.getElementById('distribution_linear').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'linear', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1995-12-17T00:00:00', y: 1 }, { x: '1995-12-18T00:00:00', y: 10 }, { x: '1995-12-21T00:00:00', y: 20 }, { x: '1995-12-25T12:00:00', y: 30 }, { x: '1996-01-01T00:00:00', y: 40 }] }] }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', //!","title":"Chart.jsで時刻データを扱う"},{"content":"Vue.jsでSPAを構成した際、TwitterなどのOGPが上手く表示されなかったので、対処した。\nSPAとは?→シングルページアプリケーション この記事で扱っているSPAとは、シングルページアプリケーションを指す。\nSPAについては少し以下の記事で触れたが、詳細は他のサイトの方が詳しいと思う。\n OGPとは?→Webページのリンクを表示する仕組み OGPとはOpen Graph Protocolの略で、TwitterとかFacebookとかでリンクがいい感じに表示されるアレに使われているプロトコルのこと。\n↓これ。\nChart.jsで横軸が日付のグラフを作成する https://t.co/evu4HqqQ4f @aoki_kouyaさんから\n\u0026mdash; 青木晃也 (@aoki_kouya) April 4, 2020  webサイトをいちから作る際、OGP対応していないと、Twitterとかでリンクを貼り付けても画像や説明文は表示されない。\nそこらへんはTwitter側でなんとかしてくれるわけではなく、サイト作成者側が適切にOGP設定を行う必要がある。\nSPAの場合はOGP設定にひと工夫必要 OGPは、具体的にはHTMLにmetaタグを設定することで対応できる。\n例えば、以下のようなmetaタグを\u0026lt;head\u0026gt;要素内に設定する。\n 上記は私が作成したサイトの実装例になるが、他のサイトを調べると、head要素にprefix属性を適切に設定する必要があるようだ。\n私は設定していなかったが、とりあえずTwitterあたりでは問題なく動いているように見える。\nまあ、正しい対応ではないと思う。\nこのOGP設定だが、SPAの場合はひと工夫必要になる。\n理由は、OGPによってリンクを作成する際、JavaScriptが動かないケースがあるため。\nOGPによってリンクを表示する際、クローラーと呼ばれるプログラムによって、リンク先のページをスキャンする。\nスキャン結果に従ってリンクを表示するのだが、TwitterやFacebookのクローラーはJavaScriptを実行してくれないらしい。\nよって、JavaScriptによってmetaタグを生成しても、リンクには反映されない。\nSPAでOGP設定を適切に行う場合はここら辺の対処が必要。\n今回は特定条件時のみ静的ページを返すように実装 今回の結論、解決策になる。\n調べてみると、(Vue.jsを用いた)SPAでのOGP設定については色々対応策があった。\nいくつか参考にさせていただいたページを挙げておく。\n 上記のページの方法はページ内にも書かれている通り、全てのページで同じ内容のOGPになってしまう。\n今回はそれぞれのページで異なるOGPを設定したいため、同じ方法はとらなかった。\n 上記のページは詳しく書いてあり、参考になりそうだったが、Firebaseを使ってない、使ったことがないため今回は同じ方法をとれなかった。\n最終的には以下のページの方法を真似させていただいた。\n 上のページの方法は、TwitterやFacebookなど特定のクローラーに対してのみOGP対応用のテンプレートを返すというものになっている。\nアプローチとしては一番最初に挙げたページと同じだが、こちらはクローラー検出用にControllerを作成して、テンプレートの出し分けをしている。\n比較的容易に実装でき、ページごとに異なるOGP設定ができそうだったため、こちらの方法を採用した。\nこの方法の課題としては、OGP対応用のテンプレートを返す相手をピンポイントで指定する必要があるため、明示的に指定したクローラー以外に対しては対応できない点がある。\nまた、OGP対応用のControllerを新規に作成する必要があり、Controllerが単純に1個増える。\nなお、各サービスのクローラーの情報は以下のサイトを参考にさせていただいた。\n 雑記 Twitterでのリンク表示内容確認には以下の公式のサイトが便利。\n  これからはじめるVue.js実践入門 山田 祥寛 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/22/","summary":"Vue.jsでSPAを構成した際、TwitterなどのOGPが上手く表示されなかったので、対処した。\nSPAとは?→シングルページアプリケーション この記事で扱っているSPAとは、シングルページアプリケーションを指す。\nSPAについては少し以下の記事で触れたが、詳細は他のサイトの方が詳しいと思う。\n OGPとは?→Webページのリンクを表示する仕組み OGPとはOpen Graph Protocolの略で、TwitterとかFacebookとかでリンクがいい感じに表示されるアレに使われているプロトコルのこと。\n↓これ。\nChart.jsで横軸が日付のグラフを作成する https://t.co/evu4HqqQ4f @aoki_kouyaさんから\n\u0026mdash; 青木晃也 (@aoki_kouya) April 4, 2020  webサイトをいちから作る際、OGP対応していないと、Twitterとかでリンクを貼り付けても画像や説明文は表示されない。\nそこらへんはTwitter側でなんとかしてくれるわけではなく、サイト作成者側が適切にOGP設定を行う必要がある。\nSPAの場合はOGP設定にひと工夫必要 OGPは、具体的にはHTMLにmetaタグを設定することで対応できる。\n例えば、以下のようなmetaタグを\u0026lt;head\u0026gt;要素内に設定する。\n 上記は私が作成したサイトの実装例になるが、他のサイトを調べると、head要素にprefix属性を適切に設定する必要があるようだ。\n私は設定していなかったが、とりあえずTwitterあたりでは問題なく動いているように見える。\nまあ、正しい対応ではないと思う。\nこのOGP設定だが、SPAの場合はひと工夫必要になる。\n理由は、OGPによってリンクを作成する際、JavaScriptが動かないケースがあるため。\nOGPによってリンクを表示する際、クローラーと呼ばれるプログラムによって、リンク先のページをスキャンする。\nスキャン結果に従ってリンクを表示するのだが、TwitterやFacebookのクローラーはJavaScriptを実行してくれないらしい。\nよって、JavaScriptによってmetaタグを生成しても、リンクには反映されない。\nSPAでOGP設定を適切に行う場合はここら辺の対処が必要。\n今回は特定条件時のみ静的ページを返すように実装 今回の結論、解決策になる。\n調べてみると、(Vue.jsを用いた)SPAでのOGP設定については色々対応策があった。\nいくつか参考にさせていただいたページを挙げておく。\n 上記のページの方法はページ内にも書かれている通り、全てのページで同じ内容のOGPになってしまう。\n今回はそれぞれのページで異なるOGPを設定したいため、同じ方法はとらなかった。\n 上記のページは詳しく書いてあり、参考になりそうだったが、Firebaseを使ってない、使ったことがないため今回は同じ方法をとれなかった。\n最終的には以下のページの方法を真似させていただいた。\n 上のページの方法は、TwitterやFacebookなど特定のクローラーに対してのみOGP対応用のテンプレートを返すというものになっている。\nアプローチとしては一番最初に挙げたページと同じだが、こちらはクローラー検出用にControllerを作成して、テンプレートの出し分けをしている。\n比較的容易に実装でき、ページごとに異なるOGP設定ができそうだったため、こちらの方法を採用した。\nこの方法の課題としては、OGP対応用のテンプレートを返す相手をピンポイントで指定する必要があるため、明示的に指定したクローラー以外に対しては対応できない点がある。\nまた、OGP対応用のControllerを新規に作成する必要があり、Controllerが単純に1個増える。\nなお、各サービスのクローラーの情報は以下のサイトを参考にさせていただいた。\n 雑記 Twitterでのリンク表示内容確認には以下の公式のサイトが便利。\n  これからはじめるVue.js実践入門 山田 祥寛 Amazon Kindle 楽天 楽天Kobo   ","title":"Vue.jsを使ったSPAにおいてTwitter等でのリンク表示がいい感じになるようにする"},{"content":"webサイト上で横軸が日付情報のグラフを作りたかったのでChart.js周りで色々調べた。\n横軸が単純な数値じゃない場合は面倒くさくなるかなと思っていたが、Chart.jsが非常に使いやすく、すんなりできた。\n最終的に作ったもの 画像で貼ろうと思ったが、結構サイズが大きいのでリンクで貼り付けておく。\n Chart.jsとは  グラフ類を描画するためのJavaScriptライブラリ。\nHTMLの\u0026lt;canvas\u0026gt;要素に2Dグラフィックを描画できる。\nライセンスはMIT。\nChat.jsのインストールと使い方 基本的な使い方を公式に従って確認してみる。\nインストール 今回はとりあえず手軽そうなCDNを使う。\nよってHTMLに以下のタグを追加する。\n\u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.3/Chart.bundle.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; なお、ここで Chart.bundle.min.js を選択したのは、今回日付データを扱うため。\nChart.min.js の方は日付データを扱うためのライブラリMoment.jsが含まれていない。\n使い方 HTML側はグラフ描画用の\u0026lt;canvas\u0026gt;タグを作成する。\n先ほどのCDNの参照及び今回作成するJavaScriptの読み込みを合わせて、HTML側のソースは以下のようになる。\nindex.html  JavaScript側は公式のサンプルを参考にして以下のようにする。\nmain.js  これら2つのファイルを適当なところに保存して、index.htmlをブラウザで開く。\n以下のようなグラフがブラウザ上に表示される。\n横軸が日付情報のデータをプロットする サンプルが動いたので、今回扱いたい、横軸が日付情報のデータをプロットする。\nHTML側は特に変更せず、JavaScript側を以下のように変更する。\nmain.js  参考ページ\nmain.jsを上記のように変更し、index.htmlをブラウザで開くと以下のようなグラフが描画される。\n横軸が日付になっており、横軸の間隔もデータに合わせていい感じになっている。\nまとめ Chart.jsを使って横軸が日付情報のデータをプロットできた。\n色々と描画オプションがあるようなので、それぞれ変更させるとどのようにグラフが変化するか、また今度まとめたい。\n 確かな力が身につくJavaScript「超」入門 第2版 狩野 祐東 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/21/","summary":"webサイト上で横軸が日付情報のグラフを作りたかったのでChart.js周りで色々調べた。\n横軸が単純な数値じゃない場合は面倒くさくなるかなと思っていたが、Chart.jsが非常に使いやすく、すんなりできた。\n最終的に作ったもの 画像で貼ろうと思ったが、結構サイズが大きいのでリンクで貼り付けておく。\n Chart.jsとは  グラフ類を描画するためのJavaScriptライブラリ。\nHTMLの\u0026lt;canvas\u0026gt;要素に2Dグラフィックを描画できる。\nライセンスはMIT。\nChat.jsのインストールと使い方 基本的な使い方を公式に従って確認してみる。\nインストール 今回はとりあえず手軽そうなCDNを使う。\nよってHTMLに以下のタグを追加する。\n\u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.3/Chart.bundle.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; なお、ここで Chart.bundle.min.js を選択したのは、今回日付データを扱うため。\nChart.min.js の方は日付データを扱うためのライブラリMoment.jsが含まれていない。\n使い方 HTML側はグラフ描画用の\u0026lt;canvas\u0026gt;タグを作成する。\n先ほどのCDNの参照及び今回作成するJavaScriptの読み込みを合わせて、HTML側のソースは以下のようになる。\nindex.html  JavaScript側は公式のサンプルを参考にして以下のようにする。\nmain.js  これら2つのファイルを適当なところに保存して、index.htmlをブラウザで開く。\n以下のようなグラフがブラウザ上に表示される。\n横軸が日付情報のデータをプロットする サンプルが動いたので、今回扱いたい、横軸が日付情報のデータをプロットする。\nHTML側は特に変更せず、JavaScript側を以下のように変更する。\nmain.js  参考ページ\nmain.jsを上記のように変更し、index.htmlをブラウザで開くと以下のようなグラフが描画される。\n横軸が日付になっており、横軸の間隔もデータに合わせていい感じになっている。\nまとめ Chart.jsを使って横軸が日付情報のデータをプロットできた。\n色々と描画オプションがあるようなので、それぞれ変更させるとどのようにグラフが変化するか、また今度まとめたい。\n 確かな力が身につくJavaScript「超」入門 第2版 狩野 祐東 Amazon Kindle 楽天 楽天Kobo   ","title":"Chart.jsで横軸が日付のグラフを作成する"},{"content":"今回作成したサイト 今回作成したサイトは以下のサイト。\nClubPage\n私が所属している電子工作サークルのホームページを作った。\nこのブログからcssを流用しているので、見た目がだいぶ似ている。\n構成  Laravel Vue.js  バックエンドはLaravel、フロントエンドはVue.jsを使っている。\nシングルページアプリケーションとは 今回はシングルページアプリケーション(SPA)という構成を採用した。\nシングルページアプリケーションとは\n シングルページアプリケーション（英: single-page application、SPA）とは、単一のWebページのみから構成することで、デスクトップアプリケーションのようなユーザ体験を提供するWebアプリケーションまたはWebサイトである。必要なコード（HTML、JavaScript、CSS）は最初にまとめて読み込むか[1]、ユーザの操作などに応じて動的にサーバと通信し、必要なものだけ読み込みを行う。 Wikipediaより\n というものらしい。\nこれまでRuby on RailsやDjangoとかを使ってwebサイトを作ってみてきたが、どれも各ページ毎に、サーバ側でHTMLの生成を行うマルチページアプリケーションだった。\n今回はLaravelでwebAPIを作成し、そのAPIから取得した情報を、Vue.jsを使ってブラウザ上に表示するような構成にした。\nなぜSPAにしたのか 理由は1つで、今回参考にしたサイトがSPAを採用していたから。\n今回参考にしたサイトは以下のサイト。\n このサイトにあるチュートリアル、書かれている情報量がかなり多くて、これだけの情報をタダで得られるのが信じられない。\n今回初めてLaravelとVue.jsを触ったが、このサイトのおかげで色々と基礎を学ぶことができた。\n今回学んだこと、感じたこと インタフェース部とエンジン部の分離はやはり重要 今回の構成はAPI(Laravel)と表示部(Vue.js)を分離している。\nたとえばhttps://home.lchika.club/api/tagsにアクセスすれば、Laravelの機能によって、タグの一覧をJSON形式で取得できる。\nこういったAPIから取得した情報をVue.jsで処理して、ブラウザにレンダリングする。\nこのように分離することで、UIを変えたいときは、基本的にVue.jsの部分のみを修正するだけですむ。\nLaravelの処理部は一切手を加える必要がない。\nまあ、一般的なwebフレームワークを使っていれば、実装の分離は適切に行われているが、ある一部分のフレームワークをごっそり切り替えることはできない。\n今回の構成では、例えばフロントエンドにReactを使いたければ、Laravel部分はそのままで、フロントエンド部分をごっそり切り替えられる(と思うけど、Laravel-mixとかの関係で難しいのかもしれない\u0026hellip;)。\nこのように、インタフェースとエンジン部の分離で得られる効果はやはりでかいなと思った。\nローディング表示がないとwebサイトの質が落ちる これは個人的な印象かもしれないが、ローディング画面の表示はユーザにとって(ある程度)重要だなと思った。\n現状ではローディング表示は実装していないため、読み込み直後は一部のコンテンツが表示されていないし、一部のコンテンツについては結構な時間表示されないことがある。\nこれはサイト利用者側からすると、違和感を覚える要因になり、満足度の低下につながると思った。\nYouTubeとかもローディング中はコンテンツ表示部をグレーで表示したりしている。\n最初はこれ意味あるのか?と思っていたが、今回自分でJavaScriptを多用したサイトを構築してみて、こういった対応も重要だなと思った。\nWYSIWYGエディタの選定は毎回の課題 今回はブログのようなサイトを構築したので、フォームから記事を投稿できる必要があった。\nさすがにHTML直打ちで入力はできないし、リッチテキストエディタを採用する必要があった。\n毎回どのリッチテキストエディタを使うか苦労するが、今回も例に漏れず苦労した。\nとりあえず今のところはCKEditorを使っている。\nただ、まだ課題があって、iframeの挿入が上手くいっていない。\nいつも大体、リッチテキストエディタまわりは\n 画像の挿入 画像の整形 外部コンテンツの挿入 目次の作成  あたりで詰まっている。\n今回はまだ目次の作成について考えていないので、目次が必要になった時に多分また詰まる。\nSPAのパフォーマンス向上はバンドルサイズの削減こそ正義 今回作成したサイトのパフォーマンス向上については以下の記事に書いた。\n 最初特に何も考えずにサイトを作っていたら、パフォーマンスが崩壊した。\n今もパフォーマンスが良いとは言えないが、バンドルサイズを削減することで、パフォーマンスが向上した。\nこういったパフォーマンス向上のための施策は必要不可欠だなと感じた。\nまとめ 今回はLaravelとVue.jsを使ってサイトを作った。\nweb系をいじっていると時間を無限に吸われる。\n これからはじめるVue.js実践入門 山田 祥寛 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/20/","summary":"今回作成したサイト 今回作成したサイトは以下のサイト。\nClubPage\n私が所属している電子工作サークルのホームページを作った。\nこのブログからcssを流用しているので、見た目がだいぶ似ている。\n構成  Laravel Vue.js  バックエンドはLaravel、フロントエンドはVue.jsを使っている。\nシングルページアプリケーションとは 今回はシングルページアプリケーション(SPA)という構成を採用した。\nシングルページアプリケーションとは\n シングルページアプリケーション（英: single-page application、SPA）とは、単一のWebページのみから構成することで、デスクトップアプリケーションのようなユーザ体験を提供するWebアプリケーションまたはWebサイトである。必要なコード（HTML、JavaScript、CSS）は最初にまとめて読み込むか[1]、ユーザの操作などに応じて動的にサーバと通信し、必要なものだけ読み込みを行う。 Wikipediaより\n というものらしい。\nこれまでRuby on RailsやDjangoとかを使ってwebサイトを作ってみてきたが、どれも各ページ毎に、サーバ側でHTMLの生成を行うマルチページアプリケーションだった。\n今回はLaravelでwebAPIを作成し、そのAPIから取得した情報を、Vue.jsを使ってブラウザ上に表示するような構成にした。\nなぜSPAにしたのか 理由は1つで、今回参考にしたサイトがSPAを採用していたから。\n今回参考にしたサイトは以下のサイト。\n このサイトにあるチュートリアル、書かれている情報量がかなり多くて、これだけの情報をタダで得られるのが信じられない。\n今回初めてLaravelとVue.jsを触ったが、このサイトのおかげで色々と基礎を学ぶことができた。\n今回学んだこと、感じたこと インタフェース部とエンジン部の分離はやはり重要 今回の構成はAPI(Laravel)と表示部(Vue.js)を分離している。\nたとえばhttps://home.lchika.club/api/tagsにアクセスすれば、Laravelの機能によって、タグの一覧をJSON形式で取得できる。\nこういったAPIから取得した情報をVue.jsで処理して、ブラウザにレンダリングする。\nこのように分離することで、UIを変えたいときは、基本的にVue.jsの部分のみを修正するだけですむ。\nLaravelの処理部は一切手を加える必要がない。\nまあ、一般的なwebフレームワークを使っていれば、実装の分離は適切に行われているが、ある一部分のフレームワークをごっそり切り替えることはできない。\n今回の構成では、例えばフロントエンドにReactを使いたければ、Laravel部分はそのままで、フロントエンド部分をごっそり切り替えられる(と思うけど、Laravel-mixとかの関係で難しいのかもしれない\u0026hellip;)。\nこのように、インタフェースとエンジン部の分離で得られる効果はやはりでかいなと思った。\nローディング表示がないとwebサイトの質が落ちる これは個人的な印象かもしれないが、ローディング画面の表示はユーザにとって(ある程度)重要だなと思った。\n現状ではローディング表示は実装していないため、読み込み直後は一部のコンテンツが表示されていないし、一部のコンテンツについては結構な時間表示されないことがある。\nこれはサイト利用者側からすると、違和感を覚える要因になり、満足度の低下につながると思った。\nYouTubeとかもローディング中はコンテンツ表示部をグレーで表示したりしている。\n最初はこれ意味あるのか?と思っていたが、今回自分でJavaScriptを多用したサイトを構築してみて、こういった対応も重要だなと思った。\nWYSIWYGエディタの選定は毎回の課題 今回はブログのようなサイトを構築したので、フォームから記事を投稿できる必要があった。\nさすがにHTML直打ちで入力はできないし、リッチテキストエディタを採用する必要があった。\n毎回どのリッチテキストエディタを使うか苦労するが、今回も例に漏れず苦労した。\nとりあえず今のところはCKEditorを使っている。\nただ、まだ課題があって、iframeの挿入が上手くいっていない。\nいつも大体、リッチテキストエディタまわりは\n 画像の挿入 画像の整形 外部コンテンツの挿入 目次の作成  あたりで詰まっている。\n今回はまだ目次の作成について考えていないので、目次が必要になった時に多分また詰まる。\nSPAのパフォーマンス向上はバンドルサイズの削減こそ正義 今回作成したサイトのパフォーマンス向上については以下の記事に書いた。\n 最初特に何も考えずにサイトを作っていたら、パフォーマンスが崩壊した。\n今もパフォーマンスが良いとは言えないが、バンドルサイズを削減することで、パフォーマンスが向上した。\nこういったパフォーマンス向上のための施策は必要不可欠だなと感じた。\nまとめ 今回はLaravelとVue.jsを使ってサイトを作った。","title":"Laravel+Vue.jsでSPA(シングルページアプリケーション)を作成した"},{"content":"Laravel+Vue.jsで作成していたwebサイトの応答速度が激遅だったので、対策を実施した。\n環境  Laravel 6.14.0 Vue.js 2.6.11  対策前はページロードに20秒かかっていた 特に何も考えずにwebサイトを作っていたら、トップページのロードに20秒ほどかかっていた。 これではさすがにwebサイトとして成立しない。\nGoogleが提供しているwebページ分析ツールLighthouseを使ってパフォーマンスの測定を行ったところ、以下のような結果になった。\n堂々の0点である。提示されている対応策の詳細を見ると、以下のように書かれていた。\nどうやらバンドル後のapp.jsのサイズが約4MBと、超巨大になっているらしい。対応策としてapp.jsのサイズ削減を行った。\n対策1：本番用ビルド設定を適用する バンドルサイズ 3.86MB(app.js) → 1.59MB(app.js)\n解説 そもそもビルド設定が間違っていた…。ビルドコマンドとして\nnpm run dev を実行していたが、これは開発用のビルドコマンドらしい。\n本番用のビルドコマンド\nnpm run prod を実行したところ、バンドルサイズは3.86MB→1.59MBと、約40%になった。\nLighthouseの結果 0点→10点に上昇した。\n対策2：gzipで圧縮する バンドルサイズ 1.59MB(app.js) → 395KB(app.js.gz)\n解説 以下のページを参考に、gzipでapp.jsを圧縮した。\n そのままコピペで適用できた。\nLighthouseの結果 10点→63点に上昇した。\n対策3：ページごとにJSファイルを分割する バンドルサイズ 395KB(app.js.gz) → 243KB(app.js.gz)\n解説 以下のページを参考に、ページごとにJSファイルを分割した。\n こちらも特に詰まることはなく、ほぼコピペで実装できた。\nこの対策を実装したことで、ビルド後のJSファイルが以下のように、分割されて生成されるようになった。\nLighthouseの結果 63点→87点に上昇した。\n対策4：bootstrap-vueの選択的インポート バンドルサイズ 243KB(app.js.gz) → 129KB(app.js.gz)\n解説 対策1～3で、あらかた大きなところは対策できたと思ったので、より細かいところを対応するため、webpack-bundle-analyzerを使ってバンドル結果の分析をした。\nwebpack-bundle-analyzerの出力結果は以下のようになった。\nこの結果から\n bootstrap-vue ckeditor lodash  あたりのサイズが大きいらしいことが分かった。\nbootstrap-vueの公式ドキュメントによると、コンポーネントの選択的インポートができるらしい。この説明に従って、現在使用しているコンポーネントのみインポートするようにした。\n具体的にはapp.jsで\nimport { BootstrapVue, IconsPlugin } from \u0026#39;bootstrap-vue\u0026#39; としていたところを、\nimport { BImg } from \u0026#39;bootstrap-vue\u0026#39; Vue.component(\u0026#39;b-img\u0026#39;, BImg) import { BRow } from \u0026#39;bootstrap-vue\u0026#39; Vue.component(\u0026#39;b-row\u0026#39;, BRow) import { BCol } from \u0026#39;bootstrap-vue\u0026#39; Vue.component(\u0026#39;b-col\u0026#39;, BCol) import { BForm } from \u0026#39;bootstrap-vue\u0026#39; Vue.component(\u0026#39;b-form\u0026#39;, BForm) import { BFormGroup } from \u0026#39;bootstrap-vue\u0026#39; Vue.component(\u0026#39;b-form-group\u0026#39;, BFormGroup) import { BButton } from \u0026#39;bootstrap-vue\u0026#39; Vue.component(\u0026#39;b-button\u0026#39;, BButton) import { BFormInput } from \u0026#39;bootstrap-vue\u0026#39; Vue.component(\u0026#39;b-form-input\u0026#39;, BFormInput) と書き変えた。公式ドキュメントをみるとバンドルサイズ削減のためにまだ色々できそうだが、英語力とwebpackへの理解が足りないため断念した。\nLighthouseの結果 87点→97点に上昇した。 あと最終結果として、パフォーマンス以外の項目も掲載しておく。\nまとめ いろいろやってLighthouseのパフォーマンスの結果を0点から97点まであげた。比較的簡単に対応できるのはここまでで、これ以上の改善は色々広範囲の知識が必要になりそうだった。100点を達成したかったが、今回はここまで。\n これからはじめるVue.js実践入門 山田 祥寛 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/19/","summary":"Laravel+Vue.jsで作成していたwebサイトの応答速度が激遅だったので、対策を実施した。\n環境  Laravel 6.14.0 Vue.js 2.6.11  対策前はページロードに20秒かかっていた 特に何も考えずにwebサイトを作っていたら、トップページのロードに20秒ほどかかっていた。 これではさすがにwebサイトとして成立しない。\nGoogleが提供しているwebページ分析ツールLighthouseを使ってパフォーマンスの測定を行ったところ、以下のような結果になった。\n堂々の0点である。提示されている対応策の詳細を見ると、以下のように書かれていた。\nどうやらバンドル後のapp.jsのサイズが約4MBと、超巨大になっているらしい。対応策としてapp.jsのサイズ削減を行った。\n対策1：本番用ビルド設定を適用する バンドルサイズ 3.86MB(app.js) → 1.59MB(app.js)\n解説 そもそもビルド設定が間違っていた…。ビルドコマンドとして\nnpm run dev を実行していたが、これは開発用のビルドコマンドらしい。\n本番用のビルドコマンド\nnpm run prod を実行したところ、バンドルサイズは3.86MB→1.59MBと、約40%になった。\nLighthouseの結果 0点→10点に上昇した。\n対策2：gzipで圧縮する バンドルサイズ 1.59MB(app.js) → 395KB(app.js.gz)\n解説 以下のページを参考に、gzipでapp.jsを圧縮した。\n そのままコピペで適用できた。\nLighthouseの結果 10点→63点に上昇した。\n対策3：ページごとにJSファイルを分割する バンドルサイズ 395KB(app.js.gz) → 243KB(app.js.gz)\n解説 以下のページを参考に、ページごとにJSファイルを分割した。\n こちらも特に詰まることはなく、ほぼコピペで実装できた。\nこの対策を実装したことで、ビルド後のJSファイルが以下のように、分割されて生成されるようになった。\nLighthouseの結果 63点→87点に上昇した。\n対策4：bootstrap-vueの選択的インポート バンドルサイズ 243KB(app.js.gz) → 129KB(app.js.gz)\n解説 対策1～3で、あらかた大きなところは対策できたと思ったので、より細かいところを対応するため、webpack-bundle-analyzerを使ってバンドル結果の分析をした。\nwebpack-bundle-analyzerの出力結果は以下のようになった。\nこの結果から\n bootstrap-vue ckeditor lodash  あたりのサイズが大きいらしいことが分かった。","title":"Laravel+Vue.jsでLighthouseのスコアを0点から97点にした(バンドルサイズ削減)"},{"content":"やりたいこと 俯瞰で見ているUSBカメラの画像を使って、自律的に動いているロボットの向きを検出する。\nまた、カメラ画像における指定の座標と、ロボットがなす角を算出する。\n動作環境  ターゲットボード  JetsonNano   OS  Ubuntu 18.04.4 LTS   使用ライブラリ  Python版 OpenCV 4.1.1    処理結果 今回実装した処理の結果は、画像で示すと以下のような感じになる。\n方法 今回実装した方法は以下の通り。\n カメラ画像をHSV形式に変換 カメラ画像のうち指定のHSV範囲に従ってマスク画像を作成 マスク画像から輪郭を抽出し、1番面積の大きい領域のみ残したマスク画像を作成 カメラ画像とマスク画像の共通領域を抽出 4.で作成した画像から黒色領域を抽出 5.で抽出した黒色領域のうち、1番面積の大きい領域の重心p1、2番目に面積の大きい領域の重心p2を計算 p2からp1へのベクトルと、p1とp2の中点から指定の座標へのベクトルのなす角を計算  処理順に従って中間生成画像を並べると以下のような感じになる。\n元画像\n手順3.で作成したマスク画像\n手順5.で抽出した黒色領域\n手順6.で算出した各黒色領域の重心\n処理結果\nソースコード  一部自作モジュールもimportしているのでコピペでは動きません。\n呼び出し方は例えば以下のように。\n 処理時間や実行時のリソース使用量など 処理時間 今回の一連の処理呼び出しにどれだけ時間がかかっているか計測したところ、以下の通りだった。\nelapsed_time:0.49050045013427734[sec] elapsed_time:0.500530481338501[sec] elapsed_time:0.49204468727111816[sec] だいたい0.5秒ほどかかっている。このままだとリアルタイム性が求められる場合は使えない。\nリソース使用量 Jetsonのモニタツールjtopの出力は以下のような感じ。\nプログラム稼働中\nGPUが稼働していない。GPUを使うようにすれば高速化できるのだろうか(全然そこらへんは調べていない)。\nなぜこの処理を実装したか 全体を俯瞰できるカメラの情報を使って、ロボットを指定位置に誘導したかった。\nカメラ側がロボットに指定の位置に対する角度・距離情報を伝え、ロボット側でその情報を使って、モータ値を計算する。\n設計の経緯や苦労したところなど 目印をどう設計するか 向き検出のための目印をどう設計するかが、検出精度や処理速度に最も影響を与える部分だと思う。\n今回は黄色の画用紙の上に、面積の異なる黒色の画用紙を2つ(前方に面積が大きいもの、後方に面積の小さいもの)つけて目印にした。\n単純だが、思っていたより認識出来ていたと思う。しっかりしたベンチマークはとっていないが。\n最初はQRコードの活用を考えていた。OpenCVにはQRコード認識処理が実装されており、向きも検出出来そうだったので。今後複数台ロボットを同時に動かすことを考えても、QRコードだったら文字列情報を付加できるので、各ロボットの識別に使えそうとも思っていた。\nただ、ある程度QRコードが大きく写っていないと認識できなかった。今回はカメラと認識対象ロボットの距離が遠い時でも認識できるようにしたかったので、QRコード案はやめた。\n画像処理の実装はそこまで苦労しなかった OpenCVはネットに情報がたくさん落ちているので、処理実装部分はそこまで苦労しなかった。今回実装した処理はほぼネットで紹介されている実装方法をそのまま使っている。\n雑記 VSCodeのRemote-SSH機能が非常に使いやすかった。\n 詳解 OpenCV 3 ―コンピュータビジョンライブラリを使った画像処理・認識 オライリージャパン (2018/5/26) Amazon 楽天   ","permalink":"https://kouya17.com/posts/18/","summary":"やりたいこと 俯瞰で見ているUSBカメラの画像を使って、自律的に動いているロボットの向きを検出する。\nまた、カメラ画像における指定の座標と、ロボットがなす角を算出する。\n動作環境  ターゲットボード  JetsonNano   OS  Ubuntu 18.04.4 LTS   使用ライブラリ  Python版 OpenCV 4.1.1    処理結果 今回実装した処理の結果は、画像で示すと以下のような感じになる。\n方法 今回実装した方法は以下の通り。\n カメラ画像をHSV形式に変換 カメラ画像のうち指定のHSV範囲に従ってマスク画像を作成 マスク画像から輪郭を抽出し、1番面積の大きい領域のみ残したマスク画像を作成 カメラ画像とマスク画像の共通領域を抽出 4.で作成した画像から黒色領域を抽出 5.で抽出した黒色領域のうち、1番面積の大きい領域の重心p1、2番目に面積の大きい領域の重心p2を計算 p2からp1へのベクトルと、p1とp2の中点から指定の座標へのベクトルのなす角を計算  処理順に従って中間生成画像を並べると以下のような感じになる。\n元画像\n手順3.で作成したマスク画像\n手順5.で抽出した黒色領域\n手順6.で算出した各黒色領域の重心\n処理結果\nソースコード  一部自作モジュールもimportしているのでコピペでは動きません。\n呼び出し方は例えば以下のように。\n 処理時間や実行時のリソース使用量など 処理時間 今回の一連の処理呼び出しにどれだけ時間がかかっているか計測したところ、以下の通りだった。\nelapsed_time:0.49050045013427734[sec] elapsed_time:0.500530481338501[sec] elapsed_time:0.49204468727111816[sec] だいたい0.5秒ほどかかっている。このままだとリアルタイム性が求められる場合は使えない。\nリソース使用量 Jetsonのモニタツールjtopの出力は以下のような感じ。\nプログラム稼働中\nGPUが稼働していない。GPUを使うようにすれば高速化できるのだろうか(全然そこらへんは調べていない)。\nなぜこの処理を実装したか 全体を俯瞰できるカメラの情報を使って、ロボットを指定位置に誘導したかった。\nカメラ側がロボットに指定の位置に対する角度・距離情報を伝え、ロボット側でその情報を使って、モータ値を計算する。\n設計の経緯や苦労したところなど 目印をどう設計するか 向き検出のための目印をどう設計するかが、検出精度や処理速度に最も影響を与える部分だと思う。\n今回は黄色の画用紙の上に、面積の異なる黒色の画用紙を2つ(前方に面積が大きいもの、後方に面積の小さいもの)つけて目印にした。\n単純だが、思っていたより認識出来ていたと思う。しっかりしたベンチマークはとっていないが。\n最初はQRコードの活用を考えていた。OpenCVにはQRコード認識処理が実装されており、向きも検出出来そうだったので。今後複数台ロボットを同時に動かすことを考えても、QRコードだったら文字列情報を付加できるので、各ロボットの識別に使えそうとも思っていた。\nただ、ある程度QRコードが大きく写っていないと認識できなかった。今回はカメラと認識対象ロボットの距離が遠い時でも認識できるようにしたかったので、QRコード案はやめた。","title":"カメラ画像から対象物の向きを検出する(OpenCV)"},{"content":"概要 適当に抽出したVTuberについて、チャンネル登録者数増加数のランキングサイトを作成した。\n1日1回、午前5時30分ごろ更新。\n 構成 Golang + Gin + Bootstrap\n 雑記 Golangを使ったwebアプリを初めて作成した。Golangは未使用の変数、importがデフォルトでコンパイルエラーになるのが印象的だった。ルールが厳格な印象。\nYouTubeの現在の仕様で、チャンネル登録者数が多いほど、刻み幅が大きくなる。その刻み幅分を超えないと、増加量が0になってしまうので、1日単位だと正確なランキングにならない。\n今回のサイトを作るにあたって、既存サイトを調べてみた。色々凝っているサイトが多数あった。やはりここら辺(YouTube周り)は色んなビジネスが成り立っているんだろうなと思った。\n 改訂2版 みんなのGo言語 技術評論社 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/17/","summary":"概要 適当に抽出したVTuberについて、チャンネル登録者数増加数のランキングサイトを作成した。\n1日1回、午前5時30分ごろ更新。\n 構成 Golang + Gin + Bootstrap\n 雑記 Golangを使ったwebアプリを初めて作成した。Golangは未使用の変数、importがデフォルトでコンパイルエラーになるのが印象的だった。ルールが厳格な印象。\nYouTubeの現在の仕様で、チャンネル登録者数が多いほど、刻み幅が大きくなる。その刻み幅分を超えないと、増加量が0になってしまうので、1日単位だと正確なランキングにならない。\n今回のサイトを作るにあたって、既存サイトを調べてみた。色々凝っているサイトが多数あった。やはりここら辺(YouTube周り)は色んなビジネスが成り立っているんだろうなと思った。\n 改訂2版 みんなのGo言語 技術評論社 Amazon Kindle 楽天 楽天Kobo   ","title":"VTuberの簡易ランキングサイトを作った"},{"content":"今年もあっという間だったがもう終わるらしい。 この1年で作った物の振り返りをしたいと思う。\n今年作ったもの 1月～2月　ロボサッカー用ロボ作成 2月終わりごろにロボカップジュニア同等のルールのロボコンに出る機会があり、プログラムの一部作成を担当した。\nロボの構成は大体以下の通り。\n 機体はLEGOのマインドストーム 赤外線センサでボールの位置を検出 ラズパイを使って、カメラからの画像を解析しゴール位置を検出  3人でチームを組んでおり、進捗管理ツールとしてtrello、情報共有ツールとしてslack、ソースコード管理ツールとしてGitHubを使った。\ntrello、slackあたりはあまり活用できなかったが、GitHubはやはり便利だなと思った。\nできたもの  マインドストーム側プログラム(GitHub) ラズパイ側プログラム(GitHub)  3月～7月　イライラ棒作成 私が所属している電子工作サークル「メカトロ同行会エルチカ」の作成物として、イライラ棒ゲームを作成した。基本的にプログラム部分を担当した。上の写真はNT名古屋出展時のもの。\nこの作品では以下のように色んな機器の通信を試してみた。\n ArduinoとPCの通信(USBシリアル通信) PCとラズパイの通信(HTTP) PCとESP32の通信(HTTP) ラズパイとwebアプリの通信(HTTP)  色んな機器を連携させる作品を初めて作ってみたが、どれか一つが動かないと全部ダメになる仕組みに一部なってしまった。\nできたもの  イライラ棒wikiページ スコア表示用ソフト(GitHub) 結果プリント用ラズパイプログラム(GitHub) スタートモジュール用プログラム(GitHub) ゴールモジュール用プログラム(GitHub) イライラ棒結果ランキングページ  4月　github.ioページ公開 github.ioを使ってwebページを公開した。なお、現在更新はしていない。\nできたもの  github.ioページ  8月　KiCad用ツール作成 KiCadを使う機会が多くなってきたので基板発注用の簡単なツール(Windowsアプリ)を作ってみた。\nこちらの記事で少し内容について紹介している。\nできたもの  KiCadHelper(GitHub)  8月～　射的ゲーム作成 赤外線を使った射的ゲームを現在も作成中。同じようなものを以前一度作成していて、今回は複数人プレイに対応させる予定。\nできたもの  鋭意諸々作成中  今年を振り返ってみて 今年は電子工作に割ける時間が結構多かった。ただこうして振り返ってみると、かけた時間に比べ、出来上がった物の量がいまいち物足りないな…と思う。\n","permalink":"https://kouya17.com/posts/16/","summary":"今年もあっという間だったがもう終わるらしい。 この1年で作った物の振り返りをしたいと思う。\n今年作ったもの 1月～2月　ロボサッカー用ロボ作成 2月終わりごろにロボカップジュニア同等のルールのロボコンに出る機会があり、プログラムの一部作成を担当した。\nロボの構成は大体以下の通り。\n 機体はLEGOのマインドストーム 赤外線センサでボールの位置を検出 ラズパイを使って、カメラからの画像を解析しゴール位置を検出  3人でチームを組んでおり、進捗管理ツールとしてtrello、情報共有ツールとしてslack、ソースコード管理ツールとしてGitHubを使った。\ntrello、slackあたりはあまり活用できなかったが、GitHubはやはり便利だなと思った。\nできたもの  マインドストーム側プログラム(GitHub) ラズパイ側プログラム(GitHub)  3月～7月　イライラ棒作成 私が所属している電子工作サークル「メカトロ同行会エルチカ」の作成物として、イライラ棒ゲームを作成した。基本的にプログラム部分を担当した。上の写真はNT名古屋出展時のもの。\nこの作品では以下のように色んな機器の通信を試してみた。\n ArduinoとPCの通信(USBシリアル通信) PCとラズパイの通信(HTTP) PCとESP32の通信(HTTP) ラズパイとwebアプリの通信(HTTP)  色んな機器を連携させる作品を初めて作ってみたが、どれか一つが動かないと全部ダメになる仕組みに一部なってしまった。\nできたもの  イライラ棒wikiページ スコア表示用ソフト(GitHub) 結果プリント用ラズパイプログラム(GitHub) スタートモジュール用プログラム(GitHub) ゴールモジュール用プログラム(GitHub) イライラ棒結果ランキングページ  4月　github.ioページ公開 github.ioを使ってwebページを公開した。なお、現在更新はしていない。\nできたもの  github.ioページ  8月　KiCad用ツール作成 KiCadを使う機会が多くなってきたので基板発注用の簡単なツール(Windowsアプリ)を作ってみた。\nこちらの記事で少し内容について紹介している。\nできたもの  KiCadHelper(GitHub)  8月～　射的ゲーム作成 赤外線を使った射的ゲームを現在も作成中。同じようなものを以前一度作成していて、今回は複数人プレイに対応させる予定。\nできたもの  鋭意諸々作成中  今年を振り返ってみて 今年は電子工作に割ける時間が結構多かった。ただこうして振り返ってみると、かけた時間に比べ、出来上がった物の量がいまいち物足りないな…と思う。","title":"今年(2019年)作ったものを振り返る"},{"content":"今回の経緯  フルカラーLEDを最大5個制御したい！ 最初NeoPixel(マイコン内蔵LED)を使う予定だったが、いろいろ試した結果、今回の条件では使えなさそうという結論になった。 マイコン内蔵ではない、通常のフルカラーLEDを使うことにする。 ただ、使用できるピン数は限られているため、マイコンボードからはI2CやSPI等で制御する形にしたい。 フルカラーLEDドライバを探す。 スイッチサイエンスで探したらよさそうなのがあったけど値が張る。 秋月でさがしたらこんなのがあった。 安い。もともとはドットマトリクス制御用らしいが、フルカラーLED制御にも転用できそう。  準備物    部品名 購入先     ESPr Developer 32 スイッチサイエンス   16×8LEDマトリクスドライバーモジュール(HT16K33) 秋月電子通商   RGBフルカラーLED 秋月電子通商    結果  とりあえず2個のフルカラーLEDを制御できた。 ただし、r, g, bはそれぞれ0(OFF)か1(ON)でしか調整することは出来ない。   プログラムとハマりポイント  プログラム  GitHub   ハマりポイント  I2Cアドレスでハマった。 秋月の解説ページでは初期状態なら0xE0(write)と書いてあったので、Arduinoのスケッチでアドレスを0xE0に指定したがなにも反応しなかった。 調べてみるとこの方法は誤りで、Arduinoの場合は最下位ビットを除いた7ビット分をアドレス値に設定する必要があるらしい。 スレーブアドレスを0x70と設定したらうまくいった。 参考：https://tool-lab.com/make/pic-practice-37/    雑記  いま探したらaitendoでもっと安い、100円のフルカラーLEDドライバ(単体)があった。 ただこっちはチップ単体の値段だし、データシートが中文で読むのが大変そう…。   IoT開発スタートブック　── ESP32でクラウドにつなげる電子工作をはじめよう！ 技術評論社 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/15/","summary":"今回の経緯  フルカラーLEDを最大5個制御したい！ 最初NeoPixel(マイコン内蔵LED)を使う予定だったが、いろいろ試した結果、今回の条件では使えなさそうという結論になった。 マイコン内蔵ではない、通常のフルカラーLEDを使うことにする。 ただ、使用できるピン数は限られているため、マイコンボードからはI2CやSPI等で制御する形にしたい。 フルカラーLEDドライバを探す。 スイッチサイエンスで探したらよさそうなのがあったけど値が張る。 秋月でさがしたらこんなのがあった。 安い。もともとはドットマトリクス制御用らしいが、フルカラーLED制御にも転用できそう。  準備物    部品名 購入先     ESPr Developer 32 スイッチサイエンス   16×8LEDマトリクスドライバーモジュール(HT16K33) 秋月電子通商   RGBフルカラーLED 秋月電子通商    結果  とりあえず2個のフルカラーLEDを制御できた。 ただし、r, g, bはそれぞれ0(OFF)か1(ON)でしか調整することは出来ない。   プログラムとハマりポイント  プログラム  GitHub   ハマりポイント  I2Cアドレスでハマった。 秋月の解説ページでは初期状態なら0xE0(write)と書いてあったので、Arduinoのスケッチでアドレスを0xE0に指定したがなにも反応しなかった。 調べてみるとこの方法は誤りで、Arduinoの場合は最下位ビットを除いた7ビット分をアドレス値に設定する必要があるらしい。 スレーブアドレスを0x70と設定したらうまくいった。 参考：https://tool-lab.com/make/pic-practice-37/    雑記  いま探したらaitendoでもっと安い、100円のフルカラーLEDドライバ(単体)があった。 ただこっちはチップ単体の値段だし、データシートが中文で読むのが大変そう…。   IoT開発スタートブック　── ESP32でクラウドにつなげる電子工作をはじめよう！ 技術評論社 Amazon Kindle 楽天 楽天Kobo   ","title":"マトリクスLEDドライバを使ってフルカラーLEDを制御してみる"},{"content":"概要  ソニーのシングルボードコンピュータSpresense及びSpresense用カメラモジュールを使って、公式で公開されているサンプルプログラムを動かした。 大体READMEの手順に従ったが、ところどころ詰まるところがあったのでメモとして残しておく。  Spresenseの簡単な説明  ソニーが開発したシングルボードコンピュータ。 Arduino IDEで開発可能。 NuttXというリアルタイムOSベースのSpresense SDKでも開発できる(今回はこっち)。 機能の主な特徴は以下。  GPSが標準で内蔵されている。 マルチコア(6コア!)プロセッサ。   メインボード単体だとネットワーク機能が使えないという点が個人的にはネックだと思っている。  Spresenseが出る少し前に流行し始めたEPS32系にすべて持っていかれている印象がある。    サンプル実行手順 今回動かしたサンプル 今回はカメラモジュールの動作確認がしたかったので公式が公開しているexamples/cameraを動かした。\n実行環境  OS：Windows10上のWSL(Ubuntu16.04) 開発環境：Spresense SDK  手順 基本はスタートガイド参照。\n1.開発ツールのセットアップ、ブートローダのインストールを行う。\n ここは公式の手順通りでいけるはずなので省略。  2.手順1.でSpresenseのプロジェクト一式をGitHubから落としてきているはずなので、その中のexamples/camera/README.txtを確認する。\n READMEを読むと、液晶コントローラILI9340で駆動しているLCDを使って、カメラが取得した画像を表示できるようになっていることが分かる。  3.ILI9340で駆動しているLCDを用意してSpresenseにつなぐ。\n 今回はちょうど手元にILI9341で駆動している2.2インチTFTがあったのでそれを使った。 ここが引っかかるポイントその1(だと思う)。 READMEを読むと、拡張ボードを使わない場合はSPI5のピンにSPI関係の4ピンをつなげばよいということは書いてあるのだが、TFTのDCピンとRESETピンの配線先ピンが分からない。 この問題に関しては、コードを読みこむ必要があり(本当はどこかに説明書きがあるのかもしれないが…)、/sdk/bsp/board/spresense/include/board.h198行目～に定義されている。  #if defined(CONFIG_LCD_ON_MAIN_BOARD) /* Display connected to main board. */#define DISPLAY_RST PIN_I2S0_BCK #define DISPLAY_DC PIN_I2S0_LRCK  #define DISPLAY_SPI 5  この記述から、DCピンはI2S0 LRCKピン、RESETピンはI2S0 BCKピンにつなげば良いことがわかる。  4.サンプルプログラムのコンフィグレーションを行う。\n ここが引っかかるポイントその2。 READMEには/sdk/configs/device/camera-defconfigに何個か設定を追記して、コマンド./tools/config.py examples/camera device/lcdを実行すればよいと書いてあるが、これだとLCDに画像が映らない。 理由は、/sdk/configs/device/camera-defconfigに設定を追記した場合、devise/camera-defconfigを変更したのだから、コマンド./tools/config.py examples/camera device/lcd devise/camera(追加)を実行しないと変更が反映されない。 そもそもここらへんの設定はexamples/cameraに対する設定なのだから、examples/camera-defconfigに書くべきだと思うのだが違うのだろうか…。 あと、これはなくても良いかもしれないが、サンプルプログラム中、画像の色形式についてYUV形式からRGB形式に変換する処理が含まれており、この部分を有効にするにはコマンド./tools/config.py examples/camera device/lcd devise/camera feature/imageproc(さらに追加)を実行する必要がある。  5.カーネルとSDKのビルドを行う。\n 以下のコマンドでOK。  make buildkernel make 6.Spresenseにプログラムを書き込む。\n 以下のコマンドでOK。  tools/flash.sh -c /dev/ttyXXX(Spresenseが接続されているUSBポート) nuttx.spk 7.TeraTermとかでSpresenseに接続する。\n8.Spresenseのプロンプトでコマンドcameraを実行する。\n プロンプトに以下のように表示され、LCDにカメラからの画像が表示されればOK(のはず)。  NuttShell (NSH) NuttX-7.22 nsh\u0026gt; camera nximage_initialize: Initializing LCD nximage_initialize: Open NX nximage_initialize: Screen resolution (320,240) FILENAME:/mnt/spif/VIDEO001.YUV FILENAME:/mnt/spif/VIDEO002.YUV FILENAME:/mnt/spif/VIDEO003.YUV FILENAME:/mnt/spif/VIDEO004.YUV FILENAME:/mnt/spif/VIDEO005.YUV FILENAME:/mnt/spif/VIDEO006.YUV FILENAME:/mnt/spif/VIDEO007.YUV FILENAME:/mnt/spif/VIDEO008.YUV FILENAME:/mnt/spif/VIDEO009.YUV FILENAME:/mnt/spif/VIDEO010.YUV 雑記  後で少し調べてみたら開発者フォーラムにLCD接続に関する投稿があった。  ちょうど2日前の投稿。タイムリー。   Spresense SDKに関しては開発者フォーラムの方にそこそこ情報がありそう。   SONY SPRESENSE メインボード CXD5602PWBMAIN1 スプレッセンス(Spresense) Amazon 楽天   ","permalink":"https://kouya17.com/posts/14/","summary":"概要  ソニーのシングルボードコンピュータSpresense及びSpresense用カメラモジュールを使って、公式で公開されているサンプルプログラムを動かした。 大体READMEの手順に従ったが、ところどころ詰まるところがあったのでメモとして残しておく。  Spresenseの簡単な説明  ソニーが開発したシングルボードコンピュータ。 Arduino IDEで開発可能。 NuttXというリアルタイムOSベースのSpresense SDKでも開発できる(今回はこっち)。 機能の主な特徴は以下。  GPSが標準で内蔵されている。 マルチコア(6コア!)プロセッサ。   メインボード単体だとネットワーク機能が使えないという点が個人的にはネックだと思っている。  Spresenseが出る少し前に流行し始めたEPS32系にすべて持っていかれている印象がある。    サンプル実行手順 今回動かしたサンプル 今回はカメラモジュールの動作確認がしたかったので公式が公開しているexamples/cameraを動かした。\n実行環境  OS：Windows10上のWSL(Ubuntu16.04) 開発環境：Spresense SDK  手順 基本はスタートガイド参照。\n1.開発ツールのセットアップ、ブートローダのインストールを行う。\n ここは公式の手順通りでいけるはずなので省略。  2.手順1.でSpresenseのプロジェクト一式をGitHubから落としてきているはずなので、その中のexamples/camera/README.txtを確認する。\n READMEを読むと、液晶コントローラILI9340で駆動しているLCDを使って、カメラが取得した画像を表示できるようになっていることが分かる。  3.ILI9340で駆動しているLCDを用意してSpresenseにつなぐ。\n 今回はちょうど手元にILI9341で駆動している2.2インチTFTがあったのでそれを使った。 ここが引っかかるポイントその1(だと思う)。 READMEを読むと、拡張ボードを使わない場合はSPI5のピンにSPI関係の4ピンをつなげばよいということは書いてあるのだが、TFTのDCピンとRESETピンの配線先ピンが分からない。 この問題に関しては、コードを読みこむ必要があり(本当はどこかに説明書きがあるのかもしれないが…)、/sdk/bsp/board/spresense/include/board.h198行目～に定義されている。  #if defined(CONFIG_LCD_ON_MAIN_BOARD) /* Display connected to main board. */#define DISPLAY_RST PIN_I2S0_BCK #define DISPLAY_DC PIN_I2S0_LRCK  #define DISPLAY_SPI 5  この記述から、DCピンはI2S0 LRCKピン、RESETピンはI2S0 BCKピンにつなげば良いことがわかる。  4.","title":"Spresense SDKでカメラモジュール用のサンプルを動かした"},{"content":"概要  当初ESPr Developer 32でNeoPixelを1個制御したかった 点灯自体はするのだが、色が正しく反映されない(赤を指定してるつもりでも、緑や青色になる) 上記問題を解消するのに丸1日かかった  結論  NeoPixelの動作電圧5Vに対し、IOの電圧を3.3Vにしてしまっていた  丸1日、試したこと  最初に、今回使用したNeoPixelの動作電圧がデータシート上4.5V~6Vということで、NeoPixelのVCCは5V、DINはESPr Developer 32のIO27ピンに接続した。 ライブラリAdafruit_NeoPixelを使って動作確認用プログラムを作成。 なんか色がおかしい！clear()も正常に働いていない。 \u0026ldquo;esp32 NeoPixel\u0026quot;でググる。 なんか他の人は大体正常に動作してるっぽい。 ネットの記事が比較的古い情報だったので、ライブラリのバージョンを疑う。 ライブラリAdafruit_NeoPixelのバージョンをネットの記事に合わせて下げる(ver1.3.0→ver1.2.3)。 動作不良変わりなし。 Arduino core for the ESP32のバージョンを下げる(ver1.0.4→ver1.0.0)。 動作不良変わりなし。 ライブラリAdafruit_NeoPixelがESP32ボードだとうまく使えないという記事を見かけ、NeoPixelライブラリ自体を疑う。 代替ライブラリとしてFastLEDのサンプルを動かす。 これも色がおかしい！ 代替ライブラリとしてNeoPixelBusのサンプルを動かす。 相変わらず色がおかしい。 Arduino Pro Mini互換機でAdafruit_NeoPixelのサンプルを動かす。 ちゃんと想定通りの色が出る。 ネットで再度ググるが\u0026quot;ESP32でもNeoPixelは使える\u0026quot;という情報ばかり。 動作確認に使っていたESPr Developer 32ボードを疑う。 違うESPr Developer 32ボードで動作確認する。 動作不良変わりなし。 同じESP32開発ボードであるESP32 DevKitCを使って動作確認する。 動作不良変わりなし。 電源だけArduino ProMini互換機からとってみたりする。 動作不良変わりなし。 ライブラリAdafruit_NeoPixelの実装を見てみる。 ESP32関係ボードの場合は、マクロF_CPUの値からCPU動作周波数を確認し、インラインアセンブラで信号のHIGH/LOWのタイミングを制御しているらしい。 一応マクロF_CPUの値をシリアル出力させて確認してみる。 F_CPU=240000000(240MHz)で想定通り。 本当に240MHzで動いてるのかを確認しようかとも思ったが、確認方法もパッとわからないので後回しにする。 ビルドオプションでCPU動作周波数を変えてみる。 動作不良変わりなし。 Adafruit_NeoPixelのProjectページ(GitHub)のissueを確認して類似の問題がないか確認する。 https://github.com/adafruit/Adafruit_NeoPixel/issues/139を見つける。 細かいことはよくわからないがNeoPixelBusを使えば正常に動いたという報告があるみたい。 といっても前述の通りNeoPixelBusのサンプルをそのまま動かしただけでは正常に動作しなかったため、NeoPixelBusのProjectページ(GitHub)のwikiを見る。 https://github.com/Makuna/NeoPixelBus/wiki/FAQ-%232を見つける。 自分の無知さに絶望する。  対策 とりあえず以下のいずれかの方法で正常な色が出力されることを確認。\n ロジックレベル変換モジュールを使用してIOを5Vに変換する 電源電圧を3.3Vにする(最低定格電圧未満だがとりあえず動いた)  戒め ロジックレベルは動作電圧に合わせましょう(当たり前)。\n IoT開発スタートブック　── ESP32でクラウドにつなげる電子工作をはじめよう！ 技術評論社 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/13/","summary":"概要  当初ESPr Developer 32でNeoPixelを1個制御したかった 点灯自体はするのだが、色が正しく反映されない(赤を指定してるつもりでも、緑や青色になる) 上記問題を解消するのに丸1日かかった  結論  NeoPixelの動作電圧5Vに対し、IOの電圧を3.3Vにしてしまっていた  丸1日、試したこと  最初に、今回使用したNeoPixelの動作電圧がデータシート上4.5V~6Vということで、NeoPixelのVCCは5V、DINはESPr Developer 32のIO27ピンに接続した。 ライブラリAdafruit_NeoPixelを使って動作確認用プログラムを作成。 なんか色がおかしい！clear()も正常に働いていない。 \u0026ldquo;esp32 NeoPixel\u0026quot;でググる。 なんか他の人は大体正常に動作してるっぽい。 ネットの記事が比較的古い情報だったので、ライブラリのバージョンを疑う。 ライブラリAdafruit_NeoPixelのバージョンをネットの記事に合わせて下げる(ver1.3.0→ver1.2.3)。 動作不良変わりなし。 Arduino core for the ESP32のバージョンを下げる(ver1.0.4→ver1.0.0)。 動作不良変わりなし。 ライブラリAdafruit_NeoPixelがESP32ボードだとうまく使えないという記事を見かけ、NeoPixelライブラリ自体を疑う。 代替ライブラリとしてFastLEDのサンプルを動かす。 これも色がおかしい！ 代替ライブラリとしてNeoPixelBusのサンプルを動かす。 相変わらず色がおかしい。 Arduino Pro Mini互換機でAdafruit_NeoPixelのサンプルを動かす。 ちゃんと想定通りの色が出る。 ネットで再度ググるが\u0026quot;ESP32でもNeoPixelは使える\u0026quot;という情報ばかり。 動作確認に使っていたESPr Developer 32ボードを疑う。 違うESPr Developer 32ボードで動作確認する。 動作不良変わりなし。 同じESP32開発ボードであるESP32 DevKitCを使って動作確認する。 動作不良変わりなし。 電源だけArduino ProMini互換機からとってみたりする。 動作不良変わりなし。 ライブラリAdafruit_NeoPixelの実装を見てみる。 ESP32関係ボードの場合は、マクロF_CPUの値からCPU動作周波数を確認し、インラインアセンブラで信号のHIGH/LOWのタイミングを制御しているらしい。 一応マクロF_CPUの値をシリアル出力させて確認してみる。 F_CPU=240000000(240MHz)で想定通り。 本当に240MHzで動いてるのかを確認しようかとも思ったが、確認方法もパッとわからないので後回しにする。 ビルドオプションでCPU動作周波数を変えてみる。 動作不良変わりなし。 Adafruit_NeoPixelのProjectページ(GitHub)のissueを確認して類似の問題がないか確認する。 https://github.com/adafruit/Adafruit_NeoPixel/issues/139を見つける。 細かいことはよくわからないがNeoPixelBusを使えば正常に動いたという報告があるみたい。 といっても前述の通りNeoPixelBusのサンプルをそのまま動かしただけでは正常に動作しなかったため、NeoPixelBusのProjectページ(GitHub)のwikiを見る。 https://github.com/Makuna/NeoPixelBus/wiki/FAQ-%232を見つける。 自分の無知さに絶望する。  対策 とりあえず以下のいずれかの方法で正常な色が出力されることを確認。","title":"NeoPixelをESP32開発ボードで点灯させるのに丸1日かかった話"},{"content":"Purpose Play startup sound when raspi boots\nPrepare  Raspberry Pi Zero W Gravity: UART MP3 Voice Module  Reference  https://qiita.com/ikemura23/items/6f9adce99a3db555a0e4 http://hendigi.karaage.xyz/2016/11/auto-boot/ https://tomosoft.jp/design/?p=11677 https://wiki.dfrobot.com/Voice_Module_SKU__DFR0534  Auto run methods /etc/rc.local Run script as root\ncrontab Run script as user\nsystemd Manage as a service\nImplement Select /etc/rc.local at this time\nEdit /home/pi/Boot/boot_syateki_server.py as below\nimport serial # open serial port s = serial.Serial(\u0026#39;/dev/serial0\u0026#39;, 9600, timeout=10) # set volume 0x16 (0x00 - 0x1E) s.write(serial.to_bytes([0xAA,0x13,0x01,0x16,0xD4])) # play file 01.mp3 s.write(serial.to_bytes([0xAA,0x07,0x02,0x00,0x01,0xB4])) Confirm access authorizations\npi@raspberrypi:~/Boot $ pwd /home/pi/Boot pi@raspberrypi:~/Boot $ ls -l ./ total 4 -rw-r--r-- 1 root root 248 Oct 13 10:37 boot_syateki_server.py Edit /etc/rc.local as below\n#!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \u0026#34;exit 0\u0026#34; on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. # Print the IP address _IP=$(hostname -I) || true if [ \u0026#34;$_IP\u0026#34; ]; then printf \u0026#34;My IP address is %s\\n\u0026#34; \u0026#34;$_IP\u0026#34; fi echo \u0026#34;Boot syateki server START\u0026#34; /usr/bin/python3 /home/pi/Boot/boot_syateki_server.py \u0026amp; echo \u0026#34;Boot syateki server END\u0026#34; exit 0 Reboot raspi and confirm whether speaker sounds\npi@raspberrypi:~ $ sudo reboot カラー図解　最新　Ｒａｓｐｂｅｒｒｙ　Ｐｉで学ぶ電子工作　作って動かしてしくみがわかる (ブルーバックス) 講談社 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/12/","summary":"Purpose Play startup sound when raspi boots\nPrepare  Raspberry Pi Zero W Gravity: UART MP3 Voice Module  Reference  https://qiita.com/ikemura23/items/6f9adce99a3db555a0e4 http://hendigi.karaage.xyz/2016/11/auto-boot/ https://tomosoft.jp/design/?p=11677 https://wiki.dfrobot.com/Voice_Module_SKU__DFR0534  Auto run methods /etc/rc.local Run script as root\ncrontab Run script as user\nsystemd Manage as a service\nImplement Select /etc/rc.local at this time\nEdit /home/pi/Boot/boot_syateki_server.py as below\nimport serial # open serial port s = serial.Serial(\u0026#39;/dev/serial0\u0026#39;, 9600, timeout=10) # set volume 0x16 (0x00 - 0x1E) s.","title":"[Raspi Zero W]Play startup sound with UART MP3 Voice Module"},{"content":"Environment  Board  Raspberry Pi Zero W   OS  result of lsb_release -a    No LSB modules are available. Distributor ID: Raspbian Description: Raspbian GNU/Linux 10 (buster) Release: 10 Codename: buster  OLED display  HiLetgo 0.96\u0026quot; I2C シリアル 128×64 OLED LCDディスプレイSSD1306液晶     Setting i2c Enabling i2c  sudo raspi-config select 5 Interfacing Options select P5 I2C select Yes  Installing packages sudo apt-get update sudo apt-get install i2c-tools python-smbus Connect to OLED Wiring  confirm pin assign  http://xn\u0026ndash;ccke1di9d4h.com/blog/raspberrypi-zero-wh-pin/   connect 3.3V, GND, SCL, SDA  Check i2c  confirm slave address by sudo i2cdetect -y 1   0 1 2 3 4 5 6 7 8 9 a b c d e f 00: -- -- -- -- -- -- -- -- -- -- -- -- -- 10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 30: -- -- -- -- -- -- -- -- -- -- -- -- 3c -- -- -- 40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 70: -- -- -- -- -- -- -- -- Software Installation git clone https://github.com/adafruit/Adafruit_Python_SSD1306.git cd Adafruit_Python_SSD1306 sudo python setup.py install Run example cd examples python stats.py カラー図解　最新　Ｒａｓｐｂｅｒｒｙ　Ｐｉで学ぶ電子工作　作って動かしてしくみがわかる (ブルーバックス) 講談社 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/11/","summary":"Environment  Board  Raspberry Pi Zero W   OS  result of lsb_release -a    No LSB modules are available. Distributor ID: Raspbian Description: Raspbian GNU/Linux 10 (buster) Release: 10 Codename: buster  OLED display  HiLetgo 0.96\u0026quot; I2C シリアル 128×64 OLED LCDディスプレイSSD1306液晶     Setting i2c Enabling i2c  sudo raspi-config select 5 Interfacing Options select P5 I2C select Yes  Installing packages sudo apt-get update sudo apt-get install i2c-tools python-smbus Connect to OLED Wiring  confirm pin assign  http://xn\u0026ndash;ccke1di9d4h.","title":"Using an I2C OLED Display with the Raspberry Pi Zero W"},{"content":"elecrowへプリント基板を発注する際に必要なファイル変換作業をソフトで自動化した。\n作成ソフト  動作OS：Windows 開発環境：Visual Studio 2017  以下のGitHubのページから実行ファイルをダウンロードできる。 https://github.com/kouya17/KiCadHelper/releases\n(ページ中のAssets内KiCadHelper.zipをクリック)\n本ソフトが代替する作業 本ソフトは以下書籍(KiCadではじめる「プリント基板」製作)中のp131-p133の作業を代替する。\nKiCadではじめる「プリント基板」製作\n外川貴規 工学社 2018年02月\n売り上げランキング : 楽天ブックスで購入Amazonで購入 by ヨメレバ 具体的には以下の処理を行う。\n 製造ファイル・ドリルファイルのファイル名統一 拡張子の変更(.drl→.txt、.gm1→.gml) 必要ファイルをzipに圧縮  各ファイル及びzipファイル名は「elecrow-プロジェクトフォルダ名-日付」となる。\nzip圧縮前のフォルダはプロジェクトフォルダ下に残る。\n使い方  ソフトを起動する プロジェクトフォルダに、すでに製造ファイル・ドリルファイルを出力済みのKiCadプロジェクトのフォルダを指定する zipファイル出力先に、発注用のzipファイルを出力する場所を指定する 開始ボタンを押す  画面下部にログが表示され、処理中にエラー等があればエラーログが出力される。\n","permalink":"https://kouya17.com/posts/9/","summary":"elecrowへプリント基板を発注する際に必要なファイル変換作業をソフトで自動化した。\n作成ソフト  動作OS：Windows 開発環境：Visual Studio 2017  以下のGitHubのページから実行ファイルをダウンロードできる。 https://github.com/kouya17/KiCadHelper/releases\n(ページ中のAssets内KiCadHelper.zipをクリック)\n本ソフトが代替する作業 本ソフトは以下書籍(KiCadではじめる「プリント基板」製作)中のp131-p133の作業を代替する。\nKiCadではじめる「プリント基板」製作\n外川貴規 工学社 2018年02月\n売り上げランキング : 楽天ブックスで購入Amazonで購入 by ヨメレバ 具体的には以下の処理を行う。\n 製造ファイル・ドリルファイルのファイル名統一 拡張子の変更(.drl→.txt、.gm1→.gml) 必要ファイルをzipに圧縮  各ファイル及びzipファイル名は「elecrow-プロジェクトフォルダ名-日付」となる。\nzip圧縮前のフォルダはプロジェクトフォルダ下に残る。\n使い方  ソフトを起動する プロジェクトフォルダに、すでに製造ファイル・ドリルファイルを出力済みのKiCadプロジェクトのフォルダを指定する zipファイル出力先に、発注用のzipファイルを出力する場所を指定する 開始ボタンを押す  画面下部にログが表示され、処理中にエラー等があればエラーログが出力される。","title":"elecrow発注用zipファイル作成ソフト(KiCad向け)を作った"},{"content":"参考サイト Bootstrap公式ドキュメント\nhttps://getbootstrap.com/docs/4.3/components/card/\nCard decksの部分をほぼそのまま使った。\ncss部分は以下のQiitaの記事を参考にさせていただいた。\nhttps://qiita.com/iwato/items/840b831ad66fec0dd4c1\nコード html \u0026lt;div class=\u0026quot;col-sm-4\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;card card-link post-card\u0026quot;\u0026gt; \u0026lt;a href=\u0026quot;{% url 'posts:post_detail' post.id %}\u0026quot;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;img class=\u0026quot;card-img-top\u0026quot; src=\u0026quot;{{ post.middle_image.url }}\u0026quot; alt=\u0026quot;thumbnail\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;card-body\u0026quot;\u0026gt; \u0026lt;h5 class=\u0026quot;card-title\u0026quot;\u0026gt;{{ post.title }}\u0026lt;/h5\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;card-footer\u0026quot;\u0026gt; \u0026lt;small class=\u0026quot;text-muted\u0026quot;\u0026gt;published at {{ post.published|date:\u0026quot;Y/n/j\u0026quot; }}\u0026lt;/small\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 一行に最大3つ記事を表示するためにcol-sm-4を指定している。\n記事のサムネイル、タイトル、公開日を表示するようにしている。\ncss /*div要素全体にリンクをつけるために必要な要素*/ .card-link { position: relative; } .card-link a { position: absolute; top: 0; left: 0; height:100%; width: 100%; } .post-card { margin-top: 5px; margin-bottom: 5px; } カード全体にリンクをつけるために.card-linkに対してcssを設定した。\nまた、カードの上下に間隔がなかったため、marginを別途設定した。\n Djangoビギナーズブック カットシステム Amazon 楽天   ","permalink":"https://kouya17.com/posts/8/","summary":"参考サイト Bootstrap公式ドキュメント\nhttps://getbootstrap.com/docs/4.3/components/card/\nCard decksの部分をほぼそのまま使った。\ncss部分は以下のQiitaの記事を参考にさせていただいた。\nhttps://qiita.com/iwato/items/840b831ad66fec0dd4c1\nコード html \u0026lt;div class=\u0026quot;col-sm-4\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;card card-link post-card\u0026quot;\u0026gt; \u0026lt;a href=\u0026quot;{% url 'posts:post_detail' post.id %}\u0026quot;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;img class=\u0026quot;card-img-top\u0026quot; src=\u0026quot;{{ post.middle_image.url }}\u0026quot; alt=\u0026quot;thumbnail\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;card-body\u0026quot;\u0026gt; \u0026lt;h5 class=\u0026quot;card-title\u0026quot;\u0026gt;{{ post.title }}\u0026lt;/h5\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;card-footer\u0026quot;\u0026gt; \u0026lt;small class=\u0026quot;text-muted\u0026quot;\u0026gt;published at {{ post.published|date:\u0026quot;Y/n/j\u0026quot; }}\u0026lt;/small\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 一行に最大3つ記事を表示するためにcol-sm-4を指定している。\n記事のサムネイル、タイトル、公開日を表示するようにしている。\ncss /*div要素全体にリンクをつけるために必要な要素*/ .card-link { position: relative; } .card-link a { position: absolute; top: 0; left: 0; height:100%; width: 100%; } .post-card { margin-top: 5px; margin-bottom: 5px; } カード全体にリンクをつけるために.","title":"Djangoで記事をカード型で表示するようにした"},{"content":"参考ページ  https://programmer-jobs.blogspot.com/2012/12/djangodjango-taggit.html\ndjango-taggitとdjango-taggit-templatetagsがあればできるらしい。  実装した流れ 1 setting.pyを確認し、taggitが導入されていることを確認(taggitは昔すでに導入済み)\nただ、taggit-templatetagsは導入されていなかった。\n2 pip install django-taggit-templatetagsでインストール\n3 setting.pyのINSTALLED_APPSにtaggit-templatetagsを追加\n INSTALLED_APPS = ( ... \u0026quot;taggit\u0026quot;, \u0026quot;taggit_templatetags\u0026quot;, ... ) 4 pip freezeでインストールしたtaggit-templatetagsのバージョンを確認\n$ pip freeze ... django-taggit-templatetags==0.2.5 ... 5 requirements.txtに4.で確認したバージョン情報を追記\n... django-taggit-templatetags==0.2.5 ... 6 デプロイ\n7 動作確認したらエラー発生\n File \u0026quot;...templatetag_sugar/parser.py\u0026quot;, line 5, in \u0026lt;module\u0026gt; from django.db.models.loading import cache No module named 'django.db.models.loading' django.db.models.loadingがないらしい。\ngoogle先生に聞くと、django.db.models.loadingはDjango1.9で廃止され、現在はdjango.appを使う必要があるようだ。\nhttps://stackoverflow.com/questions/36234635/what-is-the-equivalent-of-django-db-models-loading-get-model-in-django-1-9\n8 tamplatetag_sugar/parser.pyを修正\n- from django.db.models.loading import cache + from django.apps import apps ... - return [(self, self.name, cache.get_model(app, model))] + return [(self, self.name, apps.get_model(app, model))] 9 動いた\n雑記 GitHubを見ると、django-taggit-templatetagsは9年前から更新されてないようだった。\nとりあえずググったらこれがトップにあったので使ったが、今はより適したパッケージがあるかもしれない。\n Djangoビギナーズブック カットシステム Amazon 楽天   ","permalink":"https://kouya17.com/posts/7/","summary":"参考ページ  https://programmer-jobs.blogspot.com/2012/12/djangodjango-taggit.html\ndjango-taggitとdjango-taggit-templatetagsがあればできるらしい。  実装した流れ 1 setting.pyを確認し、taggitが導入されていることを確認(taggitは昔すでに導入済み)\nただ、taggit-templatetagsは導入されていなかった。\n2 pip install django-taggit-templatetagsでインストール\n3 setting.pyのINSTALLED_APPSにtaggit-templatetagsを追加\n INSTALLED_APPS = ( ... \u0026quot;taggit\u0026quot;, \u0026quot;taggit_templatetags\u0026quot;, ... ) 4 pip freezeでインストールしたtaggit-templatetagsのバージョンを確認\n$ pip freeze ... django-taggit-templatetags==0.2.5 ... 5 requirements.txtに4.で確認したバージョン情報を追記\n... django-taggit-templatetags==0.2.5 ... 6 デプロイ\n7 動作確認したらエラー発生\n File \u0026quot;...templatetag_sugar/parser.py\u0026quot;, line 5, in \u0026lt;module\u0026gt; from django.db.models.loading import cache No module named 'django.db.models.loading' django.db.models.loadingがないらしい。\ngoogle先生に聞くと、django.db.models.loadingはDjango1.9で廃止され、現在はdjango.appを使う必要があるようだ。\nhttps://stackoverflow.com/questions/36234635/what-is-the-equivalent-of-django-db-models-loading-get-model-in-django-1-9\n8 tamplatetag_sugar/parser.pyを修正\n- from django.db.models.loading import cache + from django.apps import apps .","title":"Djangoでタグクラウドを実装した"},{"content":"概要 VirtualBox6.0で共有フォルダの自動マウント機能がうまく効かなかったが解消した。\n問題 VirtualBox6.0は共有フォルダの自動マウントをオンにしてもうまくマウントされないらしい。\nhttps://unofficialtokyo.com/2018/12/virtualbox-ubuntu1804-on-windows/#Ubuntu\n今回の解決法 毎ログイン時にmountコマンドを実行する。\nhttps://www.souichi.club/technology/virtualbox-share/\nただ、上記サイト内の記述だとうまくパスワード入力が省略されなかった。\n以下のサイトの記述を参考にしたらうまくいった。\nhttps://www.usagi1975.com/31jan172009/\n ハッキング・ラボのつくりかた 仮想環境におけるハッカー体験学習 翔泳社 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/6/","summary":"概要 VirtualBox6.0で共有フォルダの自動マウント機能がうまく効かなかったが解消した。\n問題 VirtualBox6.0は共有フォルダの自動マウントをオンにしてもうまくマウントされないらしい。\nhttps://unofficialtokyo.com/2018/12/virtualbox-ubuntu1804-on-windows/#Ubuntu\n今回の解決法 毎ログイン時にmountコマンドを実行する。\nhttps://www.souichi.club/technology/virtualbox-share/\nただ、上記サイト内の記述だとうまくパスワード入力が省略されなかった。\n以下のサイトの記述を参考にしたらうまくいった。\nhttps://www.usagi1975.com/31jan172009/\n ハッキング・ラボのつくりかた 仮想環境におけるハッカー体験学習 翔泳社 Amazon Kindle 楽天 楽天Kobo   ","title":"VirtualBox6.0で共有フォルダの自動マウント機能が働かない"},{"content":"概要 GitHub Pagesを使って簡易ブログを公開しました。\nhttps://kouya17.github.io\n構成 公開環境：GitHub Pages\n静的サイトジェネレータ：Hugo\nテンプレートテーマ：aether\n雑記 無料でサイト公開できるのはすごい。\n静的サイトジェネレータとしてHexoも検討したが、ネット上の情報が中国語中心だったため断念した。\n作成物に関する投稿はGitHub Pagesのほうにして、こちらはより雑多なものを扱うことにする。\n 独習Git 翔泳社 Amazon Kindle 楽天 楽天Kobo   ","permalink":"https://kouya17.com/posts/4/","summary":"概要 GitHub Pagesを使って簡易ブログを公開しました。\nhttps://kouya17.github.io\n構成 公開環境：GitHub Pages\n静的サイトジェネレータ：Hugo\nテンプレートテーマ：aether\n雑記 無料でサイト公開できるのはすごい。\n静的サイトジェネレータとしてHexoも検討したが、ネット上の情報が中国語中心だったため断念した。\n作成物に関する投稿はGitHub Pagesのほうにして、こちらはより雑多なものを扱うことにする。\n 独習Git 翔泳社 Amazon Kindle 楽天 楽天Kobo   ","title":"GitHub Pagesを使って静的サイトを公開した"},{"content":"ソースコード https://github.com/kouya17/esp32_weather_light\n説明 ESP32を使って、LEDの色で天気通知をしてくれるプログラムを作りました。\n単3電池2本駆動で8時間程度しか持たないのが課題ですね…。\n IoT開発スタートブック　── ESP32でクラウドにつなげる電子工作をはじめよう！ 技術評論社 Amazon Kindle 楽天 楽天Kobo    ","permalink":"https://kouya17.com/posts/1/","summary":"ソースコード https://github.com/kouya17/esp32_weather_light\n説明 ESP32を使って、LEDの色で天気通知をしてくれるプログラムを作りました。\n単3電池2本駆動で8時間程度しか持たないのが課題ですね…。\n IoT開発スタートブック　── ESP32でクラウドにつなげる電子工作をはじめよう！ 技術評論社 Amazon Kindle 楽天 楽天Kobo    ","title":"LEDの色で天気通知する(ESP32)"}]