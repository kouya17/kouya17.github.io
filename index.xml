<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>kouya17.com</title><link>https://kouya17.com/</link><description>Recent content on kouya17.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 12 Jun 2022 18:44:00 +0900</lastBuildDate><atom:link href="https://kouya17.com/index.xml" rel="self" type="application/rss+xml"/><item><title>[C#]YouTubeLiveチャットに特定の文字列が書き込まれたことを検知する</title><link>https://kouya17.com/posts/youtube-chat-hooks1/</link><pubDate>Sun, 12 Jun 2022 18:44:00 +0900</pubDate><guid>https://kouya17.com/posts/youtube-chat-hooks1/</guid><description>試作ソフト動作時の様子 APIキー及び対象配信のURLを入力し、チャットを取得する。取得したチャットに対し、特定文字列が含まれているものを抽出して表示する。
上記GIF右側にあるソフトのコードは以下に置いている。
実装内容の詳細 YouTubeLiveチャットの取得 YouTubeLiveチャットはYouTube Data APIを利用して取得する。YouTube Data APIは、YouTubeが公開している、YouTube上の情報を参照及び変更できるAPIである。コードサンプルページを見ると、以下の言語向けのクライアントライブラリがあることを確認できる。
Go Java JavaScript .NET PHP Python Ruby 今回は.NET用のGoogleAPIクライアントライブラリを使う。
APIキーの準備 YouTube Data APIを利用するために、APIキーを作成する必要がある。APIキーの作成方法はここでは詳しく記載しない。以下のサイト等を参考にしてほしい。
プロジェクトへのライブラリーの追加 プロジェクトにGoogle.Apis.YouTube.v3をインストールする。以下手順の説明はVisual Studio 2022の利用を想定している。
Visual Studio 2022のタブから プロジェクト &amp;gt; NuGetパッケージの管理 を選択する。 Google.Apis.YouTube.v3を検索し、最新バージョンをインストールする。 プログラムの実装 チャットを取得するコードを実装する。チャットを取得するまでの流れは以下のような形にしている。
配信URLからVideo IDを取得する。 Video IDからData APIを使って対象配信のChat IDを取得する。 Chat IDからLive Streaming APIを使ってチャットのデータを取得する。 配信URLからVideo IDを取得する Video IDは配信URLのクエリパラメータ中のvパラメータに相当する。例えば、配信URLがhttps://www.youtube.com/watch?v=fLM31tHrD5MであればfLM31tHrD5MがVideo IDである。
Video ID抽出のための処理はSystem.Web.HttpUtility.ParseQueryString()を利用した。以下に実装のサンプルを記載する。なお、下記サンプルはvパラメータがない場合を想定していない。
var uri = new Uri(stringUrl); // stringUrlには配信URLが入る var query = System.</description></item><item><title>Raspberry Pi Zero W を使ってタイムラプス生成デバイスを作成する</title><link>https://kouya17.com/posts/timelapse-maker/</link><pubDate>Mon, 21 Mar 2022 13:03:00 +0900</pubDate><guid>https://kouya17.com/posts/timelapse-maker/</guid><description>デモ 本デバイスは以下の動画のように操作できる。
操作時の動画は以下のような感じです。 pic.twitter.com/7l2ocbjlh1
&amp;mdash; 青木晃也 (@aoki_kouya) March 20, 2022 本デバイスを使うと、以下のサイトにあるような動画を生成できる。
動画の出力条件のうち、主な項目は以下の通り。
再生速度: 30倍速 fps: 30 bps: 3Mbps HLS形式 音声なし 料理の動画を掲載しているのは、特に他にコンスタントに撮影できる対象が考えつかなかったため。これらの動画の撮影のために、3Dプリンターでプリントした部品を使ってラズパイとカメラを固定し、台所上部に吊り下げている。
Raspberry Pi Zeroを使って、思い付きでタイムラプスカメラを作りました。 pic.twitter.com/w1fCBfPVzm
&amp;mdash; 青木晃也 (@aoki_kouya) March 5, 2022 確認済み動作条件 ボード Raspberry Pi Zero W OS Raspberry Pi OS(Legacy) 2022-01-28 現時点(2022/03/21)で推奨OSとなっているRaspberry Pi OSはbullseye版だが、buster版(Legacy)の方を使う。これは、ffmpeg(今回利用するマルチメディア処理ソフト)まわりの環境がbullseye版とbuster版で異なっているため。bullseye版を利用する場合、ハードウェアエンコードを有効にするために、ffmpegをビルドする必要が出てくる。
必要な部品 Raspberry Pi Zero W (スイッチサイエンス) microSDカード (Amazon) Raspberry Pi カメラモジュール (スイッチサイエンス) Raspberry Pi Zero用カメラケーブル (スイッチサイエンス) ELECROW 3.</description></item><item><title>ReactでYouTubeのプレーヤーを制御する</title><link>https://kouya17.com/posts/react-youtube/</link><pubDate>Sat, 15 Jan 2022 13:15:00 +0900</pubDate><guid>https://kouya17.com/posts/react-youtube/</guid><description>react-youtubeというReactコンポーネントを利用したサンプルサイトを作成したので、紹介する。
作成したサンプルサイト ページを開くと、特定のYouTube動画が読みこまれる。YouTubeプレーヤーを操作する毎に操作情報(+操作時の再生位置)が記録される。記録された操作は、YouTubeプレーヤー下にテキストで表示される。操作情報は、「download」ボタンを押すことでファイルとして出力できる。
上記サンプルサイトでは、現状以下の操作のみ記録する。
再生準備完了(ready) 再生開始(play) 一時停止(pause) 再生終了(end) 視聴者毎の動画視聴状況を確認したいケースを考える 今回のサンプルサイトは、「視聴者ごとに、動画の視聴状況詳細を確認したいケース」を想定して作成した。視聴者は操作情報をファイルとして出力し、管理者へ提出する。管理者は提出されたファイルから、視聴状況を確認するという想定である。サンプルでは操作情報をファイルで出力しているが、これを「ユーザー情報を付与してサーバー上に保存」とかすれば、ファイルをやり取りするという手続きは不要になる。
今回想定したケースのように、YouTubeから情報を取得したい場合の手段としては、YouTube Data APIを活用するという方法がある。ただ、YouTube Data APIには、「特定のユーザーの、各視聴動画に対する操作情報」という細かい情報までは用意されていない。取得できても「特定のユーザーの視聴履歴」までである。
少し調べたところ、YouTubeのIFrame Player APIを使うと、YouTubeプレーヤーに対する操作を取得出来るようだったので、今回はそちらを利用した。
YouTubeのIFrame Player APIを利用する IFrame Player APIを利用することで、再生中の動画に関する情報を取得できる。情報の取得はJavaScript関数を使って行う。また、特定のプレーヤーイベントに対して、イベントリスナーを設定できる。今回のサンプルサイトを例にすると、再生準備完了(onReady)、再生開始(onPlay)、一時停止(onPause)、再生終了(onEnd)に対してイベントリスナーを設定している。
今回のサンプルサイトはNext.jsを使って作成したため、ReactコンポーネントとしてIFrame Player APIを利用できるreact-youtubeを利用した。
サンプルサイトの作成とデプロイ サンプルサイトの実装内容としては、react-youtubeのサンプルに、プレーヤーイベント発生時のイベントリスナーを設定し、操作情報を画面に出力及びファイルとしてダウンロード出来るようにしたくらいである。
ソースコードは以下に公開している。
サンプルサイトはGitHub Pagesを使って稼働させている。GitHub Pagesへのデプロイは、GitHub Actionsを利用して、mainブランチ更新時に自動で行われるようにしている。ここら辺は、以下のサイトを参考にさせていただいた。</description></item><item><title>機械学習を使ってApexのプレイ動画から武器使用率を算出する</title><link>https://kouya17.com/posts/44/</link><pubDate>Mon, 01 Nov 2021 22:45:01 +0900</pubDate><guid>https://kouya17.com/posts/44/</guid><description>最終的な目標はプレイスタイルの解析 近年、動画投稿・配信サービスの普及により、ゲーム実況というものがエンタメの1カテゴリとして確立してきていると思う。感染症流行の影響が大きいと思うが、2021年1月～3月の主要ゲーム配信サイト(Twitch、YouTube Gaming、Facebook Gaming)の合計視聴時間が前年同期比で80%増加したというデータもある。1ゲームのジャンルとしてはアクション・RPG・シミュレーションなどあらゆるものが実況の対象になるが、今回はFPSのゲーム実況にスポットを当てる。
FPSの分野では、オンライン対戦システムおよび、いわゆるランクシステムが整備されているものが多くなっている。ランクシステムは、プレイヤーの習熟度のようなものを可視化する。こうしたランクに関する情報は、視聴者にとって、プレイヤーのことをパッと理解するために非常に役立っていると思う。
現状、ランク情報やダメージ数、キルデス比等はゲームシステムから参照できるようになっていることが多いと思う。ただ、個人的にはより細かい、プレイヤーのプレイスタイルのようなものを可視化できる情報があると、プレイヤーについて理解を深めるためのよい材料になると思った。
最終的にはゲームシステムから直接確認出来るデータだけでなく、ゲームプレイ動画の解析結果から、プレイヤーのプレイスタイルをうまく可視化したい。今回は、Apexを題材にし、プレイヤーのプレイスタイルを示す1つの指標として、武器使用率をゲームプレイ動画から解析できないか試す。まあ、こういった情報はわざわざ動画を解析しなくても、ゲームシステム側で実装してもらえれば、より正確で容易にデータが取れるとは思う。
全体の作業の流れ 今回の全体の作業の流れは以下のようになっている。
各作業の内容について説明していく。ただし、ゲームプレイ動画のキャプチャ作業については特に説明することがないので、何も説明しない。
武器名表示領域の切り取り データセットを整備するために、ゲーム画面のうち、武器名を表示している領域のみを切り取った画像を作成する必要がある。今回は、動画に対して一定時間ごとのフレームを画像化→特定領域を切り取り→特定の場所に画像ファイルとして保存、という流れでデータセット用の画像を作成した。
一連の画像切り取り作業実施のために、ツールを作成した。GUI部分はPySimpleGUIを利用している。
主に以下を設定できる。
切り取り対象とする動画の保存場所 武器1欄(*1)の画像を保存する場所 武器2欄(*1)の画像を保存する場所 画像を切り抜く領域(*2)(全体サイズに対する割合で指定) (*2)で設定した領域からどれだけずらした画像を何個保存するか 画像切り取り対象とするフレームの時間間隔 (*1)Apexでは、武器を同時に2つ持つことができる。今回は武器1欄と武器2欄を別々のデータセットとして扱う。理由はそちらの方がより精度が上がりそうと思ったためである。
このツールを使えば、ボタンを1回ポチーすることで、後は待っていれば所定の場所に武器名表示領域の画像が保存される。
画像に対して正解ラベルを付ける データセットの正解ラベル付けについて、どのような形式で保存するのが一般的かを私は知らないのだが、今回は各正解ラベル(武器名)ごとに保存フォルダを分けることにした。
以下のように、全武器+&amp;ldquo;武器なし&amp;rdquo;(_None)について画像をフォルダ分けした。
この作業にも専用ツールを作成した。
データセットが置かれている場所を指定して、startボタンを押すと、所定の個数づつ未分類の画像が表示される。表示された画像について、正解となる武器名のボタンをポチポチしていき、画像をフォルダ分けする。
ただ、今回データセットの元にしたキャプチャ動画は武器を決まった順番で使うようにしていた。そのため、切り抜かれた画像が最初から決まった武器順で並んでおり、結局このツールはあまり使わなかった。
学習・モデルの保存 データセットの準備が出来たので、ようやく学習をする。学習部分はコードベースで説明する。
まず、データセットの読み込みを行う。かなりコードが汚い。武器名のリストは、以下のコードに含まれていない関数を使ってファイルから読みこんでいる。
Kerasを用いてモデルを生成する関数を作成する。
Optunaを使ってハイパーパラメータを調整しつつ、成績の良いモデルを保存する。(といってもマシンが貧弱で学習に時間がかかるので、5回しか試行を回していない。)
上記のコードを実行したところ、とりあえず今回はテストデータの正解率が99.89%のモデルが出来た。テストデータの推論結果をMatplotlibを使って画像とともに確認してみる。
いい感じに推論できてそうだ。誤答をしたデータについても確認してみる。
今回は2つのデータの推論が間違っていたようだ。上は正答がFlatlineなのに対し、Devotionと推論してしまっており、下は正答がNoneなのに対し、R-301と推論してしまっている。
作成したモデルによるプレイ動画の解析 モデルが出来たので、動画を解析する。解析用のソフトを作成した。
学習用に使ったデータとは別のキャプチャ動画を使って解析したところ、以下のような結果になった。
解析結果を見たところ、大体は合っていると思うが、誤検出されている武器名が末尾に並んでいる。解析結果は正確なものではないため、データの扱いは少し考えないといけない。
最初は自作モデルを作らなくても行けると思っていた 今回、武器使用率算出のために&amp;quot;機械学習のモデルを作成・利用する&amp;quot;という手段を選択した。
この手段を取る前は、TesseractとPyOCRを利用して文字列を認識する方法をとっていた。ただ、認識精度が厳しかったので、利用を断念した。機械学習の勉強にもなると思い、代わりに自作モデルの作成という方法をとった。
今回の識別対象は決まり切った文字列なので、機械学習を使わなくても、より正確な認識方法があるかもしれない。というか、私はApexにあまり詳しくないので、そもそもゲームシステム内で武器使用率を確認できる方法があるかもしれない。
参考にした書籍・動画 機械学習については主にネット上の情報と、以下の書籍の内容を流し読みした程度の理解度である。
ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装 オライリージャパン (2016/9/24) Amazon 楽天市場 また、上記書籍を数年前に読んだときは誤差逆伝播法がいまいち理解できなかった。しかし、Twitterのタイムラインで流れてきた以下の動画が非常に参考になり、とりあえず分かった気にはなれた。出来るだけ条件を簡易にして、高校数学の範囲で説明されている。
まとめ 機械学習を利用してApexのプレイ動画から武器使用率を算出するためのツール群を作成した。解析結果には誤差が含まれるため、利用するためにはまだ工夫をする必要があると思う。
色々試行錯誤をする中で、モデルの性能に最も寄与したのはデータセットの作り方だった。データセットに偏りがあると、それを学習したモデルも偏りのある結果を出力する。データセットの質の重要性を、身をもって実感した。この部分については本記事で書けなかったので、またどこかで記事を書くかもしれない。
Q1 2021 Live Game Streaming Trends - Stream Hatchet&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>顔認識+サーボモーターで顔追従モニターの作成</title><link>https://kouya17.com/posts/43/</link><pubDate>Fri, 23 Jul 2021 05:14:50 +0900</pubDate><guid>https://kouya17.com/posts/43/</guid><description>先日、NT金沢2021というイベントに、リモートで出展参加した。「リモートで出展参加」というのは、具体的に言うと、展示物としてはオンラインで遊べるものをポスターで出展して、私はGoogleMeet(ビデオ会議)をつないでリモートで一部作品の説明をしていた。(関係機材は現地出展した友人に運んでもらった。)
GoogleMeetをつなぐには、とりあえずPCが現地(NT金沢の会場)にあれば事足りる。ただせっかくなので、モニターが、話している相手の顔を追跡するようにした。
動作の様子 人の顔に追従するモニタを試作中…。
■使用素材
画像：いらすとや(https://t.co/H01vPNj6QI) pic.twitter.com/H8fLDuSRjz
&amp;mdash; 青木晃也 (@aoki_kouya) May 30, 2021 検証時の動画だが、概ね完成形。当日はこのモニターに、GoogleMeetの画面を表示して運用していた。
ハードウェア 使用部品一覧 部品名 備考 JetsonNano開発者キット 顔認識・ビデオ会議・サーボ制御用 Webカメラ×2 1個は顔認識用、1個はビデオ会議用 モニター スピーカーも内蔵 サーボモーター S03T/2BBMG/F×2 強そうなものをチョイス PCA9685モジュール サーボ制御用 サーボやモニタ等を固定するための部品は3Dプリンターで作成した。
WebカメラはとりあえずAmazonで安いものを適当に買った。1種類目はUSB Wi-Fi子機との相性が良くなかったようで、Wi-Fiの接続が不安定になった。2種類目は特に問題なく動いていそうだったので、採用した。ただ、この記事を書いている時点で、採用したWebカメラのAmazonの商品ページはリンク切れになっていた。
ソフトウェア ソフトウェアに関する部分としては、JetsonNano起動後にコンソールで顔追従用プログラムを実行した後、ブラウザ(Chromium)でGoogleMeetを動かしていた。GoogleMeetの方は特に書くことがないため、顔追従用プログラムのソースコード及びセットアップ周りを書いていく。
顔追従用プログラムのソース ソースは以下に置いてある。
顔追従用プログラムのセットアップ周り 上記のソースを動かすには、関係ライブラリをインストールする必要がある。
PCA9685用ライブラリのインストール サーボ制御モジュールPCA9685をPythonから制御するためのPythonパッケージをインストールする。まず以下のコマンドでpip3をインストールする。
sudo apt-get install python3-pip 参考元: NVIDIA Jetson Nano 開発者キットに TensorFlow をインストールする - Qiita</description></item><item><title>Dartでブロックチェーン(のようなもの)を実装してみた</title><link>https://kouya17.com/posts/42/</link><pubDate>Tue, 04 May 2021 16:24:57 +0900</pubDate><guid>https://kouya17.com/posts/42/</guid><description>以下の記事を参考にさせていただき、Dartを使ってブロックチェーン(のようなもの)を実装してみた。
動作の様子 お試しノードをHeroku上で動かしている。https://dart-blockchain-test-app.herokuapp.com/publicにアクセスすることで、 お試しノードの情報を色々見る事ができる。
ブロックチェーンは特にファイル等で永続的に残しているわけではない。Herokuはしばらくアクセスがないとアプリが停止するため、そのタイミングで情報がリセットされる。
ソースコードとローカルでのノードの立て方 ソースコードは以下に置いてある。設計部分は参考元サイトとほぼほぼ同じになっている。
ローカルPCでノードを立てる場合は、Dartの実行環境を整えた状態で、ソースコードをダウンロードし、以下のコマンドを実行すれば良い。
dart pub get dart pub global activate webdev webdev build --output web:public dart bin/back.dart --port 6565 上記コマンドを実行後、ブラウザでhttp://localhost:6565/publicにアクセスすると、ノードの情報を確認できる。
bin/back.dartを実行する際、--peerオプションで、P2P通信を行うノードを指定できる。Heroku上のお試しノードを立ち上げた状態で、以下のようにbin/back.dartを実行すれば、Heroku上のノードとの取引を行うこともできる。
dart bin/back.dart --port 6565 --peer ws://dart-blockchain-test-app.herokuapp.com/ws ローカルPC上のノードとHeroku上のノードはWebSocketでP2P通信を実現しており、簡単な構成図を書くと以下のようになる。
HerokuでDartアプリを動かす 今回Heroku上でDartアプリを実行できるようにした。Heroku上の設定周りは以下のリポジトリを利用させていただいた。
なお、上記リポジトリのREADME中にはheroku config:add BUILDPACK_URL=https://github.com/igrigorik/heroku-buildpack-dart.gitというコマンドでビルドパックを適用するよう書いてある。しかし、以下のプルリクエストにある通り、今はheroku buildpacks:set https://github.com/igrigorik/heroku-buildpack-dart.gitというコマンドでビルドパックを登録するのが正しいらしい。
とりあえず形にはなったが、まだまだ不明点がある 今回、ブロックチェーン(みたいなもの)を実装することで、ブロックチェーンへの理解が深まった。ただ、実装していく過程で、Bitcoinに対する疑問点が新たに出てきた。
マイナーへの報酬は普通のトランザクションと比べて特殊な形式になると思うが、どのような形式になっているか。 大体のネット上の記事では&amp;quot;悪意のあるブロックチェーンを生成するには、善意のブロックチェーンよりも長いチェーンを生成する必要があるため困難&amp;quot;という説明がされている。difficultyを改竄できれば、計算能力が乏しくても、いくらでも長い悪意のあるブロックチェーンを生成できそうだが、そこら辺はどのように回避しているか。 P2P通信をどのように実現しているか。 アフィリエイト
絵で見てわかるブロックチェーンの仕組み 翔泳社 (2020/12/21) Amazon Kindle 楽天ブックス 楽天Kobo</description></item><item><title>Bitcoinのブロックチェーンの中身を見てみる</title><link>https://kouya17.com/posts/41/</link><pubDate>Thu, 29 Apr 2021 05:05:55 +0900</pubDate><guid>https://kouya17.com/posts/41/</guid><description>ただのにわかではあるが、ブロックチェーンは信頼性が重要な技術だと思っている。傍目から見たら、これだけ広く不特定多数に使われていても、これまで特に技術自体について大きく信頼性が揺らぐような事象は起きていないように見える。実用に十分耐えうる技術のようだ。なんとなく興味が出てきたのでゴールデンウィークの期間を使って色々勉強してみる。
Bitcoin Coreのインストール まずは暗号通貨Bitcoinを題材に、ブロックチェーンの&amp;quot;ブロック&amp;quot;の中身を見てみる。現在までのブロックを全て取得するために、以下を参考にBitcoin Coreをインストールする。
初回起動時はデータ保存場所を聞かれるが、デフォルトの場所を使うようにする。
起動後はブロックチェーンデータの同期が始まる。
完全な同期までは数日かかるらしい。自分の場合は3日ほど放置していたら完了していた。
ブロックの中身を読んでみる デフォルトだとMacOSは/Users/&amp;lt;username&amp;gt;/Library/Application Support/Bitcoin/blocks/以下にブロックデータが置かれるらしい。データの総サイズを確認してみる。
$ du -hs /Users/&amp;lt;username&amp;gt;/Library/Application\ Support/Bitcoin/blocks/ 361G /Users/&amp;lt;username&amp;gt;/Library/Application Support/Bitcoin/blocks/ サイズがだいぶでかい。これはこれからもトランザクションが発生するごとに増えていくのだろう。
中身はバイナリになっている。フォーマットは以下のようになっているらしい。
フィールド サイズ magic bytes 4 bytes size 4 bytes block header 80 bytes tx count 可変 transaction date 可変 引用元
上記引用元サイトの情報を参考に、バイナリを読んでみる。(といっても、バイナリを読まなくても、上記引用元サイトに、必要な情報は全て書かれているので、ほぼ上記サイトの内容をかいつまんで日本語訳するような内容になる。)
$ hexdump -C -n 293 blk00000.dat 00000000 f9 be b4 d9 1d 01 00 00 01 00 00 00 00 00 00 00 |.</description></item><item><title>所定のYouTubeチャンネルの配信予定をLEDで通知する(M5StickC)</title><link>https://kouya17.com/posts/40/</link><pubDate>Sat, 03 Apr 2021 23:09:39 +0900</pubDate><guid>https://kouya17.com/posts/40/</guid><description>できたもの 動画の出来が色々とアレだが、動作時の様子を以下の動画の後半で確認できる。
動画だと映りが悪いが、写真だとLEDをつけた時の様子は以下のような感じになる。
作成目的 M5系列からYouTube Data APIを使った作例を作ってみたくて、作成した。
使用部品 M5StickC　1個 PCA9685 16チャンネル PWMモジュール　3個 フルカラーLED　12個 筐体設計 STLデータは以下に公開してある。
M5StickCとPCA9685モジュールが3個入るような箱を設計した。最初はだいぶコンパクトに設計していたのだが、配線が箱に収まらないことが分かり、途中で拡張した。もともとはM5StickCも箱の中に収める想定だったのだが、入らなかったので、横につけることにした。
フルカラーLEDは配置する場所に自由度を持たせられるように、個別に格納する箱を作成して、それぞれ4本の配線でM5StickC側の箱と接続するようにした。
配線 今回の配線の概要図を以下に示す。
PCA9685モジュールとフルカラーLEDの接続部分はテキトーになっているが、LEDについてはRGB順で1個目のLEDは1個目のPCA9685モジュールの0,1,2chに接続、2個目のLEDは1個目のPCA9685モジュールの3,4,5chに接続…という形で接続している。
ここが一番しんどかった。PCA9685モジュールを3個使っているのでやろうと思えば最大16個のフルカラーLEDを接続できるが、12個配線したところで力尽きた。
ソフト実装 ソフト全体は以下のリポジトリに置いてある。
src/main.cppについて以下の部分は各ユーザー毎に書き換える必要がある。
APIキー アクセスポイントのSSID アクセスポイントのパスワード あと、src/main.cpp中のchannelsをいじれば対象とするYouTubeチャンネルとLEDの色を変更できる。
M5StickCからYouTube Data APIを利用する部分は、以下のAPIラッパーを利用させていただいた。
ただし、このままではチャンネル情報しか取得できないため、以下の関数等を追加している。
std::vector&amp;lt;String&amp;gt; YoutubeApi::getUpcomingBroadcasts(char *channelId) 指定されたChannelIDのチャンネルの配信予定(VideoID)を取得する。 BroadcastDetails YoutubeApi::getBroadcastDetails(char *videoId) 指定されたVideoIDの動画の配信詳細情報を取得する。 配信開始予定時刻は詳細情報からしか取れない。 課題 きちんと検証はしていないが、現状のソースだと、YouTube Data APIのクォータを異常に消費する。本当は1分ごとに情報取得とかをしたかったが、無料枠ではクォータが足りなくなる。
各LEDに紐づくチャンネルIDと、LEDの色がハードコーディングされているので、外部から設定できるようにしたい。
アフィリエイト
M5Stack M5StickC ESP32ミニIoT開発ボードm5stack iotキット フィンガーコンピューターカラー0.</description></item><item><title>html要素の背景色が変化するアニメーションをCSSで(無理やり)実装する</title><link>https://kouya17.com/posts/39/</link><pubDate>Sun, 07 Feb 2021 16:27:14 +0900</pubDate><guid>https://kouya17.com/posts/39/</guid><description>CSSによるアニメーションについて CSSではanimationプロパティを利用することによって、JavaScriptを使うことなく、HTML要素にアニメーションを付与することができる。
CSSアニメーションの概要は以下の記事を参考にさせていただいた。
今回作りたいもの 今回、以下のような流れでアニメーションする要素を作りたい。
要素の左端から背景色がだんだん元々の背景色Aから色B変わっていき、背景全体が色Bになる。この時、色Aと色Bの境界はグラデーションにする。 背景全体が色Bのまましばらく維持する。 要素の左端から背景色がだんだん色Bから色Aに変わっていき、背景全体が色Aにもどる。この時、色Bと色Aの境界はグラデーションにする。 少し調べてみると、グラデーションは&amp;lt;gradient&amp;gt;データ型を利用すれば問題なさそうだった。問題は、グラデーションの境界位置をアニメーションする部分である。以下のように、ただ単に0%と100%のkeyframeを指定しただけでは、思ったようなアニメーションにならなかった。
See the Pen wrong case by kouya17 (@kouya17) on CodePen. 私の知識ではスマートなやり方が分からなかったので、keyframeを0%から100%まですべて指定する形で(無理やり)実装することにした。 keyframes生成用のpythonスクリプトを作成する pythonで、以下のようなkeyframes生成用のプログラムを作成した。
このスクリプトによって生成したテキストをkeyframesとしてCSSに貼り付けると、以下のようなアニメーションになる。
See the Pen linear by kouya17 (@kouya17) on CodePen. とりあえずやりたいことは出来た。しかし、完成したアニメーションを見ると、動きにメリハリがない。アニメーションの進行具合はCSSの`animation`要素の`animation-timing-function`プロパティを設定すればいじることができる。 ただ、今回のパターンだと、うまく設定できないようだ。下の例はanimation-timing-functionプロパティをlinearとease-in-outにそれぞれ設定したものを並べた例である。それぞれのアニメーションの進行具合に違いが生じるはずだが、特に差がない。
See the Pen linear and ease-in-out by kouya17 (@kouya17) on CodePen. 先ほど作成したpythonスクリプトを修正して、イージングの要素も取り入れることにする。 イージングの要素を実装する イージングの要素を実装するには、3次ベジェ曲線について把握しておく必要がある。ベジェ曲線については以下の記事を参考にさせていただいた。
pythonでベジェ曲線生成クラスを作成する pythonでベジェ曲線を生成するためのクラスを以下のように作成した。
上のスクリプトを実行すると p0=[0,0], p1=[0,1], p2=[1,0], p3=[1,1] とした時の3次ベジェ曲線が生成される。グラフにプロットすると以下のような曲線になる。
keyframes生成用のpythonスクリプトを修正する 先ほど作成したkeyframes生成用のスクリプトにベジェ曲線生成用クラスを組み込む。組み込んだ後のスクリプトは以下のようになる。
このスクリプトを使って、 p0=[0,0], p1=[0,0.</description></item><item><title>Chromeからactix-webにHTTPリクエストを実行した際に発生したCORSエラーの解消</title><link>https://kouya17.com/posts/38/</link><pubDate>Sat, 16 Jan 2021 15:46:11 +0900</pubDate><guid>https://kouya17.com/posts/38/</guid><description>環境についてざっくり クライアント側 ブラウザ: Google Chrome バージョン 87.0.4280.141 (Official Build) (64ビット) OS: Windows 10 Home 19041.746 HTTPクライアント: axios v0.21.1 サーバ側 OS: Raspbian GNU/Linux 10 (buster) webフレームワーク: actix-web v3.3.2 現象 Access to XMLHttpRequest ... has been blocked by CORS policyというエラーが出力される Raspberry Pi 4 Model B上でバックエンド(actix-web)とフロントエンド(Vue CLI)を稼働させている。WindowsPCからブラウザを使ってフロントエンドにアクセスし、axiosを使ってバックエンド(actix-web)にGETリクエストを実行した。すると、ブラウザのコンソールに以下のようなエラーが出力された。(ローカルIPが記述されていたところは一部編集している。)
Access to XMLHttpRequest at &amp;#39;http://&amp;lt;localIP&amp;gt;:50001/&amp;#39; from origin &amp;#39;http://&amp;lt;localIP&amp;gt;:8080&amp;#39; has been blocked by CORS policy: No &amp;#39;Access-Control-Allow-Origin&amp;#39; header is present on the requested resource.</description></item><item><title>今年(2020年)作ったものを振り返る</title><link>https://kouya17.com/posts/37/</link><pubDate>Wed, 30 Dec 2020 21:43:06 +0900</pubDate><guid>https://kouya17.com/posts/37/</guid><description>今年もあっという間だったがもう終わるらしい。この1年で周りの環境は急激に変わった。ただ、自分の趣味のモノづくり周りについては、やってることは特に去年と変わらず、自分の作りたいものを作るという感じだった。
ついでに過去の振り返りもリンクを貼り付けておく。
今年(2019年)作ったものを振り返る | kouya17.com
今年作ったもの 去年11月~2月　ボール収集ロボの作成 色が付いたボールを所定の場所に集めるロボコンのようなものに参加する機会があったので、作成した。
機体は3Dプリンタで作成し、ラズパイ4とラズパイ用カメラを搭載している。カメラからの画像を処理し、所定の色のボールを認識し、ボールに近づく。ボールにある程度近づいたらボールを確保する。
機体の写真を以下に載せる。
機体上部に謎の模様があるのは、より広い範囲が見えるカメラが取り付けられているJetsonNanoから機体を制御するため。JetsonNanoは機体の向きを確認し、ソケット通信によって機体に指令を送る。
できたもの
関連記事 カメラ画像から対象物の向きを検出する(OpenCV) | kouya17.com 2月~4月　所属サークルのHP作成 Laravelの勉強がてら、HPを作成した。初めてVue.jsを使ってSPAを作成したこともあり、フロントエンドとバックエンドの分離についての理解を深めることができた。
画面デザインやリソースの構造についてあまりしっかり検討せずに進めてしまったのが少し心残りではある。特にリソースの構造については、現状、記事を作品カテゴリとコラムカテゴリにカテゴリ分けしているのだが、特にカテゴリ分けの意味がないという問題がある。ここについては何とか出来たんじゃないかなと思っている。
できたもの 関連記事 Laravel+Vue.jsでLighthouseのスコアを0点から97点にした(バンドルサイズ削減) | kouya17.com Laravel+Vue.jsでSPA(シングルページアプリケーション)を作成した | kouya17.com Vue.jsを使ったSPAにおいてTwitter等でのリンク表示がいい感じになるようにする | kouya17.com 3月~　射的ゲーム作成 今年の大部分はこの作業をしていた。昨年から引き続きこの作業はしているが、まだしっかりとした形にはなっていない。来年にはどこかで公開できるようにしたい。
できたもの まだ公開できるものはなし。
関連記事 マトリクスLEDドライバを使ってフルカラーLEDを制御してみる | kouya17.com 赤外線を周期的に(矩形波で)出力するモジュールを作る | kouya17.com 5月　感染シミュレーション用プログラムの作成 学生時代に勉強していた数値計算の復習がてら、感染シミュレーション用のプログラムを作成した。
SEIRモデルという単語は結構色んなところで見かけるようになったので、ここで少し勉強して、理解を深めておいて良かったと思っている。
できたもの
関連記事 感染症数理モデルについて触りの部分だけ学ぶ | kouya17.com 都道府県間通勤・通学を考慮したパンデミックシミュレーションもどき | kouya17.com 全国規模の感染シミュレーションと結果の可視化をしてみる | kouya17.com 10月　換気状況モニタの作成 感染症関連で何か作れないか考え、二酸化炭素濃度モニタを作成した。こういった環境モニタ系はもっと利用シーンが増えると思っていた(店舗が換気状況のアピールに使う等)のだが、今のところあまりそういった(利用シーンが増えた・よく見かけるようになった)実感はない。</description></item><item><title>Windows10でPlatformIOが起動しない問題の解消</title><link>https://kouya17.com/posts/36/</link><pubDate>Sat, 21 Nov 2020 17:30:42 +0900</pubDate><guid>https://kouya17.com/posts/36/</guid><description>環境 OS: Windows10 Home 1903 VSCode: version 1.51.1 PlatformIO IDE: version 2.2.1 PlatformIOとは PlafrotmIOとは、組み込みソフト開発向けの、様々なプラットフォーム・アーキテクチャ・フレームワークに対応した開発用ツール。
公式のドキュメントによると、このツールは以下の課題を解消することを目的としている。
特定のMCU/ボード用の開発ソフトウェアをセットアップするためのプロセスが複雑であること。そして、そのソフトウェアがサポートされているOSを搭載しているPCを入手する必要があること。 それぞれのハードウェアプラットフォームはそれぞれ異なるツールチェーン、IDE等を必要とし、開発環境の学習に時間がかかること。 一般的なセンサやアクチュエータの使用方法を示す適切なライブラリやコードサンプルを探す必要があること。 チームメンバと、それぞれが使用しているOSに依らずにプロジェクトを共有する必要があること。 上記の課題はPlatformIOを用いれば、次の手段によって解決される。
platform.iniに対象のボードを設定する。 ボードのリストに基づいて、PlatformIOは必要なツールチェーンをダウンロード・インストールする。 ユーザはコードを開発し、PlatformIOはコードがコンパイルされ、対象のボードに書き込めるように手配する。 要するに、組み込みソフト開発向けの、開発環境整備用ツールらしい。 様々なIDE向けのプラグインを提供しているが、公式が推奨しているIDEはVSCodeとCLion。
今回はM5Stack用のプログラムを作成しようと思って、開発環境周りを少し調べたら、 PlatformIOが良い という記事を見かけたので試してみた。ただし、この記事ではPlafromIOの使い方については特に触れない。
PlatformIOのHome画面がloading状態のままスタックする VSCodeに、以下のPlatformIOプラグインを入れた。
PlatformIOには以下のようなHome画面がある。
インストール直後は、この画面が以下のように loading&amp;hellip; という表示のまま数分経っても変わらなかった。以前他のPCにインストールした時はこのようなことはなかったので、何かよろしくない環境にハマったのだろうと思った。
プラグインをインストールし直しても状況は変わらず 以下の公式コミュニティへの問い合わせを参考に、pip uninstall platformio後に~/.platformio/を削除し、プラグインを再インストールしたが、状況は変わらなかった。
久しぶりに起動したPCで、色々アップデートが走っていたので、諸々のバージョンの整合性等が合ってないのかなと思っていたが、そうではないようだった。
IEを無効化したら解消した 以下の問い合わせを参考に、IEを無効化したら解消した。
IEの無効化は以下のwindowsの機能の有効化または無効化の画面から設定できる。
PlatformIOのHome画面は、IEだと表示できないらしい。なぜ表示ブラウザとしてIEが選択されたのか等はわからないが、とりあえず問題が解消されたので良しとする。
なお、一度表示されるようになってからは、IEの無効化を解除しても、表示されるようになった。
アフィリエイト
みんなのM5Stack入門 リックテレコム (2019/11/8) Amazon Kindle 楽天ブックス 楽天Kobo</description></item><item><title>Spresense VSCode IDEトラブルシューティング(適宜更新予定)</title><link>https://kouya17.com/posts/35/</link><pubDate>Sat, 11 Jul 2020 13:23:52 +0900</pubDate><guid>https://kouya17.com/posts/35/</guid><description>ASMPワーカープログラム作成後、プロジェクトのクリーンに失敗する 環境 OS : macOS Mojave v10.14.6 Spesense SDK : v2.0.1 Spresense VSCode IDE : v1.2.0 現象 公式のマルチコアアプリケーション作成ガイドに沿って、プロジェクトをビルドし、ボードに書き込んだ。
しかし思った通りにアプリケーションが動かなかったため、プロジェクトのクリーンを一度行ったら以下のようなエラーが出た。
&amp;gt; Executing task in folder myproject: .vscode/build.sh clean &amp;lt; usage: grep [-abcDEFGHhIiJLlmnOoqRSsUVvwxZ] [-A num] [-B num] [-C[num]] [-e pattern] [-f file] [--binary-files=value] [--color=when] [--context[=num]] [--directories=action] [--label] [--line-buffered] [--null] [pattern] [file ...] Create .version make[3]: Nothing to be done for `clean&amp;#39;. Makefile:12: /Users/username/spresense/spresense/sdk/apps/.vscode/worker.mk: No such file or directory make[6]: *** No rule to make target `/Users/username/spresense/spresense/sdk/apps/.</description></item><item><title>UIFlowの環境でHuskyLensを使えるようにする</title><link>https://kouya17.com/posts/34/</link><pubDate>Sun, 28 Jun 2020 20:47:32 +0900</pubDate><guid>https://kouya17.com/posts/34/</guid><description>UIFlowとは M5Stack系列がサポートしている開発環境の1つ。主な特徴は以下の通り。
Web上でコーディングから書き込みまでできる。 ビジュアルプログラミング言語Blockly及びMicroPythonをサポートしている。 HuskyLensとは AI処理向けのSoC Kendryte K210 を搭載したAIカメラモジュール。顔認識、オブジェクト追跡、オブジェクト認識、ライン追跡、色認識、タグ（AprilTag）認識などの機能が実装されている。接続インタフェースはUART及びI2C。
MicroPython環境でHuskyLensを使う Pythonで実装されたHuskyLensとの通信用ライブラリは、RaspberryPiでの利用向けに公開されている。
MicroPython上でHuskyLensを使う場合、基本的にはここから必要な処理をコピペして使えば良さそうなのだが、一部 MicroPython(on ESP32) でサポートされていないモジュールを利用している箇所があるため、その部分を書き換える必要がある。ただし、今回はUARTでの通信を前提とする。
環境 UIFlowファームウェア : ver1.4.5.1 HuskyLens用Pythonライブラリ : 2020/06/28時点の最新 書き換えが必要な箇所 UART通信用オブジェクト生成部分 書き換え前
self.huskylensSer = serial.Serial(port=comPort, baudrate=speed) 書き換え後例
self.serial = machine.UART(1, tx=32, rx=33) self.serial.init(9600, bits=8, parity=None, stop=1) MicroPythonではmachineモジュールがもつクラスUARTを使ってUART通信バスにアクセスできる。ピン番号やボーレートは自分の環境に合わせて設定する。
参考 : クラス UART -- 二重シリアル通信バス — MicroPython 1.12 ドキュメント
16進数文字列からバイナリデータへの変換部分 書き換え前
def cmdToBytes(self, cmd): return bytes.fromhex(cmd) 書き換え後例
import ubinascii def cmdToBytes(self, cmd): return ubinascii.</description></item><item><title>赤外線を周期的に(矩形波で)出力するモジュールを作る</title><link>https://kouya17.com/posts/33/</link><pubDate>Sun, 21 Jun 2020 18:17:42 +0900</pubDate><guid>https://kouya17.com/posts/33/</guid><description>現在作成中である射的ゲームの銃ユニット用に、赤外線を周期的に(矩形波で)出力するモジュールを作成する。
電源 まず必要な電圧・電流を調べる 今回使う赤外線LED → ５ｍｍ赤外線ＬＥＤ　９４０ｎｍ　ＯＳＩ５ＦＵ５１１１Ｃ－４０　（５個入）: LED(発光ダイオード) 秋月電子通商-電子部品・ネット通販
赤外線LEDの絶対最大定格
最大順方向電流 If : 100mA 赤外線LEDの電気的特性
順方向電圧 : If=100mAの時 代表値 1.35V、 最大値 1.6V If=100mAとなっているが(なかなか高出力…)、50%の50mAが流れるように設計する。
電源の選定 50mA流しても大丈夫 + 1.35V以上を確保できる電源を選ぶ。
値段等も考えると、1000円でバッテリー+電池保護+電源管理がついてるPowerCが自分にとって使いやすそう。
M5StickCは使わないので、BATピンだけ活用する。
PowerCの給電周りの仕組みについては公式ドキュメントとM5StickCのバッテリー拡張HATをためす – Lang-shipを参考にさせていただいた。
安定化 定電圧をとるために3端子レギュレータをかます。
低飽和型レギュレーター　３Ｖ５００ｍＡ　ＮＪＭ２８８４Ｕ１－０３: 半導体 秋月電子通商-電子部品・ネット通販とかでよさそう。
→ ip3005のデータシートによると2.5V付近まで落ちるみたいなことが書いてあるので、これだとバッテリ残量少ないときダメそう。
秋月で買えるやつでベストそうなのは低ドロップレギュレーター　２．５Ｖ１．５Ａ　ＬＴ１９６３ＡＥＳ８－２．５: 半導体 秋月電子通商-電子部品・ネット通販。これを使う。
だいぶいいやつなので高い…(1個200円)。
赤外線出力 矩形波生成 矩形波（方形波）発生回路の理論的な理解 - 電子工作で覚える！電子回路を参考にさせていただく。
上記サイト中の式
$$ T = 1.386 \times C_1R_4 $$
において、\(C_1 = 0.1 \mu F\)に固定した場合、</description></item><item><title>Ambientを使って、PowerCに接続されているバッテリーの情報を記録する</title><link>https://kouya17.com/posts/32/</link><pubDate>Sat, 06 Jun 2020 22:57:28 +0900</pubDate><guid>https://kouya17.com/posts/32/</guid><description>Ambientとは 本当にすばらしいサービス。
Ambient – IoTデーター可視化サービス
データをテキストでダウンロードできるということで、今回使用させていただくことを決めた。
データーのダウンロード機能をリリースしました – Ambient
PowerCとは M5StickCと接続できるバッテリー充電モジュール。
16340バッテリー(3.7V/700mAh)を2個搭載できる。
USB Type-A端子から、外部へ電源供給(5V/1.5A)ができる。
Ambientを使ってバッテリーの電圧を記録する 今回はAmbientの使い方の勉強を兼ねて、PowerCを使ってバッテリーの情報を記録する。
PowerCは電源管理IC IP5209 が付いており、I2C通信によってバッテリーの情報を確認できる。
IP5209から取得できる情報 PowerCのサンプルスケッチをみると、電圧値と電流(電気量)を取得できるらしい。
スイッチサイエンスの紹介ページには
M5StickCと接続すると、I2C（アドレス0x75）を通じて、電圧、電流、その他の情報を確認できます。
とあり、その他も取得できるとあるので、他にどんな情報を取得できるかIP5209の以下のデータシートを読んでみた。
IP5209-Injoinic.pdf
このデータシートには
The built-in 14bit ADC in IP5209 measures battery voltage and current accurately. ADC data are available on I2C interface. IP5209 has integrated a fuel gauge algorithm, acquiring battery’s state of charge precisely.
とあり、バッテリーの電圧と電流が測定できるとしか書かれていない。
私が見つけたこのデータシートにはレジスタの情報が(私が読んだ範囲では)見つからなかった。
もうちょっと探せば他の形式のデータシートがあるかもしれない。
少し調べてみると、M5Stackで使われている電源IC IP5306 はカスタム品で、出回っているデータシートにはレジスタの情報がないらしい。
以下のようなTweetを見つけたので引用させていただく。</description></item><item><title>サイトをスマホで表示したとき下端にメニューバーが表示されるようにする</title><link>https://kouya17.com/posts/31/</link><pubDate>Sun, 31 May 2020 16:40:28 +0900</pubDate><guid>https://kouya17.com/posts/31/</guid><description>スマホの場合は、画面下端の方が圧倒的にタップしやすく、スマホアプリは大体画面下部に重要なナビゲーションが並んでいる。
本サイトもそれに倣って改修する。
環境 Django v2.1.4 Bootstrap v4.2.1 スマホのみに表示する Bootstrapの機能を使う。
以下のようにHTMLに記載すれば、スマホ(画面サイズ768px未満)の場合のみ表示される。
&amp;lt;div class=&amp;#34;d-block d-sm-none&amp;#34;&amp;gt;スマホのみ表示&amp;lt;/div&amp;gt; iPhone8の画面サイズで表示すると以下のように最下部に&amp;quot;スマホのみ表示&amp;quot;というテキストが表示されるが、
iPadの画面サイズで表示すると&amp;quot;スマホのみ表示&amp;quot;というテキストが表示されなくなる。
参考
表示ユーティリティ～Bootstrap4移行ガイド 下端に固定する CSSで実装する 表示位置を絶対位置で指定する。
以下のようにCSSで指定することで画面下端に要素を固定できる。
/* スマホ用画面下端メニュー */ .footer-menu-bar { position: fixed; /* 要素の位置を固定する */ bottom: 0px; /* 絶対位置を指定する(下0px) */ } ただ、この方法だと他コンテンツと重なって表示されてしまうので他の用途に使う場合は注意。(今回の用途としては重なっても問題なしとしている)
参考
【css】フッターをページ下部に固定する方法【お手軽コピペ】 | Pで作業軽減しましょ 横並びでリスト表示する Bootstrapの機能を使ってリスト表示を実装する。
HTMLを以下のように変更する。
&amp;lt;div class=&amp;#34;d-block d-sm-none&amp;#34;&amp;gt;スマホのみ表示&amp;lt;/div&amp;gt; ↓
&amp;lt;div class=&amp;#34;d-sm-none d-block footer-menu-bar w-100 border-top&amp;#34;&amp;gt; &amp;lt;div class=&amp;#34;row m-2&amp;#34;&amp;gt; &amp;lt;div class=&amp;#34;col&amp;#34;&amp;gt; TOP &amp;lt;/div&amp;gt; &amp;lt;div class=&amp;#34;col&amp;#34;&amp;gt; 検索 &amp;lt;/div&amp;gt; &amp;lt;div class=&amp;#34;col&amp;#34;&amp;gt; タグ &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; 表示は以下のようになる。</description></item><item><title>ロジスティック回帰を使ってYouTubeタイトルから投稿者を推論する</title><link>https://kouya17.com/posts/30/</link><pubDate>Mon, 25 May 2020 21:36:26 +0900</pubDate><guid>https://kouya17.com/posts/30/</guid><description>Pythonでロジスティック回帰を使って、VTuberの動画タイトルから投稿者を推論してみる。
ただ、投稿者を推論するといっても、二値分類なので、投稿者が A か B かを分類するのみ。
使用するライブラリ 今回は以下のライブラリを使用する。
scikit-learn : テキストのベクトル化、ロジスティック回帰 MeCab : 日本語形態素解析 データセット 適当に選んだRan Channel / 日ノ隈らん 【あにまーれ】さんとKuzuha Channelさんの動画タイトルをデータセットとして使う。
動画タイトルはYouTube Data APIを使って取得した。
1日のAPIリクエスト上限に引っかかって、すべては取得できなかったが、日ノ隈らんさんのタイトルは475個、葛葉さんのタイトルは456個取得できた。
タイトル取得に使用したプログラムは以下に置いてある。
また、タイトル中の【】内には大体投稿者名が入っている。
これを使うと簡単に分類できてしまうと思うので、今回は削除する。
(まあ、【】内でなくても、投稿者名が入っていることがあるので結局、というところもあるが…)
プログラムと推論結果 今回推論に使用したプログラムは以下に置いてある。
テストデータは直近(このプログラムを作成中)に投稿された以下のタイトルを使う。
Ice breaker / ChroNoiR けんきさんとあびつんさんとランクいってくる！！！ １行目が葛葉さんの動画タイトルで、2行目が日ノ隈らんさんの動画タイトルになっている。
推論結果 推論結果は以下のようになった。
[0.45 0.69] なお、1個目が1行目の結果で、2個目が2行目の結果になる。
また、この結果が 1.0 に近いほど日ノ隈らんさんのタイトル、 0 に近いほど葛葉さんのタイトル&amp;quot;っぽい&amp;quot;と判断できる。
推論結果の分析 どういった理由で今回の推論結果が出たのか分析する。
ロジスティック回帰では各特徴量の係数(重み)と切片(バイアス)を見ることができる。 今回の特徴量はタイトルに含まれる単語なので、各単語について係数(重み)を見てみる。
まず1行目のタイトルの各単語の結果は以下の通り。
以下でwordとtf_idfというラベルで出力しているものが、それぞれ単語名及びtf-idfという手法によって得られた、各単語の重要度になる。
ただ、このデータに関してはすべての単語について、学習元データに含まれていなかったため、this word does not existと表示されている。
つまり、上記推論結果で0.45と出ているが、これは切片(バイアス)成分によるものである。
title_index: 0 word: Ice tf_idf: this word dose not exist word: breaker tf_idf: this word dose not exist word: / tf_idf: this word dose not exist word: ChroNoiR tf_idf: this word dose not exist 次に２行目のタイトルの各単語の結果は以下の通り。</description></item><item><title>全国規模の感染シミュレーションと結果の可視化をしてみる</title><link>https://kouya17.com/posts/29/</link><pubDate>Wed, 20 May 2020 22:37:30 +0900</pubDate><guid>https://kouya17.com/posts/29/</guid><description>先日、都道府県間通勤・通学を考慮した感染症流行のシミュレーションを行った。
この時は関東のみのシミュレーションだったが、今回は全国規模の感染シミュレーションを実施してみる。
なお、この記事ではモデルに関する説明は特に記載しないため、モデルの詳細は以下過去の記事を参照していただきたい。
感染症数理モデルについて触りの部分だけ学ぶ | kouya17.com
都道府県間通勤・通学を考慮したパンデミックシミュレーションもどき | kouya17.com
※本シミュレーションや上記リンク先のシミュレーションは、実測値等には全く基づいていないため、完全に空想上のシミュレーションになります。
本シミュレーションの結果が実情を反映していたり、今後の予測を示していたりはしません。
利用するデータ シミュレーションを全国規模に拡張するには各都道府県の人口のデータと、各都道府県間の通勤・通学者人数のデータが必要になる。
これらのデータは総務省統計局「平成27年国勢調査結果」1を利用した。
パラメータ、初期状態について SEIRモデルに置ける各パラメータは今回以下のように設定する。
基本再生産数 \(R_0\) : 2.0
平均潜伏期間 \(l\) : 5日
平均発症期間 \(i\) : 14日
初期状態はAM0時に、東京都に1人の感染者がいる状態とする。
今回は都道府県間通勤・通学について
通勤・通学者なし 通勤・通学者あり、通勤・通学者数は特に加工しない 通勤・通学者あり、通勤・通学者数は元データの20%にする 通勤・通学者あり、通勤・通学者数は元データの0.01%にする の4パターン、計算を実施して、通勤・通学者数が与える影響を調べてみる。
また、前回のシミュレーションでは休日の概念を設けていなかった。
働きっぱなしは可哀想なので、土日は休みにし、通勤・通学はしないものとする。
ただし、祝日や土日以外の長期休暇は考慮しない。
結果及び可視化 今回はデータが少し多くなってくるので、可視化の方法も工夫する必要がある。
japanmapという、日本地図を都道府県別に色分けできるライブラリがあるようなので、これを利用させていただく。
発症者比率の可視化 各都道府県について、人口に占める発症者の比率(\(\frac{I}{S+E+I+R}\))を可視化する。
発症者の比率を赤色の濃淡で表したアニメーションを以下に示す。
通勤・通学者なしの場合 都道府県の人の移動が発生しないため、東京でしか流行しない。
通勤・通学者ありの場合 日本全国に感染が広がる。
ただし、感染のピークは東京から遠い都道府県ほど遅くなる。
通勤・通学者あり、ただし人数20%の場合 人数を絞る前の結果とほぼ変わらないように見える。
通勤・通学者あり、ただし人数0.01%の場合 人数を思いっきり絞ってみると、人数を絞る前と比べて感染が広がるスピードが遅いように見える。
1000日目時点での累計感染者数の可視化 それぞれの条件で1000日目時点での累計感染者数(\(E+I+R\))をグラフ化する。
横軸を各都道府県、縦軸を累計感染者数にした棒グラフを以下に示す。
通勤・通学における各条件の結果をそれぞれ色を変えて並べている。
累計感染者数に関しては、通勤・通学者数を絞っても変化はない。
ソースコード 今回使用したコードは以下に置いてある。
ただし、全然整理できていない…。
統計局ホームページ/平成27年国勢調査/調査の結果&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>LogisticRegression().fit()のConvergenceWarningを解消する</title><link>https://kouya17.com/posts/27/</link><pubDate>Sat, 16 May 2020 14:45:12 +0900</pubDate><guid>https://kouya17.com/posts/27/</guid><description>環境 Anacondaの環境情報
&amp;gt;conda info conda version : 4.8.2 conda-build version : 3.18.11 python version : 3.7.6.final.0 platform : win-64 現象 pythonでLogisticRegression.fit()を実行したところ、以下のワーニングが出た。
ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. 実行時のjupyter notebookは以下のような感じ。
一応正解率は出せているらしい。
解決策の1つ モデルが収束していないそうなので、max_iterを明示的に設定して、反復回数を増やす。
max_iterの初期値は100だが、上の例では1000に設定している。
今回はこれで警告が出なくなったので、よしとする。
参考 見て試してわかる機械学習アルゴリズムの仕組み 機械学習図鑑posted with ヨメレバ秋庭 伸也/杉山 阿聖 翔泳社 2019年04月17日 楽天ブックスで購入楽天koboで購入Amazonで購入Kindleで購入</description></item><item><title>Vue.jsでローディング表示を実装する</title><link>https://kouya17.com/posts/26/</link><pubDate>Wed, 06 May 2020 19:45:05 +0900</pubDate><guid>https://kouya17.com/posts/26/</guid><description>ローディング表示モジュールの調査 vue.jsでローディング表示を行う場合、ローディング表示に関係するモジュールについて調査する。 出来れば、
実装が簡単 よく見かけるような見た目(独自性がない) メンテされている という条件にあてはまるものを採用したい。
調べた結果を以下の表にまとめた。
名前 種類 GitHub Star 最終更新(2020/05/06時点) 備考 vue-loading Vueコンポーネント 2018/12/22 紹介している記事が多い vue-spinner Vueコンポーネント 2017/10/07 Vue2.0をサポートしていない Single Element CSS Spinners css 2019/12/09 よく見かける見た目 vue-loading-overlay Vueコンポーネント 2020/04/18 全画面に表示するらしい vue-wait Vueコンポーネント 2019/10/08 だいぶ機能がリッチそう ajaxload.info gif - - よく見かける見た目 調べて出てきた候補は以上。</description></item><item><title>都道府県間通勤・通学を考慮したパンデミックシミュレーションもどき</title><link>https://kouya17.com/posts/25/</link><pubDate>Tue, 05 May 2020 21:35:17 +0900</pubDate><guid>https://kouya17.com/posts/25/</guid><description>先日、SEIRモデルを用いた感染症流行のシミュレーションを行った。
このプログラムを少し拡張して、都道府県間通勤・通学を考慮したパンデミックシミュレーションもどきを行う。
※本シミュレーションや上記リンク先のシミュレーションは、実測値等には全く基づいていないため、完全に空想上のシミュレーションになります。
本シミュレーションの結果が実情を反映していたり、今後の予測を示していたりはしません。
都道府県間通勤・通学をシミュレーションに組み込む 都道府県間通勤・通学の影響をシミュレーションに組み込むため、総務省統計局「平成27年国勢調査結果」1を参考にする。
今回は東京都・群馬県・栃木県・茨城県・埼玉県・千葉県・神奈川県間の自宅外就業者数及び通学者数データを扱う。
モデルへの組み込み方としては、移動用のグループを形成し、移動用のグループは移動先のデータと感染率を共有するという方法を採る。
なお、今回はシミュレーション内時刻が8:00の時に通勤・通学者が一斉に(移動時間0で)移動し、シミュレーション内時刻が17:00の時に通勤・通学者が一斉に(移動時間0で)帰宅すると仮定する。
東京都と千葉県間の人口移動を例にして説明する。
まず、初期状態として以下のような状態になっているとする。
値はすべて適当だが、東京都に10人感染者(\(I\))がいるとする。
この後、時間を進行させる。
シミュレーション内の時刻が8:00になった時、人口移動用のグループを分割する。
移動用グループの総人数は総務省統計局「平成27年国勢調査結果」1における自宅外就業者数及び通学者数に従う。
\(S:E:I:R\)比は移動元の都道府県データの比と等しくする。
ここで、移動用グループは移動先の都道府県のデータと、感染率\(\frac{R_0I}{iN}\)を共有する。
感染率を共有させた状態で、時間を進行させる。
なお、感染率を共有する関係で、非感染者から潜伏感染者への遷移に関する項については時間刻みに対して1次精度になってしまう。
シミュレーション内の時刻が17:00になった時、人口移動用のグループを移動元のデータに合流させる。
上の図中の値もすべて適当だが、以上のような形で、都道府県間の感染伝播をシミュレートする。
計算する 人口データについて 人口データは各自治体のページを参考に、以下の値を使う。
都道府県 人口[人] 備考 東京都 13951635 2020年1月1日時点推計2 茨城県 2866325 2020年1月1日時点推計3 栃木県 1942313 2019年10月1日時点推計4 群馬県 1938053 2019年10月1日時点推計5 埼玉県 7341794 2020年4月1日時点推計6 千葉県 6280344 2020年4月1日時点推計7 神奈川県 9204965 2020年4月1日時点推計8 各都道府県毎の通勤・通学による流入・流出人口は総務省統計局「平成27年国勢調査結果」1より、以下の値(単位は[人])を使う。縦が流出元で横が流出先。</description></item><item><title>感染症数理モデルについて触りの部分だけ学ぶ</title><link>https://kouya17.com/posts/24/</link><pubDate>Sat, 02 May 2020 22:58:59 +0900</pubDate><guid>https://kouya17.com/posts/24/</guid><description>目的 特にしっかりした目的はない。
なんとなく数値シミュレーションについて学び直したくなったため、現在身近にある現象を題材にして学ぶ。
ネットの情報を参考に、適当にシミュレーションを走らせてみる。
※本シミュレーションは、実測値等には全く基づいていないため、完全に空想上のシミュレーションになります。
本シミュレーションの結果が実情を反映していたり、今後の予測を示していたりはしません。
今回扱うシミュレーションモデル SEIRモデルを扱う。
SEIRモデルとは SEIRモデル(エスイーアイアールモデル)とは感染症流行の数理モデルである。
モデルは
感染症に対して免疫を持たない者(Susceptible) 感染症が潜伏期間中の者(Exposed) 発症者(Infectious) 感染症から回復し免疫を獲得した者(Recovered) から構成され、その頭文字をとってSEIRモデルと呼ばれる。
(Wikipediaより)
モデル式は以下のような式で表される。
$$ \begin{aligned} \frac{dS}{dt} &amp;amp;= m(N - S) - bSI &amp;amp;(1) \cr \frac{dE}{dt} &amp;amp;= bSI - (m + a)E &amp;amp;(2) \cr \frac{dI}{dt} &amp;amp;= aE - (m + g)I &amp;amp;(3) \cr \frac{dR}{dt} &amp;amp;= gI - mR &amp;amp;(4) \end{aligned} $$
ここで\(t\)は時間、\(m\)は出生率及び死亡率、\(a\)は感染症の発生率、\(b\)は感染症への感染率、\(g\)は感染症からの回復率を表す。
また\(N\)は全人口を示し、
$$N \equiv S + E + I + R　(5)$$</description></item><item><title>Chart.jsで時刻データを扱う</title><link>https://kouya17.com/posts/23/</link><pubDate>Sun, 26 Apr 2020 13:57:32 +0900</pubDate><guid>https://kouya17.com/posts/23/</guid><description>Chart.js公式のドキュメントを参考にして、Chart.jsで時刻データを扱う場合の描画オプションについて確認する。
入力データ 入力データはx軸データをy軸データを各点それぞれ指定する。
今回はx軸データに時刻データを使う。
例としては、以下のように指定すればOK。
data: [{ x: &amp;#39;1995-12-17T00:00:00&amp;#39;, y: 1 }, { x: &amp;#39;1995-12-18T00:00:00&amp;#39;, y: 10 }, { x: &amp;#39;1995-12-21T00:00:00&amp;#39;, y: 20 }, { x: &amp;#39;1995-12-25T12:00:00&amp;#39;, y: 30 }, { x: &amp;#39;1996-01-01T00:00:00&amp;#39;, y: 40 }] データフォーマット 時刻スケールのデータを使う場合、フォーマットはMoment.jsが扱えるフォーマットであればどういった形式でも良い。詳細はMoment.jsのドキュメント参照。
描画オプション 時刻目盛りを扱う場合、以下のオプションを設定できる。
adapters.date 外部の時刻ライブラリ(つまりMoment.js以外)を使うためのオプション。
distribution データのプロット方法。
指定できる値はlinearまたはseriesのいずれか。
初期値はlinear。
linearはデータ描画間隔が時間間隔に応じて変化する。
seriesはデータ描画間隔がすべて同じになる。
以下に同一データを用いてdistribution設定のみ変えた場合の描画例を示す。
linearを指定した場合
var ctx = document.getElementById('distribution_linear').getContext('2d'); var chart = new Chart(ctx, { // The type of chart we want to create type: 'line', // The data for our dataset data: { datasets: [{ label: 'linear', backgroundColor: 'rgb(255, 99, 132)', borderColor: 'rgb(255, 99, 132)', fill: false, data: [{ x: '1995-12-17T00:00:00', y: 1 }, { x: '1995-12-18T00:00:00', y: 10 }, { x: '1995-12-21T00:00:00', y: 20 }, { x: '1995-12-25T12:00:00', y: 30 }, { x: '1996-01-01T00:00:00', y: 40 }] }] }, // Configuration options go here options: { scales: { xAxes: [{ type: 'time', //!</description></item><item><title>Vue.jsを使ったSPAにおいてTwitter等でのリンク表示がいい感じになるようにする</title><link>https://kouya17.com/posts/22/</link><pubDate>Sat, 25 Apr 2020 10:47:59 +0900</pubDate><guid>https://kouya17.com/posts/22/</guid><description>Vue.jsでSPAを構成した際、TwitterなどのOGPが上手く表示されなかったので、対処した。
SPAとは?→シングルページアプリケーション この記事で扱っているSPAとは、シングルページアプリケーションを指す。
SPAについては少し以下の記事で触れたが、詳細は他のサイトの方が詳しいと思う。
OGPとは?→Webページのリンクを表示する仕組み OGPとはOpen Graph Protocolの略で、TwitterとかFacebookとかでリンクがいい感じに表示されるアレに使われているプロトコルのこと。
↓これ。
Chart.jsで横軸が日付のグラフを作成する https://t.co/evu4HqqQ4f @aoki_kouyaさんから
&amp;mdash; 青木晃也 (@aoki_kouya) April 4, 2020 webサイトをいちから作る際、OGP対応していないと、Twitterとかでリンクを貼り付けても画像や説明文は表示されない。
そこらへんはTwitter側でなんとかしてくれるわけではなく、サイト作成者側が適切にOGP設定を行う必要がある。
SPAの場合はOGP設定にひと工夫必要 OGPは、具体的にはHTMLにmetaタグを設定することで対応できる。
例えば、以下のようなmetaタグを&amp;lt;head&amp;gt;要素内に設定する。
上記は私が作成したサイトの実装例になるが、他のサイトを調べると、head要素にprefix属性を適切に設定する必要があるようだ。
私は設定していなかったが、とりあえずTwitterあたりでは問題なく動いているように見える。
まあ、正しい対応ではないと思う。
このOGP設定だが、SPAの場合はひと工夫必要になる。
理由は、OGPによってリンクを作成する際、JavaScriptが動かないケースがあるため。
OGPによってリンクを表示する際、クローラーと呼ばれるプログラムによって、リンク先のページをスキャンする。
スキャン結果に従ってリンクを表示するのだが、TwitterやFacebookのクローラーはJavaScriptを実行してくれないらしい。
よって、JavaScriptによってmetaタグを生成しても、リンクには反映されない。
SPAでOGP設定を適切に行う場合はここら辺の対処が必要。
今回は特定条件時のみ静的ページを返すように実装 今回の結論、解決策になる。
調べてみると、(Vue.jsを用いた)SPAでのOGP設定については色々対応策があった。
いくつか参考にさせていただいたページを挙げておく。
上記のページの方法はページ内にも書かれている通り、全てのページで同じ内容のOGPになってしまう。
今回はそれぞれのページで異なるOGPを設定したいため、同じ方法はとらなかった。
上記のページは詳しく書いてあり、参考になりそうだったが、Firebaseを使ってない、使ったことがないため今回は同じ方法をとれなかった。
最終的には以下のページの方法を真似させていただいた。
上のページの方法は、TwitterやFacebookなど特定のクローラーに対してのみOGP対応用のテンプレートを返すというものになっている。
アプローチとしては一番最初に挙げたページと同じだが、こちらはクローラー検出用にControllerを作成して、テンプレートの出し分けをしている。
比較的容易に実装でき、ページごとに異なるOGP設定ができそうだったため、こちらの方法を採用した。
この方法の課題としては、OGP対応用のテンプレートを返す相手をピンポイントで指定する必要があるため、明示的に指定したクローラー以外に対しては対応できない点がある。
また、OGP対応用のControllerを新規に作成する必要があり、Controllerが単純に1個増える。
なお、各サービスのクローラーの情報は以下のサイトを参考にさせていただいた。
雑記 Twitterでのリンク表示内容確認には以下の公式のサイトが便利。
これからはじめるVue.js実践入門 山田 祥寛 Amazon Kindle 楽天 楽天Kobo</description></item><item><title>Chart.jsで横軸が日付のグラフを作成する</title><link>https://kouya17.com/posts/21/</link><pubDate>Sun, 05 Apr 2020 00:39:30 +0900</pubDate><guid>https://kouya17.com/posts/21/</guid><description>webサイト上で横軸が日付情報のグラフを作りたかったのでChart.js周りで色々調べた。
横軸が単純な数値じゃない場合は面倒くさくなるかなと思っていたが、Chart.jsが非常に使いやすく、すんなりできた。
最終的に作ったもの 画像で貼ろうと思ったが、結構サイズが大きいのでリンクで貼り付けておく。
Chart.jsとは グラフ類を描画するためのJavaScriptライブラリ。
HTMLの&amp;lt;canvas&amp;gt;要素に2Dグラフィックを描画できる。
ライセンスはMIT。
Chat.jsのインストールと使い方 基本的な使い方を公式に従って確認してみる。
インストール 今回はとりあえず手軽そうなCDNを使う。
よってHTMLに以下のタグを追加する。
&amp;lt;script src=&amp;#34;https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.3/Chart.bundle.min.js&amp;#34;&amp;gt;&amp;lt;/script&amp;gt; なお、ここで Chart.bundle.min.js を選択したのは、今回日付データを扱うため。
Chart.min.js の方は日付データを扱うためのライブラリMoment.jsが含まれていない。
使い方 HTML側はグラフ描画用の&amp;lt;canvas&amp;gt;タグを作成する。
先ほどのCDNの参照及び今回作成するJavaScriptの読み込みを合わせて、HTML側のソースは以下のようになる。
index.html JavaScript側は公式のサンプルを参考にして以下のようにする。
main.js これら2つのファイルを適当なところに保存して、index.htmlをブラウザで開く。
以下のようなグラフがブラウザ上に表示される。
横軸が日付情報のデータをプロットする サンプルが動いたので、今回扱いたい、横軸が日付情報のデータをプロットする。
HTML側は特に変更せず、JavaScript側を以下のように変更する。
main.js 参考ページ
main.jsを上記のように変更し、index.htmlをブラウザで開くと以下のようなグラフが描画される。
横軸が日付になっており、横軸の間隔もデータに合わせていい感じになっている。
まとめ Chart.jsを使って横軸が日付情報のデータをプロットできた。
色々と描画オプションがあるようなので、それぞれ変更させるとどのようにグラフが変化するか、また今度まとめたい。
確かな力が身につくJavaScript「超」入門 第2版 狩野 祐東 Amazon Kindle 楽天 楽天Kobo</description></item><item><title>Laravel+Vue.jsでSPA(シングルページアプリケーション)を作成した</title><link>https://kouya17.com/posts/20/</link><pubDate>Sun, 15 Mar 2020 22:59:47 +0900</pubDate><guid>https://kouya17.com/posts/20/</guid><description>今回作成したサイト 今回作成したサイトは以下のサイト。
ClubPage
私が所属している電子工作サークルのホームページを作った。
このブログからcssを流用しているので、見た目がだいぶ似ている。
構成 Laravel Vue.js バックエンドはLaravel、フロントエンドはVue.jsを使っている。
シングルページアプリケーションとは 今回はシングルページアプリケーション(SPA)という構成を採用した。
シングルページアプリケーションとは
シングルページアプリケーション（英: single-page application、SPA）とは、単一のWebページのみから構成することで、デスクトップアプリケーションのようなユーザ体験を提供するWebアプリケーションまたはWebサイトである。必要なコード（HTML、JavaScript、CSS）は最初にまとめて読み込むか[1]、ユーザの操作などに応じて動的にサーバと通信し、必要なものだけ読み込みを行う。 Wikipediaより
というものらしい。
これまでRuby on RailsやDjangoとかを使ってwebサイトを作ってみてきたが、どれも各ページ毎に、サーバ側でHTMLの生成を行うマルチページアプリケーションだった。
今回はLaravelでwebAPIを作成し、そのAPIから取得した情報を、Vue.jsを使ってブラウザ上に表示するような構成にした。
なぜSPAにしたのか 理由は1つで、今回参考にしたサイトがSPAを採用していたから。
今回参考にしたサイトは以下のサイト。
このサイトにあるチュートリアル、書かれている情報量がかなり多くて、これだけの情報をタダで得られるのが信じられない。
今回初めてLaravelとVue.jsを触ったが、このサイトのおかげで色々と基礎を学ぶことができた。
今回学んだこと、感じたこと インタフェース部とエンジン部の分離はやはり重要 今回の構成はAPI(Laravel)と表示部(Vue.js)を分離している。
たとえばhttps://home.lchika.club/api/tagsにアクセスすれば、Laravelの機能によって、タグの一覧をJSON形式で取得できる。
こういったAPIから取得した情報をVue.jsで処理して、ブラウザにレンダリングする。
このように分離することで、UIを変えたいときは、基本的にVue.jsの部分のみを修正するだけですむ。
Laravelの処理部は一切手を加える必要がない。
まあ、一般的なwebフレームワークを使っていれば、実装の分離は適切に行われているが、ある一部分のフレームワークをごっそり切り替えることはできない。
今回の構成では、例えばフロントエンドにReactを使いたければ、Laravel部分はそのままで、フロントエンド部分をごっそり切り替えられる(と思うけど、Laravel-mixとかの関係で難しいのかもしれない&amp;hellip;)。
このように、インタフェースとエンジン部の分離で得られる効果はやはりでかいなと思った。
ローディング表示がないとwebサイトの質が落ちる これは個人的な印象かもしれないが、ローディング画面の表示はユーザにとって(ある程度)重要だなと思った。
現状ではローディング表示は実装していないため、読み込み直後は一部のコンテンツが表示されていないし、一部のコンテンツについては結構な時間表示されないことがある。
これはサイト利用者側からすると、違和感を覚える要因になり、満足度の低下につながると思った。
YouTubeとかもローディング中はコンテンツ表示部をグレーで表示したりしている。
最初はこれ意味あるのか?と思っていたが、今回自分でJavaScriptを多用したサイトを構築してみて、こういった対応も重要だなと思った。
WYSIWYGエディタの選定は毎回の課題 今回はブログのようなサイトを構築したので、フォームから記事を投稿できる必要があった。
さすがにHTML直打ちで入力はできないし、リッチテキストエディタを採用する必要があった。
毎回どのリッチテキストエディタを使うか苦労するが、今回も例に漏れず苦労した。
とりあえず今のところはCKEditorを使っている。
ただ、まだ課題があって、iframeの挿入が上手くいっていない。
いつも大体、リッチテキストエディタまわりは
画像の挿入 画像の整形 外部コンテンツの挿入 目次の作成 あたりで詰まっている。
今回はまだ目次の作成について考えていないので、目次が必要になった時に多分また詰まる。
SPAのパフォーマンス向上はバンドルサイズの削減こそ正義 今回作成したサイトのパフォーマンス向上については以下の記事に書いた。
最初特に何も考えずにサイトを作っていたら、パフォーマンスが崩壊した。
今もパフォーマンスが良いとは言えないが、バンドルサイズを削減することで、パフォーマンスが向上した。
こういったパフォーマンス向上のための施策は必要不可欠だなと感じた。
まとめ 今回はLaravelとVue.jsを使ってサイトを作った。</description></item><item><title>Laravel+Vue.jsでLighthouseのスコアを0点から97点にした(バンドルサイズ削減)</title><link>https://kouya17.com/posts/19/</link><pubDate>Sun, 08 Mar 2020 17:05:20 +0900</pubDate><guid>https://kouya17.com/posts/19/</guid><description>Laravel+Vue.jsで作成していたwebサイトの応答速度が激遅だったので、対策を実施した。
環境 Laravel 6.14.0 Vue.js 2.6.11 対策前はページロードに20秒かかっていた 特に何も考えずにwebサイトを作っていたら、トップページのロードに20秒ほどかかっていた。 これではさすがにwebサイトとして成立しない。
Googleが提供しているwebページ分析ツールLighthouseを使ってパフォーマンスの測定を行ったところ、以下のような結果になった。
堂々の0点である。提示されている対応策の詳細を見ると、以下のように書かれていた。
どうやらバンドル後のapp.jsのサイズが約4MBと、超巨大になっているらしい。対応策としてapp.jsのサイズ削減を行った。
対策1：本番用ビルド設定を適用する バンドルサイズ 3.86MB(app.js) → 1.59MB(app.js)
解説 そもそもビルド設定が間違っていた…。ビルドコマンドとして
npm run dev を実行していたが、これは開発用のビルドコマンドらしい。
本番用のビルドコマンド
npm run prod を実行したところ、バンドルサイズは3.86MB→1.59MBと、約40%になった。
Lighthouseの結果 0点→10点に上昇した。
対策2：gzipで圧縮する バンドルサイズ 1.59MB(app.js) → 395KB(app.js.gz)
解説 以下のページを参考に、gzipでapp.jsを圧縮した。
そのままコピペで適用できた。
Lighthouseの結果 10点→63点に上昇した。
対策3：ページごとにJSファイルを分割する バンドルサイズ 395KB(app.js.gz) → 243KB(app.js.gz)
解説 以下のページを参考に、ページごとにJSファイルを分割した。
こちらも特に詰まることはなく、ほぼコピペで実装できた。
この対策を実装したことで、ビルド後のJSファイルが以下のように、分割されて生成されるようになった。
Lighthouseの結果 63点→87点に上昇した。
対策4：bootstrap-vueの選択的インポート バンドルサイズ 243KB(app.js.gz) → 129KB(app.js.gz)
解説 対策1～3で、あらかた大きなところは対策できたと思ったので、より細かいところを対応するため、webpack-bundle-analyzerを使ってバンドル結果の分析をした。
webpack-bundle-analyzerの出力結果は以下のようになった。
この結果から
bootstrap-vue ckeditor lodash あたりのサイズが大きいらしいことが分かった。</description></item><item><title>カメラ画像から対象物の向きを検出する(OpenCV)</title><link>https://kouya17.com/posts/18/</link><pubDate>Mon, 24 Feb 2020 14:32:15 +0900</pubDate><guid>https://kouya17.com/posts/18/</guid><description>やりたいこと 俯瞰で見ているUSBカメラの画像を使って、自律的に動いているロボットの向きを検出する。
また、カメラ画像における指定の座標と、ロボットがなす角を算出する。
動作環境 ターゲットボード JetsonNano OS Ubuntu 18.04.4 LTS 使用ライブラリ Python版 OpenCV 4.1.1 処理結果 今回実装した処理の結果は、画像で示すと以下のような感じになる。
方法 今回実装した方法は以下の通り。
カメラ画像をHSV形式に変換 カメラ画像のうち指定のHSV範囲に従ってマスク画像を作成 マスク画像から輪郭を抽出し、1番面積の大きい領域のみ残したマスク画像を作成 カメラ画像とマスク画像の共通領域を抽出 4.で作成した画像から黒色領域を抽出 5.で抽出した黒色領域のうち、1番面積の大きい領域の重心p1、2番目に面積の大きい領域の重心p2を計算 p2からp1へのベクトルと、p1とp2の中点から指定の座標へのベクトルのなす角を計算 処理順に従って中間生成画像を並べると以下のような感じになる。
元画像
手順3.で作成したマスク画像
手順5.で抽出した黒色領域
手順6.で算出した各黒色領域の重心
処理結果
ソースコード 一部自作モジュールもimportしているのでコピペでは動きません。
呼び出し方は例えば以下のように。
処理時間や実行時のリソース使用量など 処理時間 今回の一連の処理呼び出しにどれだけ時間がかかっているか計測したところ、以下の通りだった。
elapsed_time:0.49050045013427734[sec] elapsed_time:0.500530481338501[sec] elapsed_time:0.49204468727111816[sec] だいたい0.5秒ほどかかっている。このままだとリアルタイム性が求められる場合は使えない。
リソース使用量 Jetsonのモニタツールjtopの出力は以下のような感じ。
プログラム稼働中
GPUが稼働していない。GPUを使うようにすれば高速化できるのだろうか(全然そこらへんは調べていない)。
なぜこの処理を実装したか 全体を俯瞰できるカメラの情報を使って、ロボットを指定位置に誘導したかった。
カメラ側がロボットに指定の位置に対する角度・距離情報を伝え、ロボット側でその情報を使って、モータ値を計算する。
設計の経緯や苦労したところなど 目印をどう設計するか 向き検出のための目印をどう設計するかが、検出精度や処理速度に最も影響を与える部分だと思う。
今回は黄色の画用紙の上に、面積の異なる黒色の画用紙を2つ(前方に面積が大きいもの、後方に面積の小さいもの)つけて目印にした。
単純だが、思っていたより認識出来ていたと思う。しっかりしたベンチマークはとっていないが。
最初はQRコードの活用を考えていた。OpenCVにはQRコード認識処理が実装されており、向きも検出出来そうだったので。今後複数台ロボットを同時に動かすことを考えても、QRコードだったら文字列情報を付加できるので、各ロボットの識別に使えそうとも思っていた。
ただ、ある程度QRコードが大きく写っていないと認識できなかった。今回はカメラと認識対象ロボットの距離が遠い時でも認識できるようにしたかったので、QRコード案はやめた。</description></item><item><title>VTuberの簡易ランキングサイトを作った</title><link>https://kouya17.com/posts/17/</link><pubDate>Fri, 17 Jan 2020 23:02:41 +0900</pubDate><guid>https://kouya17.com/posts/17/</guid><description>概要 適当に抽出したVTuberについて、チャンネル登録者数増加数のランキングサイトを作成した。
1日1回、午前5時30分ごろ更新。
構成 Golang + Gin + Bootstrap
雑記 Golangを使ったwebアプリを初めて作成した。Golangは未使用の変数、importがデフォルトでコンパイルエラーになるのが印象的だった。ルールが厳格な印象。
YouTubeの現在の仕様で、チャンネル登録者数が多いほど、刻み幅が大きくなる。その刻み幅分を超えないと、増加量が0になってしまうので、1日単位だと正確なランキングにならない。
今回のサイトを作るにあたって、既存サイトを調べてみた。色々凝っているサイトが多数あった。やはりここら辺(YouTube周り)は色んなビジネスが成り立っているんだろうなと思った。
改訂2版 みんなのGo言語 技術評論社 Amazon Kindle 楽天 楽天Kobo</description></item><item><title>今年(2019年)作ったものを振り返る</title><link>https://kouya17.com/posts/16/</link><pubDate>Sat, 28 Dec 2019 22:35:40 +0900</pubDate><guid>https://kouya17.com/posts/16/</guid><description>今年もあっという間だったがもう終わるらしい。 この1年で作った物の振り返りをしたいと思う。
今年作ったもの 1月～2月　ロボサッカー用ロボ作成 2月終わりごろにロボカップジュニア同等のルールのロボコンに出る機会があり、プログラムの一部作成を担当した。
ロボの構成は大体以下の通り。
機体はLEGOのマインドストーム 赤外線センサでボールの位置を検出 ラズパイを使って、カメラからの画像を解析しゴール位置を検出 3人でチームを組んでおり、進捗管理ツールとしてtrello、情報共有ツールとしてslack、ソースコード管理ツールとしてGitHubを使った。
trello、slackあたりはあまり活用できなかったが、GitHubはやはり便利だなと思った。
できたもの マインドストーム側プログラム(GitHub) ラズパイ側プログラム(GitHub) 3月～7月　イライラ棒作成 私が所属している電子工作サークル「メカトロ同行会エルチカ」の作成物として、イライラ棒ゲームを作成した。基本的にプログラム部分を担当した。上の写真はNT名古屋出展時のもの。
この作品では以下のように色んな機器の通信を試してみた。
ArduinoとPCの通信(USBシリアル通信) PCとラズパイの通信(HTTP) PCとESP32の通信(HTTP) ラズパイとwebアプリの通信(HTTP) 色んな機器を連携させる作品を初めて作ってみたが、どれか一つが動かないと全部ダメになる仕組みに一部なってしまった。
できたもの イライラ棒wikiページ スコア表示用ソフト(GitHub) 結果プリント用ラズパイプログラム(GitHub) スタートモジュール用プログラム(GitHub) ゴールモジュール用プログラム(GitHub) イライラ棒結果ランキングページ 4月　github.ioページ公開 github.ioを使ってwebページを公開した。なお、現在更新はしていない。
できたもの github.ioページ 8月　KiCad用ツール作成 KiCadを使う機会が多くなってきたので基板発注用の簡単なツール(Windowsアプリ)を作ってみた。
こちらの記事で少し内容について紹介している。
できたもの KiCadHelper(GitHub) 8月～　射的ゲーム作成 赤外線を使った射的ゲームを現在も作成中。同じようなものを以前一度作成していて、今回は複数人プレイに対応させる予定。
できたもの 鋭意諸々作成中 今年を振り返ってみて 今年は電子工作に割ける時間が結構多かった。ただこうして振り返ってみると、かけた時間に比べ、出来上がった物の量がいまいち物足りないな…と思う。</description></item><item><title>マトリクスLEDドライバを使ってフルカラーLEDを制御してみる</title><link>https://kouya17.com/posts/15/</link><pubDate>Sun, 08 Dec 2019 21:37:24 +0900</pubDate><guid>https://kouya17.com/posts/15/</guid><description>今回の経緯 フルカラーLEDを最大5個制御したい！ 最初NeoPixel(マイコン内蔵LED)を使う予定だったが、いろいろ試した結果、今回の条件では使えなさそうという結論になった。 マイコン内蔵ではない、通常のフルカラーLEDを使うことにする。 ただ、使用できるピン数は限られているため、マイコンボードからはI2CやSPI等で制御する形にしたい。 フルカラーLEDドライバを探す。 スイッチサイエンスで探したらよさそうなのがあったけど値が張る。 秋月でさがしたらこんなのがあった。 安い。もともとはドットマトリクス制御用らしいが、フルカラーLED制御にも転用できそう。 準備物 部品名 購入先 ESPr Developer 32 スイッチサイエンス 16×8LEDマトリクスドライバーモジュール(HT16K33) 秋月電子通商 RGBフルカラーLED 秋月電子通商 結果 とりあえず2個のフルカラーLEDを制御できた。 ただし、r, g, bはそれぞれ0(OFF)か1(ON)でしか調整することは出来ない。 プログラムとハマりポイント プログラム GitHub ハマりポイント I2Cアドレスでハマった。 秋月の解説ページでは初期状態なら0xE0(write)と書いてあったので、Arduinoのスケッチでアドレスを0xE0に指定したがなにも反応しなかった。 調べてみるとこの方法は誤りで、Arduinoの場合は最下位ビットを除いた7ビット分をアドレス値に設定する必要があるらしい。 スレーブアドレスを0x70と設定したらうまくいった。 参考：https://tool-lab.com/make/pic-practice-37/ 雑記 いま探したらaitendoでもっと安い、100円のフルカラーLEDドライバ(単体)があった。 ただこっちはチップ単体の値段だし、データシートが中文で読むのが大変そう…。 IoT開発スタートブック　── ESP32でクラウドにつなげる電子工作をはじめよう！ 技術評論社 Amazon Kindle 楽天 楽天Kobo</description></item><item><title>Spresense SDKでカメラモジュール用のサンプルを動かした</title><link>https://kouya17.com/posts/14/</link><pubDate>Sat, 30 Nov 2019 20:43:11 +0900</pubDate><guid>https://kouya17.com/posts/14/</guid><description>概要 ソニーのシングルボードコンピュータSpresense及びSpresense用カメラモジュールを使って、公式で公開されているサンプルプログラムを動かした。 大体READMEの手順に従ったが、ところどころ詰まるところがあったのでメモとして残しておく。 Spresenseの簡単な説明 ソニーが開発したシングルボードコンピュータ。 Arduino IDEで開発可能。 NuttXというリアルタイムOSベースのSpresense SDKでも開発できる(今回はこっち)。 機能の主な特徴は以下。 GPSが標準で内蔵されている。 マルチコア(6コア!)プロセッサ。 メインボード単体だとネットワーク機能が使えないという点が個人的にはネックだと思っている。 Spresenseが出る少し前に流行し始めたEPS32系にすべて持っていかれている印象がある。 サンプル実行手順 今回動かしたサンプル 今回はカメラモジュールの動作確認がしたかったので公式が公開しているexamples/cameraを動かした。
実行環境 OS：Windows10上のWSL(Ubuntu16.04) 開発環境：Spresense SDK 手順 基本はスタートガイド参照。
1.開発ツールのセットアップ、ブートローダのインストールを行う。
ここは公式の手順通りでいけるはずなので省略。 2.手順1.でSpresenseのプロジェクト一式をGitHubから落としてきているはずなので、その中のexamples/camera/README.txtを確認する。
READMEを読むと、液晶コントローラILI9340で駆動しているLCDを使って、カメラが取得した画像を表示できるようになっていることが分かる。 3.ILI9340で駆動しているLCDを用意してSpresenseにつなぐ。
今回はちょうど手元にILI9341で駆動している2.2インチTFTがあったのでそれを使った。 ここが引っかかるポイントその1(だと思う)。 READMEを読むと、拡張ボードを使わない場合はSPI5のピンにSPI関係の4ピンをつなげばよいということは書いてあるのだが、TFTのDCピンとRESETピンの配線先ピンが分からない。 この問題に関しては、コードを読みこむ必要があり(本当はどこかに説明書きがあるのかもしれないが…)、/sdk/bsp/board/spresense/include/board.h198行目～に定義されている。 #if defined(CONFIG_LCD_ON_MAIN_BOARD) /* Display connected to main board. */#define DISPLAY_RST PIN_I2S0_BCK #define DISPLAY_DC PIN_I2S0_LRCK #define DISPLAY_SPI 5 この記述から、DCピンはI2S0 LRCKピン、RESETピンはI2S0 BCKピンにつなげば良いことがわかる。 4.</description></item><item><title>NeoPixelをESP32開発ボードで点灯させるのに丸1日かかった話</title><link>https://kouya17.com/posts/13/</link><pubDate>Sat, 02 Nov 2019 21:21:29 +0900</pubDate><guid>https://kouya17.com/posts/13/</guid><description>概要 当初ESPr Developer 32でNeoPixelを1個制御したかった 点灯自体はするのだが、色が正しく反映されない(赤を指定してるつもりでも、緑や青色になる) 上記問題を解消するのに丸1日かかった 結論 NeoPixelの動作電圧5Vに対し、IOの電圧を3.3Vにしてしまっていた 丸1日、試したこと 最初に、今回使用したNeoPixelの動作電圧がデータシート上4.5V~6Vということで、NeoPixelのVCCは5V、DINはESPr Developer 32のIO27ピンに接続した。 ライブラリAdafruit_NeoPixelを使って動作確認用プログラムを作成。 なんか色がおかしい！clear()も正常に働いていない。 &amp;ldquo;esp32 NeoPixel&amp;quot;でググる。 なんか他の人は大体正常に動作してるっぽい。 ネットの記事が比較的古い情報だったので、ライブラリのバージョンを疑う。 ライブラリAdafruit_NeoPixelのバージョンをネットの記事に合わせて下げる(ver1.3.0→ver1.2.3)。 動作不良変わりなし。 Arduino core for the ESP32のバージョンを下げる(ver1.0.4→ver1.0.0)。 動作不良変わりなし。 ライブラリAdafruit_NeoPixelがESP32ボードだとうまく使えないという記事を見かけ、NeoPixelライブラリ自体を疑う。 代替ライブラリとしてFastLEDのサンプルを動かす。 これも色がおかしい！ 代替ライブラリとしてNeoPixelBusのサンプルを動かす。 相変わらず色がおかしい。 Arduino Pro Mini互換機でAdafruit_NeoPixelのサンプルを動かす。 ちゃんと想定通りの色が出る。 ネットで再度ググるが&amp;quot;ESP32でもNeoPixelは使える&amp;quot;という情報ばかり。 動作確認に使っていたESPr Developer 32ボードを疑う。 違うESPr Developer 32ボードで動作確認する。 動作不良変わりなし。 同じESP32開発ボードであるESP32 DevKitCを使って動作確認する。 動作不良変わりなし。 電源だけArduino ProMini互換機からとってみたりする。 動作不良変わりなし。 ライブラリAdafruit_NeoPixelの実装を見てみる。 ESP32関係ボードの場合は、マクロF_CPUの値からCPU動作周波数を確認し、インラインアセンブラで信号のHIGH/LOWのタイミングを制御しているらしい。 一応マクロF_CPUの値をシリアル出力させて確認してみる。 F_CPU=240000000(240MHz)で想定通り。 本当に240MHzで動いてるのかを確認しようかとも思ったが、確認方法もパッとわからないので後回しにする。 ビルドオプションでCPU動作周波数を変えてみる。 動作不良変わりなし。 Adafruit_NeoPixelのProjectページ(GitHub)のissueを確認して類似の問題がないか確認する。 https://github.com/adafruit/Adafruit_NeoPixel/issues/139を見つける。 細かいことはよくわからないがNeoPixelBusを使えば正常に動いたという報告があるみたい。 といっても前述の通りNeoPixelBusのサンプルをそのまま動かしただけでは正常に動作しなかったため、NeoPixelBusのProjectページ(GitHub)のwikiを見る。 https://github.com/Makuna/NeoPixelBus/wiki/FAQ-%232を見つける。 自分の無知さに絶望する。 対策 とりあえず以下のいずれかの方法で正常な色が出力されることを確認。</description></item><item><title>[Raspi Zero W]Play startup sound with UART MP3 Voice Module</title><link>https://kouya17.com/posts/12/</link><pubDate>Sun, 13 Oct 2019 19:22:50 +0900</pubDate><guid>https://kouya17.com/posts/12/</guid><description>Purpose Play startup sound when raspi boots
Prepare Raspberry Pi Zero W Gravity: UART MP3 Voice Module Reference https://qiita.com/ikemura23/items/6f9adce99a3db555a0e4 http://hendigi.karaage.xyz/2016/11/auto-boot/ https://tomosoft.jp/design/?p=11677 https://wiki.dfrobot.com/Voice_Module_SKU__DFR0534 Auto run methods /etc/rc.local Run script as root
crontab Run script as user
systemd Manage as a service
Implement Select /etc/rc.local at this time
Edit /home/pi/Boot/boot_syateki_server.py as below
import serial # open serial port s = serial.Serial(&amp;#39;/dev/serial0&amp;#39;, 9600, timeout=10) # set volume 0x16 (0x00 - 0x1E) s.</description></item><item><title>Using an I2C OLED Display with the Raspberry Pi Zero W</title><link>https://kouya17.com/posts/11/</link><pubDate>Sat, 12 Oct 2019 23:11:21 +0900</pubDate><guid>https://kouya17.com/posts/11/</guid><description>Environment Board Raspberry Pi Zero W OS result of lsb_release -a No LSB modules are available. Distributor ID: Raspbian Description: Raspbian GNU/Linux 10 (buster) Release: 10 Codename: buster OLED display HiLetgo 0.96&amp;quot; I2C シリアル 128×64 OLED LCDディスプレイSSD1306液晶 Setting i2c Enabling i2c sudo raspi-config select 5 Interfacing Options select P5 I2C select Yes Installing packages sudo apt-get update sudo apt-get install i2c-tools python-smbus Connect to OLED Wiring confirm pin assign http://xn&amp;ndash;ccke1di9d4h.</description></item><item><title>elecrow発注用zipファイル作成ソフト(KiCad向け)を作った</title><link>https://kouya17.com/posts/9/</link><pubDate>Sun, 06 Oct 2019 16:26:24 +0900</pubDate><guid>https://kouya17.com/posts/9/</guid><description>elecrowへプリント基板を発注する際に必要なファイル変換作業をソフトで自動化した。
作成ソフト 動作OS：Windows 開発環境：Visual Studio 2017 以下のGitHubのページから実行ファイルをダウンロードできる。 https://github.com/kouya17/KiCadHelper/releases
(ページ中のAssets内KiCadHelper.zipをクリック)
本ソフトが代替する作業 本ソフトは以下書籍(KiCadではじめる「プリント基板」製作)中のp131-p133の作業を代替する。
KiCadではじめる「プリント基板」製作
外川貴規 工学社 2018年02月
売り上げランキング : 楽天ブックスで購入Amazonで購入 by ヨメレバ 具体的には以下の処理を行う。
製造ファイル・ドリルファイルのファイル名統一 拡張子の変更(.drl→.txt、.gm1→.gml) 必要ファイルをzipに圧縮 各ファイル及びzipファイル名は「elecrow-プロジェクトフォルダ名-日付」となる。
zip圧縮前のフォルダはプロジェクトフォルダ下に残る。
使い方 ソフトを起動する プロジェクトフォルダに、すでに製造ファイル・ドリルファイルを出力済みのKiCadプロジェクトのフォルダを指定する zipファイル出力先に、発注用のzipファイルを出力する場所を指定する 開始ボタンを押す 画面下部にログが表示され、処理中にエラー等があればエラーログが出力される。</description></item><item><title>Djangoで記事をカード型で表示するようにした</title><link>https://kouya17.com/posts/8/</link><pubDate>Thu, 03 Oct 2019 22:52:14 +0900</pubDate><guid>https://kouya17.com/posts/8/</guid><description>参考サイト Bootstrap公式ドキュメント
https://getbootstrap.com/docs/4.3/components/card/
Card decksの部分をほぼそのまま使った。
css部分は以下のQiitaの記事を参考にさせていただいた。
https://qiita.com/iwato/items/840b831ad66fec0dd4c1
コード html &amp;lt;div class=&amp;quot;col-sm-4&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;card card-link post-card&amp;quot;&amp;gt; &amp;lt;a href=&amp;quot;{% url 'posts:post_detail' post.id %}&amp;quot;&amp;gt;&amp;lt;/a&amp;gt; &amp;lt;img class=&amp;quot;card-img-top&amp;quot; src=&amp;quot;{{ post.middle_image.url }}&amp;quot; alt=&amp;quot;thumbnail&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;card-body&amp;quot;&amp;gt; &amp;lt;h5 class=&amp;quot;card-title&amp;quot;&amp;gt;{{ post.title }}&amp;lt;/h5&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;card-footer&amp;quot;&amp;gt; &amp;lt;small class=&amp;quot;text-muted&amp;quot;&amp;gt;published at {{ post.published|date:&amp;quot;Y/n/j&amp;quot; }}&amp;lt;/small&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; 一行に最大3つ記事を表示するためにcol-sm-4を指定している。
記事のサムネイル、タイトル、公開日を表示するようにしている。
css /*div要素全体にリンクをつけるために必要な要素*/ .card-link { position: relative; } .card-link a { position: absolute; top: 0; left: 0; height:100%; width: 100%; } .post-card { margin-top: 5px; margin-bottom: 5px; } カード全体にリンクをつけるために.</description></item><item><title>Djangoでタグクラウドを実装した</title><link>https://kouya17.com/posts/7/</link><pubDate>Sun, 15 Sep 2019 15:07:15 +0900</pubDate><guid>https://kouya17.com/posts/7/</guid><description>参考ページ https://programmer-jobs.blogspot.com/2012/12/djangodjango-taggit.html
django-taggitとdjango-taggit-templatetagsがあればできるらしい。 実装した流れ 1 setting.pyを確認し、taggitが導入されていることを確認(taggitは昔すでに導入済み)
ただ、taggit-templatetagsは導入されていなかった。
2 pip install django-taggit-templatetagsでインストール
3 setting.pyのINSTALLED_APPSにtaggit-templatetagsを追加
INSTALLED_APPS = ( ... &amp;quot;taggit&amp;quot;, &amp;quot;taggit_templatetags&amp;quot;, ... ) 4 pip freezeでインストールしたtaggit-templatetagsのバージョンを確認
$ pip freeze ... django-taggit-templatetags==0.2.5 ... 5 requirements.txtに4.で確認したバージョン情報を追記
... django-taggit-templatetags==0.2.5 ... 6 デプロイ
7 動作確認したらエラー発生
File &amp;quot;...templatetag_sugar/parser.py&amp;quot;, line 5, in &amp;lt;module&amp;gt; from django.db.models.loading import cache No module named 'django.db.models.loading' django.db.models.loadingがないらしい。
google先生に聞くと、django.db.models.loadingはDjango1.9で廃止され、現在はdjango.appを使う必要があるようだ。
https://stackoverflow.com/questions/36234635/what-is-the-equivalent-of-django-db-models-loading-get-model-in-django-1-9
8 tamplatetag_sugar/parser.pyを修正
- from django.db.models.loading import cache + from django.apps import apps .</description></item><item><title>VirtualBox6.0で共有フォルダの自動マウント機能が働かない</title><link>https://kouya17.com/posts/6/</link><pubDate>Fri, 16 Aug 2019 04:34:30 +0900</pubDate><guid>https://kouya17.com/posts/6/</guid><description>概要 VirtualBox6.0で共有フォルダの自動マウント機能がうまく効かなかったが解消した。
問題 VirtualBox6.0は共有フォルダの自動マウントをオンにしてもうまくマウントされないらしい。
https://unofficialtokyo.com/2018/12/virtualbox-ubuntu1804-on-windows/#Ubuntu
今回の解決法 毎ログイン時にmountコマンドを実行する。
https://www.souichi.club/technology/virtualbox-share/
ただ、上記サイト内の記述だとうまくパスワード入力が省略されなかった。
以下のサイトの記述を参考にしたらうまくいった。
https://www.usagi1975.com/31jan172009/
ハッキング・ラボのつくりかた 仮想環境におけるハッカー体験学習 翔泳社 Amazon Kindle 楽天 楽天Kobo</description></item><item><title>GitHub Pagesを使って静的サイトを公開した</title><link>https://kouya17.com/posts/4/</link><pubDate>Thu, 02 May 2019 15:04:03 +0900</pubDate><guid>https://kouya17.com/posts/4/</guid><description>概要 GitHub Pagesを使って簡易ブログを公開しました。
https://kouya17.github.io
構成 公開環境：GitHub Pages
静的サイトジェネレータ：Hugo
テンプレートテーマ：aether
雑記 無料でサイト公開できるのはすごい。
静的サイトジェネレータとしてHexoも検討したが、ネット上の情報が中国語中心だったため断念した。
作成物に関する投稿はGitHub Pagesのほうにして、こちらはより雑多なものを扱うことにする。
独習Git 翔泳社 Amazon Kindle 楽天 楽天Kobo</description></item><item><title>LEDの色で天気通知する(ESP32)</title><link>https://kouya17.com/posts/1/</link><pubDate>Tue, 01 Jan 2019 23:07:37 +0900</pubDate><guid>https://kouya17.com/posts/1/</guid><description>ソースコード https://github.com/kouya17/esp32_weather_light
説明 ESP32を使って、LEDの色で天気通知をしてくれるプログラムを作りました。
単3電池2本駆動で8時間程度しか持たないのが課題ですね…。
IoT開発スタートブック　── ESP32でクラウドにつなげる電子工作をはじめよう！ 技術評論社 Amazon Kindle 楽天 楽天Kobo</description></item><item><title>プライバシーポリシー</title><link>https://kouya17.com/privacy_policy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kouya17.com/privacy_policy/</guid><description>プライバシーポリシーページ</description></item><item><title>免責事項</title><link>https://kouya17.com/disclaimer/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kouya17.com/disclaimer/</guid><description>免責事項ページ</description></item></channel></rss>